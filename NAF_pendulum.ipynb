{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac5833c-5ded-406e-ac52-41350c82cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7629042-5a6e-479c-b7c0-88305ec82066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for NAF\n",
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "        self.fc_mu = nn.Linear(256, action_size)\n",
    "        self.fc_l = nn.Linear(256, action_size * action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        value = self.fc_value(x)\n",
    "        mu = torch.tanh(self.fc_mu(x))\n",
    "        l = self.fc_l(x)\n",
    "\n",
    "        l_matrix = l.view(-1, action_size, action_size)\n",
    "        l_matrix = torch.tril(l_matrix, -1) + torch.diag_embed(torch.exp(torch.diagonal(l_matrix, dim1=-2, dim2=-1)))\n",
    "        p_matrix = torch.bmm(l_matrix, l_matrix.transpose(2, 1))\n",
    "\n",
    "        return value, mu, p_matrix\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        value, mu, p_matrix = self.forward(state)\n",
    "        action_diff = action - mu\n",
    "        advantage = -0.5 * torch.bmm(action_diff.unsqueeze(1), torch.bmm(p_matrix, action_diff.unsqueeze(2))).squeeze(2)\n",
    "        q_value = value + advantage\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f277494d-0c7f-441e-8e6f-daa020fa580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bf41011-f469-4b54-a33e-888854179614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    minibatch = memory.sample(batch_size)\n",
    "    states = torch.FloatTensor([e[0] for e in minibatch])\n",
    "    actions = torch.FloatTensor([e[1] for e in minibatch])\n",
    "    rewards = torch.FloatTensor([e[2] for e in minibatch])\n",
    "    next_states = torch.FloatTensor([e[3] for e in minibatch])\n",
    "    dones = torch.FloatTensor([e[4] for e in minibatch])\n",
    "\n",
    "    q_values = naf_network.q_value(states, actions)\n",
    "    next_actions = target_naf_network(next_states)[1]\n",
    "    next_q_values = target_naf_network.q_value(next_states, next_actions)\n",
    "    target_q_values = rewards.unsqueeze(1) + (1 - dones).unsqueeze(1) * discount_factor * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_naf_network.parameters(), naf_network.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbc1d7d-3a4b-4c0a-8b48-583b9db8785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    _, mu, _ = naf_network(state)\n",
    "    return mu.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f4ae35a-d5ee-49e3-8955-e623fe4171e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cstr_env(gym.Env):\n",
    "\n",
    "    def __init__(self): \n",
    "        # Define action & observation space   \n",
    "        self.action_space = spaces.Box(low = np.array([-1.0, -1.0], dtype=np.float32), \n",
    "                                       high = np.array([1.0 , 1.0], dtype=np.float32), \n",
    "                                       dtype=np.float32, shape=(2, ))   \n",
    "        self.observation_space = spaces.Box(low=np.array([-1.0, -1.0], dtype=np.float32), \n",
    "                                            high=np.array([1.0, 1.0], dtype=np.float32), \n",
    "                                            dtype=np.float32, shape=(2, ))\n",
    "        self.n_episode = 0 # current episode number.\n",
    "\n",
    "    def setpoint(self):\n",
    "        # This the reference state for each episode if you want to train for other reference point change the below line. \n",
    "        self.setpoint_states  =  np.array([.0, .0], dtype=float)     \n",
    "        self.setpoint_actions =  np.array([.0, .0], dtype=float ) \n",
    "        return None \n",
    "    \n",
    "    def calculate_reward(self, x_next, u_curr):\n",
    "        r = np.sum((s_x_next - self.scaled_setpoint_state)**2) + np.sum((s_u_curr - self.setpoint_actions)**2)  \n",
    "        reward = -r \n",
    "        return reward \n",
    "\n",
    "    \n",
    "    def is_done(self, x_next):\n",
    "        done=False\n",
    "        c1 = (abs(x_next[0] - self.setpoint_states[0]) < 0.01)\n",
    "        c2 = (abs(x_next[1] - self.setpoint_states[1]) < 0.01)\n",
    "        steady_state = c1 and c2  \n",
    "\n",
    "        # Record the steady state status for the current step\n",
    "        self.goal_state_done[self.ep_step] = steady_state\n",
    "        \n",
    "        # Check if there are at least 4 previous steps\n",
    "        if self.ep_step > 3: \n",
    "            # Get the steady state status for the last three steps\n",
    "            p3 = self.goal_state_done[self.ep_step-2] \n",
    "            p2 = self.goal_state_done[self.ep_step-1] \n",
    "            p1 = self.goal_state_done[self.ep_step-0] \n",
    "            # If the last three steps were steady states, set 'done' to True\n",
    "            if  p3 and p2 and p1:\n",
    "                done = True  \n",
    "        return done \n",
    "\n",
    "    \n",
    "    def get_dx(x, u):\n",
    "        Q = torch.tensor([[9.35, 0.41], [0.41, 0.02]])\n",
    "        R = torch.tensor([[1/500, 0], [0, 1/100]])\n",
    "        P = torch.tensor([[9.35, 0.41], [0.41, 0.02]])\n",
    "        \n",
    "        params = [0.5734, 395.3268, 100e-3, 0.1, 72e+9, 8.314e+4, 8.314, 310, -4.78e+4, 0.239, 1000, 1]\n",
    "        CAs, Ts, CF, CV, Ck0, CE, CR, CT0, CDh, Ccp, Crho, CA0s = params                   \n",
    "        g = torch.tensor([[CF/CV, 0], [0, 1/(Crho*Ccp*CV)]])\n",
    "        g1, g2 = CF/CV, 1/(Crho*Ccp*CV)\n",
    "        x1, x2 = x[0].detach().numpy(), x[1].detach().numpy()\n",
    "        \n",
    "        f1 = -Cp1*x1 - Cp2*exp(-CE/(CR*(x2+Ts))) * ((x1+CAs)**2) + Cp3\n",
    "        f2 = -Cp1*x2 + Cp4*Cp2*exp(-CE/(CR*(x2+Ts)))*((x1+CAs)**2) + Cp5 + Qs*Cp6\n",
    "        dx = torch.tensor(np.array([f1, f2])) + u@g\n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        dt = 5e-3\n",
    "        self.current_u = action \n",
    "        x_next = self.current_s + dt*self.get_dx(self.current_s, action)\n",
    "        done = self.is_done(x_next)   \n",
    "\n",
    "        # calculate the reward for the current state. \n",
    "        reward = self.calculate_reward(x_next, self.current_u) \n",
    "\n",
    "        # changing the previous state to the current state. \n",
    "        self.previous_u = self.current_u \n",
    "        # changing the current state to next state\n",
    "        self.current_s = x_next \n",
    "        # increase the step by one \n",
    "        self.ep_step += 1  \n",
    "\n",
    "        # this is the trancated condition. \n",
    "        trancated = False \n",
    "        if self.ep_step == episode_length:\n",
    "            trancated = True\n",
    "\n",
    "        if self.ep_step == episode_length-1 or done:      \n",
    "            self.n_episode += 1 \n",
    "        \n",
    "        # if done is true i.e. terminated is equal to done. \n",
    "        terminated = done\n",
    "\n",
    "        return x_next, reward, done\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.ep_step = 0 \n",
    "        self.current_u= None \n",
    "        self.previous_u = None \n",
    "        self.current_s = None \n",
    "\n",
    "        ## list of true false which stores the weather the state is near to the goal state or not. \n",
    "        self.goal_state_done = [False] * (episode_length+5)\n",
    "\n",
    "        self.setpoint_states = None\n",
    "        self.setpoint_actions = None \n",
    "\n",
    "        ## this function is set the setpoint for the current state and actions. \n",
    "        self.setpoint()  \n",
    "\n",
    "        # this is the fixed initial state. \n",
    "        state, action = np.array([0.2, -5]),  np.array([1.5, 0.1]) \n",
    "\n",
    "        self.current_u = action \n",
    "        self.previous_u = action  \n",
    "        self.current_s = state  \n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9640bf2-ffde-400c-bd69-955b7dfb1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "discount_factor = 0.99\n",
    "batch_size = 256\n",
    "tau = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\n",
    "episode_length = num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81b119a2-3554-4cd6-acc1-0d4515cf9e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cstr_env.get_dx() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m action \u001b[38;5;241m=\u001b[39m get_action(state, epsilon)\n\u001b[1;32m---> 26\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     29\u001b[0m memory\u001b[38;5;241m.\u001b[39madd((state, action, reward, next_state, done))\n",
      "Cell \u001b[1;32mIn[52], line 66\u001b[0m, in \u001b[0;36mcstr_env.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     64\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-3\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_u \u001b[38;5;241m=\u001b[39m action \n\u001b[1;32m---> 66\u001b[0m x_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_s \u001b[38;5;241m+\u001b[39m dt\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_done(x_next)   \n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# calculate the reward for the current state. \u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cstr_env.get_dx() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(memory_size)\n",
    "\n",
    "# Set up the environment\n",
    "env = cstr_env()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize networks and optimizer\n",
    "naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network.load_state_dict(naf_network.state_dict())\n",
    "target_naf_network.eval()\n",
    "\n",
    "optimizer = optim.Adam(naf_network.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = get_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        train_model()\n",
    "        if step == 500:\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"Episode: {e+1}/{num_episodes}, Reward: {total_reward}, done: {done}\")\n",
    "    print(f\"state: {state}, action: {action}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a08d4-b186-44b8-bf8b-f93700e4377a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
