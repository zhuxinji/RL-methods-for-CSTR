{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac5833c-5ded-406e-ac52-41350c82cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7629042-5a6e-479c-b7c0-88305ec82066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for NAF\n",
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "        self.fc_mu = nn.Linear(256, action_size)\n",
    "        self.fc_l = nn.Linear(256, action_size * action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        value = self.fc_value(x)\n",
    "        mu = torch.tanh(self.fc_mu(x))\n",
    "        l = self.fc_l(x)\n",
    "\n",
    "        l_matrix = l.view(-1, action_size, action_size)\n",
    "        l_matrix = torch.tril(l_matrix, -1) + torch.diag_embed(torch.exp(torch.diagonal(l_matrix, dim1=-2, dim2=-1)))\n",
    "        p_matrix = torch.bmm(l_matrix, l_matrix.transpose(2, 1))\n",
    "\n",
    "        return value, mu, p_matrix\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        value, mu, p_matrix = self.forward(state)\n",
    "        action_diff = action - mu\n",
    "        advantage = -0.5 * torch.bmm(action_diff.unsqueeze(1), torch.bmm(p_matrix, action_diff.unsqueeze(2))).squeeze(2)\n",
    "        q_value = value + advantage\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f277494d-0c7f-441e-8e6f-daa020fa580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf41011-f469-4b54-a33e-888854179614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    minibatch = memory.sample(batch_size)\n",
    "    states = torch.FloatTensor([e[0] for e in minibatch])\n",
    "    actions = torch.FloatTensor([e[1] for e in minibatch])\n",
    "    rewards = torch.FloatTensor([e[2] for e in minibatch])\n",
    "    next_states = torch.FloatTensor([e[3] for e in minibatch])\n",
    "    dones = torch.FloatTensor([e[4] for e in minibatch])\n",
    "\n",
    "    q_values = naf_network.q_value(states, actions)\n",
    "    next_actions = target_naf_network(next_states)[1]\n",
    "    next_q_values = target_naf_network.q_value(next_states, next_actions)\n",
    "    target_q_values = rewards.unsqueeze(1) + (1 - dones).unsqueeze(1) * discount_factor * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_naf_network.parameters(), naf_network.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbc1d7d-3a4b-4c0a-8b48-583b9db8785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    _, mu, _ = naf_network(state)\n",
    "    return mu.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4ae35a-d5ee-49e3-8955-e623fe4171e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cstr_env(gym.Env):\n",
    "\n",
    "    def __init__(self): \n",
    "        # Define action & observation space   \n",
    "        self.action_space = spaces.Box(low = np.array([-1.0, -1.0], dtype=np.float32), \n",
    "                                       high = np.array([1.0 , 1.0], dtype=np.float32), \n",
    "                                       dtype=np.float32, shape=(2, ))   \n",
    "        self.observation_space = spaces.Box(low=np.array([-1.0, -1.0], dtype=np.float32), \n",
    "                                            high=np.array([1.0, 1.0], dtype=np.float32), \n",
    "                                            dtype=np.float32, shape=(2, ))\n",
    "        self.n_episode = 0 # current episode number.\n",
    "\n",
    "    def setpoint(self):\n",
    "        # This the reference state for each episode if you want to train for other reference point change the below line. \n",
    "        self.setpoint_states  =  np.array([.0, .0], dtype=float)     \n",
    "        self.setpoint_actions =  np.array([.0, .0], dtype=float ) \n",
    "        return None \n",
    "    \n",
    "    def calculate_reward(self, x_next, u_curr):\n",
    "        r = np.sum((x_next - self.setpoint_states)**2) + np.sum((u_curr - self.setpoint_actions)**2)  \n",
    "        reward = -r \n",
    "        return reward \n",
    "\n",
    "    \n",
    "    def is_done(self, x_next):\n",
    "        done = False\n",
    "        c1 = (abs(x_next[0] - self.setpoint_states[0]) < 0.01)\n",
    "        c2 = (abs(x_next[1] - self.setpoint_states[1]) < 0.01)\n",
    "        steady_state = c1 and c2  \n",
    "\n",
    "        # Record the steady state status for the current step\n",
    "        self.goal_state_done[self.ep_step] = steady_state\n",
    "        \n",
    "        # Check if there are at least 4 previous steps\n",
    "        if self.ep_step > 3: \n",
    "            # Get the steady state status for the last three steps\n",
    "            p3 = self.goal_state_done[self.ep_step-2] \n",
    "            p2 = self.goal_state_done[self.ep_step-1] \n",
    "            p1 = self.goal_state_done[self.ep_step-0] \n",
    "            # If the last three steps were steady states, set 'done' to True\n",
    "            if  p3 and p2 and p1:\n",
    "                done = True  \n",
    "        return done \n",
    "\n",
    "    \n",
    "    def get_dx(self, x, u):\n",
    "        Q = torch.tensor([[9.35, 0.41], [0.41, 0.02]])\n",
    "        R = torch.tensor([[1/500, 0], [0, 1/100]])\n",
    "        P = torch.tensor([[9.35, 0.41], [0.41, 0.02]])\n",
    "        \n",
    "        params = [0.5734, 395.3268, 100e-3, 0.1, 72e+9, 8.314e+4, 8.314, 310, -4.78e+4, 0.239, 1000, 1]\n",
    "        CAs, Ts, CF, CV, Ck0, CE, CR, CT0, CDh, Ccp, Crho, CA0s = params                   \n",
    "        g1, g2 = CF/CV, 1/(Crho*Ccp*CV)\n",
    "        x1, x2 = x[0], x[1]\n",
    "        \n",
    "        f1 = (CF/CV)*(-x1) - Ck0*np.exp(-CE/(CR*(x2+Ts))) * (x1+CAs)+(CF/CV) * (CA0s-CAs)\n",
    "        f2 = (CF/CV)*(-x2) + (-CDh/(Crho*Ccp))*Ck0*np.exp(-CE/(CR*(x2+Ts)))*(x1+CAs) + CF*(CT0-Ts)/CV\n",
    "        dx = [f1, f2] + u*[g1, g2]\n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        dt = 5e-3\n",
    "        self.current_u = action\n",
    "        state = self.current_s\n",
    "        x_next = self.current_s + dt*self.get_dx(self.current_s, action)\n",
    "        done = self.is_done(x_next)   \n",
    "\n",
    "        # calculate the reward for the current state. \n",
    "        reward = self.calculate_reward(x_next, self.current_u) \n",
    "\n",
    "        # changing the previous state to the current state. \n",
    "        self.previous_u = self.current_u \n",
    "        # changing the current state to next state\n",
    "        self.current_s = x_next \n",
    "        # increase the step by one \n",
    "        self.ep_step += 1  \n",
    "\n",
    "        # this is the trancated condition. \n",
    "        trancated = False \n",
    "        if self.ep_step == episode_length:\n",
    "            trancated = True\n",
    "\n",
    "        if self.ep_step == episode_length-1 or done:      \n",
    "            self.n_episode += 1 \n",
    "        \n",
    "        # if done is true i.e. terminated is equal to done. \n",
    "        terminated = done\n",
    "\n",
    "        return x_next, reward, done\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.ep_step = 0 \n",
    "        self.current_u= None \n",
    "        self.previous_u = None \n",
    "        self.current_s = None \n",
    "\n",
    "        ## list of true false which stores the weather the state is near to the goal state or not. \n",
    "        self.goal_state_done = [False] * (episode_length+5)\n",
    "\n",
    "        self.setpoint_states = None\n",
    "        self.setpoint_actions = None \n",
    "\n",
    "        ## this function is set the setpoint for the current state and actions. \n",
    "        self.setpoint()  \n",
    "\n",
    "        # this is the fixed initial state. \n",
    "        state, action = np.array([0.2, -5]),  np.array([1.5, 0.1]) \n",
    "\n",
    "        self.current_u = action \n",
    "        self.previous_u = action  \n",
    "        self.current_s = state  \n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9640bf2-ffde-400c-bd69-955b7dfb1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "discount_factor = 0.99\n",
    "batch_size = 256\n",
    "tau = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\n",
    "episode_length = num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81b119a2-3554-4cd6-acc1-0d4515cf9e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuxinji\\AppData\\Local\\Temp\\ipykernel_35428\\901622226.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  states = torch.FloatTensor([e[0] for e in minibatch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1000, Reward: -837693.4207203381, done: False\n",
      "state: [  0.34708271 -69.04972077], action: [-0.94340295 -0.7994191 ]\n",
      "Episode: 2/1000, Reward: -431166.608013147, done: False\n",
      "state: [  0.29536817 -59.22436586], action: [-0.92685103 -0.382933  ]\n",
      "Episode: 3/1000, Reward: -774737.3204893519, done: False\n",
      "state: [  0.38404643 -67.86187368], action: [-0.4823735  -0.06091614]\n",
      "Episode: 4/1000, Reward: -682016.4114004248, done: False\n",
      "state: [  0.34463567 -66.09911374], action: [-0.976687   0.6997881]\n",
      "Episode: 5/1000, Reward: -4202657.140565672, done: False\n",
      "state: [ -0.56475534 113.8117291 ], action: [ 0.7555942 -0.7585215]\n",
      "Episode: 6/1000, Reward: -540637.8190815793, done: False\n",
      "state: [  0.32238562 -62.71129915], action: [0.9520456  0.19109891]\n",
      "Episode: 7/1000, Reward: -674016.0741523353, done: False\n",
      "state: [  0.32984612 -65.88172488], action: [ 0.50250185 -0.7424156 ]\n",
      "Episode: 8/1000, Reward: -540857.8444780464, done: False\n",
      "state: [  0.40898305 -62.60857601], action: [ 0.34059614 -0.50454164]\n",
      "Episode: 9/1000, Reward: -529324.0855324923, done: False\n",
      "state: [  0.39131444 -62.31396683], action: [0.980744  0.7550012]\n",
      "Episode: 10/1000, Reward: -529947.7397241457, done: False\n",
      "state: [  0.35074235 -62.36901753], action: [0.9664756 0.5767001]\n",
      "Episode: 11/1000, Reward: -523596.9577905057, done: False\n",
      "state: [  0.33478261 -62.24395983], action: [ 0.13635471 -0.41029483]\n",
      "Episode: 12/1000, Reward: -3732024.9239673847, done: False\n",
      "state: [ -0.60411436 143.68144773], action: [0.5038911  0.65546155]\n",
      "Episode: 13/1000, Reward: -5026205.505322984, done: False\n",
      "state: [ -0.57943437 132.98104838], action: [1. 1.]\n",
      "Episode: 14/1000, Reward: -503802.7419441081, done: False\n",
      "state: [  0.3762029 -61.5557931], action: [-0.24244307 -0.08487784]\n",
      "Episode: 15/1000, Reward: -655966.791897957, done: False\n",
      "state: [  0.3492988  -65.51310974], action: [-0.30904764  0.8381241 ]\n",
      "Episode: 16/1000, Reward: -3006522.520772397, done: False\n",
      "state: [ -0.45234826 117.60132584], action: [-0.7781504  -0.31898713]\n",
      "Episode: 17/1000, Reward: -5628513.607745405, done: False\n",
      "state: [ -0.59163544 135.20076324], action: [-0.7055404   0.32853195]\n",
      "Episode: 18/1000, Reward: -513230.56892912777, done: False\n",
      "state: [  0.38655927 -61.7962793 ], action: [ 0.74263304 -0.17775975]\n",
      "Episode: 19/1000, Reward: -4385936.006636244, done: False\n",
      "state: [ -0.59338039 146.13958995], action: [-0.10337085 -0.07105333]\n",
      "Episode: 20/1000, Reward: -2785254.5809497177, done: False\n",
      "state: [ -0.51763021 127.17425518], action: [ 1. -1.]\n",
      "Episode: 21/1000, Reward: -430538.9710084581, done: False\n",
      "state: [  0.3615754  -58.99478465], action: [-0.12194954 -0.167651  ]\n",
      "Episode: 22/1000, Reward: -655948.8853466826, done: False\n",
      "state: [  0.43115842 -65.44573462], action: [-0.03276147  0.83399206]\n",
      "Episode: 23/1000, Reward: -6205905.925580149, done: False\n",
      "state: [ -0.51185183 123.5618223 ], action: [-0.408575    0.75270754]\n",
      "Episode: 24/1000, Reward: -779597.2042223116, done: False\n",
      "state: [  0.50561644 -67.90597674], action: [-0.13551572 -0.76739347]\n",
      "Episode: 25/1000, Reward: -610830.4991636217, done: False\n",
      "state: [  0.40492668 -64.44237151], action: [ 0.79911673 -0.33945683]\n",
      "Episode: 26/1000, Reward: -780579.1602400345, done: False\n",
      "state: [  0.46812708 -67.93161561], action: [0.17513266 0.77848566]\n",
      "Episode: 27/1000, Reward: -754079.7551774281, done: False\n",
      "state: [  0.45911914 -67.40286693], action: [ 0.22750166 -0.0658043 ]\n",
      "Episode: 28/1000, Reward: -896397.8016960361, done: False\n",
      "state: [  0.44706373 -69.87597302], action: [-0.372023    0.61564356]\n",
      "Episode: 29/1000, Reward: -879672.3318442887, done: False\n",
      "state: [  0.44340088 -69.71121822], action: [-0.99604726 -0.9181665 ]\n",
      "Episode: 30/1000, Reward: -791757.2314935543, done: False\n",
      "state: [  0.38287325 -68.17348391], action: [-0.2655162  -0.41134647]\n",
      "Episode: 31/1000, Reward: -873299.8170849773, done: False\n",
      "state: [  0.41921084 -69.58406417], action: [0.3618474  0.03005151]\n",
      "Episode: 32/1000, Reward: -946813.0692000607, done: False\n",
      "state: [  0.19230247 -70.88320328], action: [ 0.9820059 -0.6831651]\n",
      "Episode: 33/1000, Reward: -627223.6364805559, done: False\n",
      "state: [  0.22123019 -65.1870689 ], action: [ 0.27955607 -0.29042724]\n",
      "Episode: 34/1000, Reward: -867296.3741103505, done: False\n",
      "state: [  0.25386177 -69.64612985], action: [0.39381734 0.27919284]\n",
      "Episode: 35/1000, Reward: -843903.5962874134, done: False\n",
      "state: [  0.25620315 -69.28668374], action: [-0.6951339   0.02310907]\n",
      "Episode: 36/1000, Reward: -888701.538146555, done: False\n",
      "state: [  0.21191075 -70.06577621], action: [0.04274327 0.7208548 ]\n",
      "Episode: 37/1000, Reward: -990392.1229196825, done: False\n",
      "state: [  0.24316237 -69.10184282], action: [-0.75556326 -0.00403383]\n",
      "Episode: 38/1000, Reward: -931799.1231110995, done: False\n",
      "state: [  0.20662077 -70.67660894], action: [-0.6043689 -0.5492321]\n",
      "Episode: 39/1000, Reward: -3047457.5286903777, done: False\n",
      "state: [  0.373377   -62.37958743], action: [-0.7811157   0.03782647]\n",
      "Episode: 40/1000, Reward: -1267551.418631183, done: False\n",
      "state: [  0.27641609 -68.84055553], action: [-0.44127613 -0.33643016]\n",
      "Episode: 41/1000, Reward: -3227469.7284973427, done: False\n",
      "state: [  0.5374673  -98.46468889], action: [-0.44887692  0.41799212]\n",
      "Episode: 42/1000, Reward: -2970450.1351918136, done: False\n",
      "state: [  0.2896092  -88.33480288], action: [ 0.04437957 -0.5216634 ]\n",
      "Episode: 43/1000, Reward: -4756666.3467536755, done: False\n",
      "state: [  0.35380696 -96.51765078], action: [-0.59434414  0.2590696 ]\n",
      "Episode: 44/1000, Reward: -1195385.9037032994, done: False\n",
      "state: [  0.30016343 -74.60852157], action: [-0.34506425  0.21754201]\n",
      "Episode: 45/1000, Reward: -4392406.875178996, done: False\n",
      "state: [ -0.61450706 152.88345007], action: [-0.18379663 -0.11976789]\n",
      "Episode: 46/1000, Reward: -1434851.0856707518, done: False\n",
      "state: [  0.22174918 -74.54351049], action: [-1. -1.]\n",
      "Episode: 47/1000, Reward: -4284031.381480011, done: False\n",
      "state: [  0.3131626  -97.33087388], action: [-0.36678934 -0.2856954 ]\n",
      "Episode: 48/1000, Reward: -3007386.6575138164, done: False\n",
      "state: [  0.51849309 -91.11374328], action: [0.85533166 0.50449693]\n",
      "Episode: 49/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 50/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.41253445 -0.860597  ]\n",
      "Episode: 51/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.3939014  0.8452137]\n",
      "Episode: 52/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 53/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 54/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 55/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.24218947 0.57001746]\n",
      "Episode: 56/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.19323249 0.45750514]\n",
      "Episode: 57/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [ 0.8380582 -0.4956062]\n",
      "Episode: 58/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 59/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 60/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 61/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 62/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.22664548 0.81895536]\n",
      "Episode: 63/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 64/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.44286042 0.21954879]\n",
      "Episode: 65/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 66/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.27807117  0.01425487]\n",
      "Episode: 67/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 68/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 69/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [ 0.38161898 -0.6851498 ]\n",
      "Episode: 70/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 71/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.52065265 0.3427523 ]\n",
      "Episode: 72/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [ 0.15498152 -0.50666386]\n",
      "Episode: 73/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 74/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 75/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 76/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 77/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.63777125 -0.24654803]\n",
      "Episode: 78/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.9709536  0.0440755]\n",
      "Episode: 79/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [ 0.14839111 -0.8420176 ]\n",
      "Episode: 80/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.6402874  -0.11501918]\n",
      "Episode: 81/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 82/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 83/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.4931839 0.9097938]\n",
      "Episode: 84/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 85/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 86/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.823639   0.50374764]\n",
      "Episode: 87/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [ 0.09931996 -0.47406447]\n",
      "Episode: 88/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 89/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.8285968   0.09714673]\n",
      "Episode: 90/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [0.6126043  0.13154252]\n",
      "Episode: 91/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 92/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 93/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.99848866  0.68472284]\n",
      "Episode: 94/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.03056765  0.5191354 ]\n",
      "Episode: 95/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 96/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [-0.8094715 -0.5203301]\n",
      "Episode: 97/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [ 0.89987254 -0.8106199 ]\n",
      "Episode: 98/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 99/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n",
      "Episode: 100/1000, Reward: nan, done: False\n",
      "state: [nan nan], action: [nan nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m memory\u001b[38;5;241m.\u001b[39madd((state, action, reward, next_state, done))\n\u001b[0;32m     30\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m---> 31\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_param, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target_naf_network\u001b[38;5;241m.\u001b[39mparameters(), naf_network\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m---> 23\u001b[0m     target_param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(tau \u001b[38;5;241m*\u001b[39m param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(memory_size)\n",
    "\n",
    "# Set up the environment\n",
    "env = cstr_env()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize networks and optimizer\n",
    "naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network.load_state_dict(naf_network.state_dict())\n",
    "target_naf_network.eval()\n",
    "\n",
    "optimizer = optim.Adam(naf_network.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = get_action(state, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        train_model()\n",
    "        if step == 500:\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"Episode: {e+1}/{num_episodes}, Reward: {total_reward}, done: {done}\")\n",
    "    print(f\"state: {state}, action: {action}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a08d4-b186-44b8-bf8b-f93700e4377a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
