{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac5833c-5ded-406e-ac52-41350c82cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from CSTREnv import cstr_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7629042-5a6e-479c-b7c0-88305ec82066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for NAF\n",
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "        self.fc_mu = nn.Linear(256, action_size, bias=False)\n",
    "        self.fc_l = nn.Linear(256, action_size * action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.fc_value(x)\n",
    "        mu = torch.tanh(self.fc_mu(x))*torch.tensor([1, 0.0167])\n",
    "        l = self.fc_l(x)\n",
    "\n",
    "        l_matrix = l.view(-1, action_size, action_size)\n",
    "        l_matrix = torch.tril(l_matrix, -1) + torch.diag_embed(torch.exp(torch.diagonal(l_matrix, dim1=-2, dim2=-1)))\n",
    "        p_matrix = torch.bmm(l_matrix, l_matrix.transpose(2, 1))\n",
    "\n",
    "        return value, mu, p_matrix\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        value, mu, p_matrix = self.forward(state)\n",
    "        action_diff = action - mu\n",
    "        advantage = -0.5 * torch.bmm(action_diff.unsqueeze(1), torch.bmm(p_matrix, action_diff.unsqueeze(2))).squeeze(2)\n",
    "        q_value = value + advantage\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf41011-f469-4b54-a33e-888854179614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch):\n",
    "    if epoch < 50:\n",
    "        return \n",
    "\n",
    "    minibatch = memory.sample(batch_size)\n",
    "    states = torch.FloatTensor([e[0] for e in minibatch])\n",
    "    actions = torch.FloatTensor([e[1] for e in minibatch])\n",
    "    rewards = torch.FloatTensor([e[2] for e in minibatch])\n",
    "    next_states = torch.FloatTensor([e[3] for e in minibatch])\n",
    "    dones = torch.FloatTensor([e[4] for e in minibatch])\n",
    "\n",
    "    q_values = naf_network.q_value(states, actions)\n",
    "    next_actions = target_naf_network(next_states)[1]\n",
    "    next_q_values = target_naf_network.q_value(next_states, next_actions)\n",
    "    target_q_values = rewards.unsqueeze(1) + (1 - dones).unsqueeze(1) * discount_factor * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_naf_network.parameters(), naf_network.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbc1d7d-3a4b-4c0a-8b48-583b9db8785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def get_action(e, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.sontag(state)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    _, mu, _ = naf_network(state)\n",
    "    return mu.detach().numpy()[0]\n",
    "\n",
    "def plot_state(states, epoch, filename='./figure2/state_at_epoch_'):\n",
    "    figure_state = np.array(states)\n",
    "    time = np.linspace(0, len(figure_state), len(figure_state))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(time, figure_state[:, 0], label='State 1')\n",
    "    plt.plot(time, figure_state[:, 1], label='State 2')\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('State Value')\n",
    "    plt.title('State Values Over Time')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(filename+f'{epoch}')\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9640bf2-ffde-400c-bd69-955b7dfb1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "discount_factor = 0.99\n",
    "batch_size = 256\n",
    "tau = 0.001\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "memory_size = 1000000\n",
    "num_episodes = 3000\n",
    "episode_length = num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b119a2-3554-4cd6-acc1-0d4515cf9e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/3000, Reward: -5.682317432610404, done: True\n",
      "state: [ 0.00045985 -0.00919278], action: [-0.00088771  0.        ], loss: None\n",
      "Episode: 2/3000, Reward: -5.683502981904332, done: True\n",
      "state: [ 0.00046099 -0.00921771], action: [-0.00088903  0.        ], loss: None\n",
      "Episode: 3/3000, Reward: -5.684329110255615, done: True\n",
      "state: [ 0.00046173 -0.00925611], action: [-0.00085705  0.        ], loss: None\n",
      "Episode: 4/3000, Reward: -5.6840365237122965, done: True\n",
      "state: [ 0.0003819  -0.00961781], action: [0.00051848 0.        ], loss: None\n",
      "Episode: 5/3000, Reward: -5.685062188390357, done: True\n",
      "state: [ 0.00041522 -0.00943051], action: [3.7460573e-08 0.0000000e+00], loss: None\n",
      "Episode: 6/3000, Reward: -5.682901601132929, done: True\n",
      "state: [ 0.00043382 -0.00942651], action: [-7.3842806e-05  0.0000000e+00], loss: None\n",
      "Episode: 7/3000, Reward: -5.6856218150024596, done: True\n",
      "state: [ 0.00045978 -0.00923548], action: [-0.0008224  0.       ], loss: None\n",
      "Episode: 8/3000, Reward: -5.686920863157319, done: True\n",
      "state: [ 0.00039476 -0.00948811], action: [0.0001454 0.       ], loss: None\n",
      "Episode: 9/3000, Reward: -5.690940547986399, done: True\n",
      "state: [ 0.00045772 -0.00931895], action: [-0.00064298  0.        ], loss: None\n",
      "Episode: 10/3000, Reward: -5.690788713522992, done: True\n",
      "state: [ 0.00040361 -0.00947948], action: [4.7040012e-05 0.0000000e+00], loss: None\n",
      "Episode: 11/3000, Reward: -5.698819398477686, done: True\n",
      "state: [ 0.00040813 -0.00945735], action: [1.5092343e-05 0.0000000e+00], loss: None\n",
      "Episode: 12/3000, Reward: -5.686899576846141, done: True\n",
      "state: [ 0.00046438 -0.00932291], action: [-0.00084708  0.        ], loss: None\n",
      "Episode: 13/3000, Reward: -5.72264869615707, done: True\n",
      "state: [ 0.00038662 -0.00952349], action: [0.00031099 0.        ], loss: None\n",
      "Episode: 14/3000, Reward: -5.690508232136767, done: True\n",
      "state: [ 0.00043373 -0.00945088], action: [-6.272163e-05  0.000000e+00], loss: None\n",
      "Episode: 15/3000, Reward: -5.693556029658884, done: True\n",
      "state: [ 0.00015788 -0.00985219], action: [8.3214678e-03 1.5365446e-05], loss: None\n",
      "Episode: 16/3000, Reward: -5.691936896057339, done: False\n",
      "state: [ 0.00079682 -0.01933586], action: [0.00088592 0.        ], loss: None\n",
      "Episode: 17/3000, Reward: -5.720986840924601, done: False\n",
      "state: [ 0.00061536 -0.01502078], action: [0.00062469 0.        ], loss: None\n",
      "Episode: 18/3000, Reward: -5.717608813359692, done: False\n",
      "state: [ 0.00038672 -0.0270641 ], action: [0.02778923 0.00016802], loss: None\n",
      "Episode: 19/3000, Reward: -5.703657489024147, done: False\n",
      "state: [ 0.00073895 -0.01680846], action: [2.3984242e-07 0.0000000e+00], loss: None\n",
      "Episode: 20/3000, Reward: -5.697620248716696, done: True\n",
      "state: [ 0.0003893  -0.00956015], action: [0.00028962 0.        ], loss: None\n",
      "Episode: 21/3000, Reward: -5.693907813047787, done: False\n",
      "state: [ 0.00060978 -0.01462816], action: [0.00032816 0.        ], loss: None\n",
      "Episode: 22/3000, Reward: -5.702177301268614, done: True\n",
      "state: [ 0.00046255 -0.0092805 ], action: [-0.00084861  0.        ], loss: None\n",
      "Episode: 23/3000, Reward: -5.698390345777613, done: False\n",
      "state: [ 0.00119208 -0.03216463], action: [1.4222128e-02 1.2196755e-08], loss: None\n",
      "Episode: 24/3000, Reward: -5.697138727133837, done: False\n",
      "state: [ 0.00047617 -0.03113122], action: [0.03202506 0.00020372], loss: None\n",
      "Episode: 25/3000, Reward: -5.7035083456127795, done: False\n",
      "state: [ 0.00093236 -0.02234121], action: [0.00075909 0.        ], loss: None\n",
      "Episode: 26/3000, Reward: -5.753611636920226, done: False\n",
      "state: [ 0.00041743 -0.02109558], action: [1.9475872e-02 5.3417356e-05], loss: None\n",
      "Episode: 27/3000, Reward: -5.832241483456578, done: False\n",
      "state: [ 0.00051611 -0.02196322], action: [1.8981395e-02 3.4489058e-05], loss: None\n",
      "Episode: 28/3000, Reward: -5.7108155726328445, done: False\n",
      "state: [ 0.00104287 -0.03319311], action: [2.4787474e-02 1.5008363e-05], loss: None\n",
      "Episode: 29/3000, Reward: -5.785406427756364, done: False\n",
      "state: [ 0.00060076 -0.02393978], action: [2.0283623e-02 3.2091473e-05], loss: None\n",
      "Episode: 30/3000, Reward: -5.72055998835245, done: False\n",
      "state: [ 0.00142524 -0.04455013], action: [-0.08143398 -0.00164948], loss: None\n",
      "Episode: 31/3000, Reward: -5.7075316064778, done: False\n",
      "state: [ 0.00120003 -0.03231664], action: [1.4157399e-02 1.1758518e-08], loss: None\n",
      "Episode: 32/3000, Reward: -5.6925810232972776, done: False\n",
      "state: [-0.00078535 -0.03049117], action: [-0.08189761 -0.00165556], loss: None\n",
      "Episode: 33/3000, Reward: -5.77188793539541, done: False\n",
      "state: [ 0.00079936 -0.0279251 ], action: [2.2148296e-02 2.2218730e-05], loss: None\n",
      "Episode: 34/3000, Reward: -5.710714303864665, done: False\n",
      "state: [ 0.0008642  -0.02817122], action: [2.0658681e-02 1.2723517e-05], loss: None\n",
      "Episode: 35/3000, Reward: -5.757528571113698, done: False\n",
      "state: [ 0.00115862 -0.02837694], action: [0.00266874 0.        ], loss: None\n",
      "Episode: 36/3000, Reward: -5.717723608351984, done: False\n",
      "state: [ 0.00107199 -0.03359743], action: [2.4613719e-02 1.2854612e-05], loss: None\n",
      "Episode: 37/3000, Reward: -5.697950657909587, done: False\n",
      "state: [ 0.0010969  -0.03152328], action: [1.8476203e-02 6.6396251e-07], loss: None\n",
      "Episode: 38/3000, Reward: -5.7189891555914665, done: False\n",
      "state: [ 0.00106505 -0.03632776], action: [3.0028347e-02 3.4846918e-05], loss: None\n",
      "Episode: 39/3000, Reward: -5.708556193740002, done: False\n",
      "state: [ 0.00183508 -0.05078742], action: [3.2981221e-02 2.6427873e-07], loss: None\n",
      "Episode: 40/3000, Reward: -5.72793057303544, done: False\n",
      "state: [ 0.00108138 -0.05251491], action: [-0.08126853 -0.00165319], loss: None\n",
      "Episode: 41/3000, Reward: -5.718137443924283, done: False\n",
      "state: [ 0.00039184 -0.02975786], action: [0.03117003 0.00022827], loss: None\n",
      "Episode: 42/3000, Reward: -5.705031736302762, done: False\n",
      "state: [ 0.0001031  -0.03052828], action: [0.03477377 0.00053151], loss: None\n",
      "Episode: 43/3000, Reward: -5.720432423373478, done: False\n",
      "state: [ 0.00110761 -0.02809793], action: [5.6143138e-03 2.0610635e-09], loss: None\n",
      "Episode: 44/3000, Reward: -5.774155903474726, done: False\n",
      "state: [ 0.00037386 -0.0397188 ], action: [0.04380256 0.00057704], loss: None\n",
      "Episode: 45/3000, Reward: -5.721225571763353, done: False\n",
      "state: [ 0.00018966 -0.03051139], action: [-0.08181268 -0.00165309], loss: None\n",
      "Episode: 46/3000, Reward: -5.777556412287917, done: False\n",
      "state: [ 0.00158646 -0.04057443], action: [1.3494651e-02 6.6038717e-09], loss: None\n",
      "Episode: 47/3000, Reward: -5.719088674366736, done: False\n",
      "state: [ 0.00102854 -0.03093654], action: [2.0354845e-02 5.0751432e-06], loss: None\n",
      "Episode: 48/3000, Reward: -5.7348715622812465, done: False\n",
      "state: [ 0.00123845 -0.05290824], action: [0.05186946 0.00023485], loss: None\n",
      "Episode: 49/3000, Reward: -5.75641121429337, done: False\n",
      "state: [ 0.001118   -0.02439944], action: [-0.00044916  0.        ], loss: None\n",
      "Episode: 50/3000, Reward: -5.723783052290542, done: False\n",
      "state: [ 0.00176981 -0.05018278], action: [-0.08126938 -0.00165074], loss: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuxinji\\AppData\\Local\\Temp\\ipykernel_20016\\1426376404.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor([e[0] for e in minibatch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51/3000, Reward: -5.697804946007159, done: False\n",
      "state: [ 0.00047954 -0.01511557], action: [7.7906307e-03 1.1287560e-06], loss: 5.440303084469633e-06\n",
      "Episode: 52/3000, Reward: -5.688273172407396, done: False\n",
      "state: [ 0.00060438 -0.02015514], action: [1.3538405e-02 6.6164589e-06], loss: 2.9359673590079183e-06\n",
      "Episode: 53/3000, Reward: -5.747881299508355, done: False\n",
      "state: [ 0.00017552 -0.01850214], action: [-0.02601359 -0.00287534], loss: 1.3305698303156532e-06\n",
      "Episode: 54/3000, Reward: -5.743828785311742, done: False\n",
      "state: [ 0.00025797 -0.01411122], action: [-0.02448335 -0.00371303], loss: 7.929840535325638e-07\n",
      "Episode: 55/3000, Reward: -5.726282134912097, done: False\n",
      "state: [ 0.00034632 -0.01325198], action: [-0.02318714 -0.00435577], loss: 4.82688847114332e-07\n",
      "Episode: 56/3000, Reward: -5.715641372808681, done: False\n",
      "state: [ 0.00052301 -0.0126873 ], action: [0.00036337 0.        ], loss: 3.0846405252304976e-07\n",
      "Episode: 57/3000, Reward: -5.705671387921439, done: False\n",
      "state: [ 0.0003581  -0.01397198], action: [9.958677e-03 8.041508e-06], loss: 1.7086054526771477e-07\n",
      "Episode: 58/3000, Reward: -5.69006111017, done: False\n",
      "state: [ 0.00024823 -0.01058424], action: [7.389936e-03 6.148849e-06], loss: 5.156895440450171e-06\n",
      "Episode: 59/3000, Reward: -5.68174244548511, done: True\n",
      "state: [ 0.00030384 -0.00969718], action: [0.00385308 0.        ], loss: 2.538983210342849e-07\n",
      "Episode: 60/3000, Reward: -5.673650763387651, done: True\n",
      "state: [ 0.00028045 -0.00981072], action: [0.00501935 0.        ], loss: 1.1549842326985527e-07\n",
      "Episode: 61/3000, Reward: -5.68955695972803, done: True\n",
      "state: [ 0.00037561 -0.00966673], action: [0.00076313 0.        ], loss: 5.238969151832862e-06\n",
      "Episode: 62/3000, Reward: -5.695069417870659, done: False\n",
      "state: [ 0.00024306 -0.01202399], action: [9.7114732e-03 1.4189409e-05], loss: 6.278587534325197e-07\n",
      "Episode: 63/3000, Reward: -5.680055289777385, done: True\n",
      "state: [ 0.00038086 -0.00962496], action: [0.00055457 0.        ], loss: 1.8400092471893004e-07\n",
      "Episode: 64/3000, Reward: -5.699172835019603, done: False\n",
      "state: [ 0.00067146 -0.01887303], action: [0.00711639 0.        ], loss: 1.4229907208118675e-07\n",
      "Episode: 65/3000, Reward: -5.670232390012691, done: False\n",
      "state: [ 0.00024984 -0.01000903], action: [-0.01595369 -0.00506667], loss: 2.866851787075575e-07\n",
      "Episode: 66/3000, Reward: -5.684226066754024, done: False\n",
      "state: [ 0.00051258 -0.01537183], action: [0.00678807 0.        ], loss: 1.4606170850584022e-07\n",
      "Episode: 67/3000, Reward: -5.687079382930409, done: False\n",
      "state: [ 0.0002801  -0.01513979], action: [1.34498915e-02 3.01841101e-05], loss: 2.5112424850703974e-07\n",
      "Episode: 68/3000, Reward: -5.67895709248227, done: False\n",
      "state: [ 0.00026758 -0.01043244], action: [0.00658004 0.        ], loss: 2.335875933567877e-06\n",
      "Episode: 69/3000, Reward: -5.699754452558564, done: False\n",
      "state: [ 0.00040313 -0.01250454], action: [-0.01331192 -0.00500243], loss: 1.6596161458437564e-06\n",
      "Episode: 70/3000, Reward: -5.686084708540796, done: False\n",
      "state: [ 0.0004245  -0.01423258], action: [8.2198428e-03 2.5696745e-06], loss: 1.928025170627734e-07\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(memory_size)\n",
    "epsilon = 1.0\n",
    "# Set up the environment\n",
    "env = cstr_env(order=1)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize networks and optimizer\n",
    "naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network.load_state_dict(naf_network.state_dict())\n",
    "target_naf_network.eval()\n",
    "\n",
    "optimizer = optim.Adam(naf_network.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    states_list = []\n",
    "    state = env.reset(e)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = get_action(e, state, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        states_list.append(state)\n",
    "        loss = train_model(e)\n",
    "        if step == 1000:\n",
    "            break\n",
    "            \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    if e%5 == 0:\n",
    "        plot_state(states_list, e)\n",
    "    print(f\"Episode: {e+1}/{num_episodes}, Reward: {total_reward}, done: {done}\")\n",
    "    print(f\"state: {state}, action: {action}, loss: {loss}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96313b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
