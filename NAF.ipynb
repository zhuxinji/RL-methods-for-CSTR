{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ac5833c-5ded-406e-ac52-41350c82cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b7629042-5a6e-479c-b7c0-88305ec82066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for NAF\n",
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "        self.fc_mu = nn.Linear(256, action_size)\n",
    "        self.fc_l = nn.Linear(256, action_size * action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.fc_value(x)\n",
    "        mu = torch.tanh(self.fc_mu(x))*torch.tensor([1, 0.167])\n",
    "        l = self.fc_l(x)\n",
    "\n",
    "        l_matrix = l.view(-1, action_size, action_size)\n",
    "        l_matrix = torch.tril(l_matrix, -1) + torch.diag_embed(torch.exp(torch.diagonal(l_matrix, dim1=-2, dim2=-1)))\n",
    "        p_matrix = torch.bmm(l_matrix, l_matrix.transpose(2, 1))\n",
    "\n",
    "        return value, mu, p_matrix\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        value, mu, p_matrix = self.forward(state)\n",
    "        action_diff = action - mu\n",
    "        advantage = -0.5 * torch.bmm(action_diff.unsqueeze(1), torch.bmm(p_matrix, action_diff.unsqueeze(2))).squeeze(2)\n",
    "        q_value = value + advantage\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f277494d-0c7f-441e-8e6f-daa020fa580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8bf41011-f469-4b54-a33e-888854179614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    if len(memory) < batch_size*50:\n",
    "        return\n",
    "\n",
    "    minibatch = memory.sample(batch_size)\n",
    "    states = torch.FloatTensor([e[0] for e in minibatch])\n",
    "    actions = torch.FloatTensor([e[1] for e in minibatch])\n",
    "    rewards = torch.FloatTensor([e[2] for e in minibatch])\n",
    "    next_states = torch.FloatTensor([e[3] for e in minibatch])\n",
    "    dones = torch.FloatTensor([e[4] for e in minibatch])\n",
    "\n",
    "    q_values = naf_network.q_value(states, actions)\n",
    "    next_actions = target_naf_network(next_states)[1]\n",
    "    next_q_values = target_naf_network.q_value(next_states, next_actions)\n",
    "    target_q_values = rewards.unsqueeze(1) + (1 - dones).unsqueeze(1) * discount_factor * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_naf_network.parameters(), naf_network.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fbc1d7d-3a4b-4c0a-8b48-583b9db8785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    _, mu, _ = naf_network(state)\n",
    "    return mu.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f4ae35a-d5ee-49e3-8955-e623fe4171e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cstr_env(gym.Env):\n",
    "\n",
    "    def __init__(self):  \n",
    "        self.action_space = spaces.Box(low = np.array([-1.0, -0.167], dtype=np.float32), \n",
    "                                       high = np.array([1.0 , 0.167], dtype=np.float32), \n",
    "                                       dtype=np.float32, shape=(2, ))   \n",
    "        self.observation_space = spaces.Box(low=np.array([-1.0, -1.0], dtype=np.float32), \n",
    "                                            high=np.array([1.0, 1.0], dtype=np.float32), \n",
    "                                            dtype=np.float32, shape=(2, ))\n",
    "        self.n_episode = 0 # current episode number.\n",
    "\n",
    "        \n",
    "    def is_done(self, x_next):\n",
    "        done = False\n",
    "        c1 = (abs(x_next[0] - self.setpoint_states[0]) < 0.01)\n",
    "        c2 = (abs(x_next[1] - self.setpoint_states[1]) < 0.01)\n",
    "        steady_state = c1 and c2  \n",
    "\n",
    "        # Record the steady state status for the current step\n",
    "        self.goal_state_done[self.ep_step] = steady_state\n",
    "        \n",
    "        if self.ep_step > 3: \n",
    "            p3 = self.goal_state_done[self.ep_step-2] \n",
    "            p2 = self.goal_state_done[self.ep_step-1] \n",
    "            p1 = self.goal_state_done[self.ep_step-0] \n",
    "            # If the last three steps were steady states, set 'done' to True\n",
    "            if  p3 and p2 and p1:\n",
    "                done = True  \n",
    "        return done \n",
    "\n",
    "    \n",
    "    def get_dx(self, x, u):\n",
    "        params = [0.5734, 395.3268, 100e-3, 0.1, 72e+9, 8.314e+4, 8.314, 310, -4.78e+4, 0.239, 1000, 1]\n",
    "        CAs, Ts, CF, CV, Ck0, CE, CR, CT0, CDh, Ccp, Crho, CA0s = params                   \n",
    "        g1, g2 = CF/CV, 1/(Crho*Ccp*CV)\n",
    "        x1, x2 = x[0], x[1]\n",
    "        \n",
    "        f1 = (CF/CV)*(-x1) - Ck0*np.exp(-CE/(CR*(x2+Ts))) * (x1+CAs)+(CF/CV) * (CA0s-CAs)\n",
    "        f2 = (CF/CV)*(-x2) + (-CDh/(Crho*Ccp))*Ck0*np.exp(-CE/(CR*(x2+Ts)))*(x1+CAs) + CF*(CT0-Ts)/CV\n",
    "        dx = [f1, f2] + u*[g1, g2]\n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        dt = 5e-3\n",
    "        self.current_u = action\n",
    "        state = self.current_s\n",
    "        x_next = self.current_s + dt*self.get_dx(self.current_s, action)\n",
    "        done = self.is_done(x_next) \n",
    "        reward = -np.sum((x_next - self.setpoint_states)**2)*0.001 + np.sum((self.current_u - self.setpoint_actions)**2)  \n",
    " \n",
    "        self.previous_u = self.current_u \n",
    "        self.current_s = x_next \n",
    "        self.ep_step += 1  \n",
    "\n",
    "        # this is the trancated condition. \n",
    "        truncated = False \n",
    "        if self.ep_step == episode_length:\n",
    "            truncated = True\n",
    "\n",
    "        if self.ep_step == episode_length-1 or done:      \n",
    "            self.n_episode += 1 \n",
    "        \n",
    "        # if done is true i.e. terminated is equal to done. \n",
    "        terminated = done\n",
    "\n",
    "        return x_next, reward, done\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.ep_step = 0 \n",
    "        self.current_u= None \n",
    "        self.previous_u = None \n",
    "        self.current_s = None \n",
    "\n",
    "        ## list of true false which stores the weather the state is near to the goal state or not. \n",
    "        self.goal_state_done = [False] * (episode_length+5)\n",
    "\n",
    "        self.setpoint_states  =  np.array([.0, .0], dtype=float)     \n",
    "        self.setpoint_actions =  np.array([.0, .0], dtype=float)\n",
    "\n",
    "        # this is the fixed initial state. \n",
    "        state, action = np.array([0.2, -5]),  np.array([1.5, 0.1]) \n",
    "        self.current_u = action \n",
    "        self.previous_u = action  \n",
    "        self.current_s = state  \n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a9fca-f5d2-4893-9f9f-d979e9d848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_range_min = np.array([-0.3, - 10.], dtype=float)   \n",
    "act_range_max = np.array([0.3, 10.], dtype=float)  \n",
    "\n",
    "transform_range_min = np.array([-1., -1.], dtype=float)  \n",
    "transform_range_max = np.array([1., 1.], dtype=float)   \n",
    "\n",
    "\n",
    "def normalize_minmax_states(X:np.ndarray):\n",
    "    global act_range_min, act_range_max, transform_range_min, transform_range_max\n",
    "    X_std = (X-act_range_min) / (act_range_max - act_range_min)\n",
    "    X_scaled = X_std * (transform_range_max - transform_range_min) +  transform_range_min \n",
    "    return X_scaled\n",
    "\n",
    "def normalize_minmax_actions(X:np.ndarray):\n",
    "    global action_min_r, action_max_r, transform_action_min_r, transform_action_max_r\n",
    "    X_std = (X-action_min_r) / (action_max_r - action_min_r) \n",
    "    X_scaled = X_std * (transform_action_max_r - transform_action_min_r) +  transform_action_min_r \n",
    "    return X_scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9640bf2-ffde-400c-bd69-955b7dfb1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "discount_factor = 0.99\n",
    "batch_size = 256\n",
    "tau = 0.001\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "memory_size = 1000000\n",
    "num_episodes = 3000\n",
    "episode_length = num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b119a2-3554-4cd6-acc1-0d4515cf9e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/3000, Reward: -664.512769228849, done: False\n",
      "state: [  0.33312334 -68.92599754], action: [ 0.67721474 -0.07768483]\n",
      "Episode: 2/3000, Reward: -184.71565291878204, done: False\n",
      "state: [  0.30610687 -56.35970204], action: [-0.07583017 -0.01099732]\n",
      "Episode: 3/3000, Reward: -560.553700130056, done: False\n",
      "state: [  0.31218187 -67.11209554], action: [-0.80822414  0.15656276]\n",
      "Episode: 4/3000, Reward: -450.7762442562561, done: False\n",
      "state: [  0.27801677 -65.00621384], action: [-0.6415193   0.06660744]\n",
      "Episode: 5/3000, Reward: -53.67134357224961, done: False\n",
      "state: [  0.26893431 -49.79229419], action: [-0.45210996 -0.1665418 ]\n",
      "Episode: 6/3000, Reward: -454.3019320540891, done: False\n",
      "state: [  0.36296254 -64.57412213], action: [0.33942252 0.11186812]\n",
      "Episode: 7/3000, Reward: -711.3744943992366, done: False\n",
      "state: [  0.30559325 -69.81920694], action: [0.33654284 0.04149591]\n",
      "Episode: 8/3000, Reward: -303.7670132604994, done: False\n",
      "state: [  0.31167618 -61.0058809 ], action: [-0.6516683  -0.11194731]\n",
      "Episode: 9/3000, Reward: -3166.017669606878, done: False\n",
      "state: [ -0.56712781 108.34074222], action: [ 0.21018541 -0.15608667]\n",
      "Episode: 10/3000, Reward: -117.7701280791402, done: False\n",
      "state: [  0.20353883 -53.72417198], action: [-0.08604248 -0.06716721]\n",
      "Episode: 11/3000, Reward: -76.66463455976371, done: False\n",
      "state: [  0.28133318 -50.90718534], action: [-0.07681642 -0.0298627 ]\n",
      "Episode: 12/3000, Reward: 153.28156943170666, done: False\n",
      "state: [  0.0763945  -15.93927763], action: [0.42909878 0.11431816]\n",
      "Episode: 13/3000, Reward: -506.13235101355184, done: False\n",
      "state: [  0.34539676 -65.81248392], action: [ 0.15467122 -0.00877659]\n",
      "Episode: 14/3000, Reward: -511.7621003220268, done: False\n",
      "state: [  0.32715123 -65.90484782], action: [-0.926372  -0.0098064]\n",
      "Episode: 15/3000, Reward: -579.3097775988232, done: False\n",
      "state: [  0.33585894 -67.3770141 ], action: [-0.8389308   0.09794937]\n",
      "Episode: 16/3000, Reward: -527.3450136430278, done: False\n",
      "state: [  0.32729658 -66.25526087], action: [-0.65652287 -0.12951243]\n",
      "Episode: 17/3000, Reward: -88.15774747285697, done: False\n",
      "state: [  0.23882796 -51.60670984], action: [ 0.45424196 -0.04821984]\n",
      "Episode: 18/3000, Reward: -552.3546547457295, done: False\n",
      "state: [  0.30631909 -66.58113303], action: [0.824553  0.1217533]\n",
      "Episode: 19/3000, Reward: -214.81583044438088, done: False\n",
      "state: [  0.29871513 -57.37153026], action: [0.2775025 0.0125451]\n",
      "Episode: 20/3000, Reward: -288.51875211954393, done: False\n",
      "state: [  0.30830687 -60.23363394], action: [-0.52369213 -0.1668885 ]\n",
      "Episode: 21/3000, Reward: 24.74835845199087, done: False\n",
      "state: [  0.16745422 -43.05003481], action: [-0.91997397  0.0757878 ]\n",
      "Episode: 22/3000, Reward: -281.0629430536519, done: False\n",
      "state: [  0.250532   -59.74022629], action: [-0.88122857 -0.06787062]\n",
      "Episode: 23/3000, Reward: -520.2855865938268, done: False\n",
      "state: [  0.28701262 -66.11956915], action: [-0.7005569  -0.03465302]\n",
      "Episode: 24/3000, Reward: -496.112755325669, done: False\n",
      "state: [  0.2871744  -65.47572302], action: [-0.55427057 -0.16694503]\n",
      "Episode: 25/3000, Reward: -269.84050695220867, done: False\n",
      "state: [  0.33701842 -59.22290023], action: [0.5710527  0.00786621]\n",
      "Episode: 26/3000, Reward: -3013.9891739073832, done: False\n",
      "state: [ -0.5702427  103.22087938], action: [-0.57567096 -0.02712446]\n",
      "Episode: 27/3000, Reward: -3420.2179785380754, done: False\n",
      "state: [-0.56118223 93.01443439], action: [ 0.56315017 -0.07841513]\n",
      "Episode: 28/3000, Reward: -2894.2873574152445, done: False\n",
      "state: [ -0.56729457 101.67095422], action: [-0.22260505 -0.07406581]\n",
      "Episode: 29/3000, Reward: -238.21254220116072, done: False\n",
      "state: [  0.38508367 -58.78800608], action: [ 0.73549885 -0.12245775]\n",
      "Episode: 30/3000, Reward: -3923.5875855774334, done: False\n",
      "state: [ -0.56926116 102.97912982], action: [-0.73575896 -0.10375049]\n",
      "Episode: 31/3000, Reward: -2693.366807853782, done: False\n",
      "state: [ -0.56891753 101.63850947], action: [-0.5901926   0.13955195]\n",
      "Episode: 32/3000, Reward: -2668.415175624938, done: False\n",
      "state: [-0.56316766 94.45850893], action: [ 0.5589733  -0.13903381]\n",
      "Episode: 33/3000, Reward: -3365.368706388972, done: False\n",
      "state: [-0.56239838 99.16574508], action: [ 0.7697294  -0.02826263]\n",
      "Episode: 34/3000, Reward: -411.40165569904923, done: False\n",
      "state: [  0.46523646 -64.09103473], action: [ 0.08403307 -0.12684393]\n",
      "Episode: 35/3000, Reward: -405.31512379294736, done: False\n",
      "state: [  0.4589777  -63.90818285], action: [-0.1906292   0.03571423]\n",
      "Episode: 36/3000, Reward: -1399.7691799665224, done: False\n",
      "state: [ -0.56810759 105.18930647], action: [-0.10267484 -0.06599972]\n",
      "Episode: 37/3000, Reward: -2893.8212139674033, done: False\n",
      "state: [ -0.57353384 120.38085147], action: [-0.5405248   0.16053264]\n",
      "Episode: 38/3000, Reward: -3087.2317593669763, done: False\n",
      "state: [ -0.56731664 101.27727072], action: [-0.02414785  0.03374251]\n",
      "Episode: 39/3000, Reward: -3085.1269442804123, done: False\n",
      "state: [ -0.56448077 105.46083201], action: [ 0.59518063 -0.07575821]\n",
      "Episode: 40/3000, Reward: -475.102067133388, done: False\n",
      "state: [  0.46228372 -65.41217858], action: [ 0.7344397  -0.15738572]\n",
      "Episode: 41/3000, Reward: -508.498798958403, done: False\n",
      "state: [  0.45116555 -65.89291906], action: [0.7956152  0.01157315]\n",
      "Episode: 42/3000, Reward: -3242.5730211035725, done: False\n",
      "state: [ -0.56840003 123.01380154], action: [0.21548334 0.03350031]\n",
      "Episode: 43/3000, Reward: -395.7905418935353, done: False\n",
      "state: [  0.46927821 -63.04296279], action: [-0.503785   0.0835738]\n",
      "Episode: 44/3000, Reward: -197.5386009302527, done: False\n",
      "state: [  0.38291994 -56.52672743], action: [ 0.48766145 -0.15929748]\n",
      "Episode: 45/3000, Reward: -281.9594060062059, done: False\n",
      "state: [  0.42169549 -60.25147528], action: [-0.49564597  0.15466866]\n",
      "Episode: 46/3000, Reward: -4865.433207468205, done: False\n",
      "state: [-0.28982043 91.56593122], action: [-0.79116565  0.04681169]\n",
      "Episode: 47/3000, Reward: -3255.511372915219, done: False\n",
      "state: [  0.66646569 -89.62288527], action: [-0.8285821  -0.11462871]\n",
      "Episode: 48/3000, Reward: -11457.192387164698, done: False\n",
      "state: [   0.83627159 -141.0994269 ], action: [-0.62503093 -0.04188954]\n",
      "Episode: 49/3000, Reward: -3761.100711434002, done: False\n",
      "state: [ -0.52640783 124.58663897], action: [-0.8600415   0.11975387]\n",
      "Episode: 50/3000, Reward: -264.7907738540776, done: False\n",
      "state: [  0.24191353 -58.66833058], action: [-0.9468354  -0.16699298]\n",
      "Episode: 51/3000, Reward: -397.99575719856216, done: False\n",
      "state: [  0.14797154 -64.37839428], action: [0.0984341  0.11889631]\n",
      "Episode: 52/3000, Reward: -651.1859913475386, done: False\n",
      "state: [  0.16843344 -70.10670532], action: [ 0.7767162  -0.12075017]\n",
      "Episode: 53/3000, Reward: -757.9283963397756, done: False\n",
      "state: [  0.14946834 -71.74981121], action: [0.74084234 0.14498538]\n",
      "Episode: 54/3000, Reward: -664.5333448089398, done: False\n",
      "state: [  0.12791915 -70.45717981], action: [-0.02044263  0.0333398 ]\n",
      "Episode: 55/3000, Reward: -753.9793674253776, done: False\n",
      "state: [  0.15134851 -71.70287465], action: [-1.         -0.16699944]\n",
      "Episode: 56/3000, Reward: -698.3355695378401, done: False\n",
      "state: [  0.12530279 -70.91444992], action: [-0.34262967  0.03365345]\n",
      "Episode: 57/3000, Reward: -730.6446745293356, done: False\n",
      "state: [  0.20111001 -71.43383185], action: [-1.         -0.16699669]\n",
      "Episode: 58/3000, Reward: -755.8515223425993, done: False\n",
      "state: [  0.16444088 -71.83808554], action: [-0.32220775  0.15586355]\n",
      "Episode: 59/3000, Reward: -623.3857104572588, done: False\n",
      "state: [  0.16281585 -70.05471333], action: [-1.         -0.16697092]\n",
      "Episode: 60/3000, Reward: -784.1497900728441, done: False\n",
      "state: [  0.14007372 -72.27351162], action: [-0.78810155  0.15593807]\n",
      "Episode: 61/3000, Reward: -733.1686250923747, done: False\n",
      "state: [  0.11631257 -71.68984591], action: [-0.3132101  -0.01037245]\n",
      "Episode: 62/3000, Reward: -767.6816076135007, done: False\n",
      "state: [  0.14276776 -72.16604233], action: [0.92555785 0.16697434]\n",
      "Episode: 63/3000, Reward: -617.3517792599623, done: False\n",
      "state: [  0.13592709 -69.90833249], action: [-0.5683573   0.07934983]\n",
      "Episode: 64/3000, Reward: -774.6571531277106, done: False\n",
      "state: [  0.10162356 -72.27940867], action: [ 0.5632543  -0.15395346]\n",
      "Episode: 65/3000, Reward: -771.0371080336753, done: False\n",
      "state: [  0.16570566 -71.95509886], action: [ 0.7005972  -0.04859337]\n",
      "Episode: 66/3000, Reward: -861.4966960664324, done: False\n",
      "state: [  0.14275475 -73.32783759], action: [-1.        -0.1669966]\n",
      "Episode: 67/3000, Reward: -709.6808529307084, done: False\n",
      "state: [  0.16200889 -71.31222345], action: [0.1315666  0.04124769]\n",
      "Episode: 68/3000, Reward: -664.8493694369939, done: False\n",
      "state: [  0.11108121 -70.80707287], action: [ 0.07836358 -0.07914518]\n",
      "Episode: 69/3000, Reward: -790.2049838258075, done: False\n",
      "state: [  0.10641279 -72.3503727 ], action: [ 0.17558512 -0.02779451]\n",
      "Episode: 70/3000, Reward: -774.9227984206664, done: False\n",
      "state: [ 2.81798132e-02 -7.24619170e+01], action: [-1.         -0.16699998]\n",
      "Episode: 71/3000, Reward: -745.5371176576807, done: False\n",
      "state: [  0.11379424 -71.9700733 ], action: [0.6817718  0.14738382]\n",
      "Episode: 72/3000, Reward: -740.9051741050031, done: False\n",
      "state: [ 6.64241772e-02 -7.19274155e+01], action: [-0.3389022   0.12071205]\n",
      "Episode: 73/3000, Reward: -711.6567739363633, done: False\n",
      "state: [  0.09542618 -71.44798841], action: [-1.         -0.16699998]\n",
      "Episode: 74/3000, Reward: -658.5978787357799, done: False\n",
      "state: [ 4.46688156e-02 -7.06193249e+01], action: [-0.17536539 -0.08865234]\n",
      "Episode: 75/3000, Reward: -1547.0117082685292, done: False\n",
      "state: [  0.37737027 -77.96275188], action: [0.34270233 0.11282493]\n",
      "Episode: 76/3000, Reward: -3166.763622969971, done: False\n",
      "state: [-0.54975339 59.87758447], action: [-0.73721075  0.07930105]\n",
      "Episode: 77/3000, Reward: -2388.0940426372954, done: False\n",
      "state: [-0.53972125 55.75754924], action: [-0.7735478   0.00141766]\n",
      "Episode: 78/3000, Reward: -2626.542881665795, done: False\n",
      "state: [-0.53188878 64.18406724], action: [ 0.62143314 -0.02943889]\n",
      "Episode: 79/3000, Reward: -2624.4486373905643, done: False\n",
      "state: [-0.55948076 66.50951025], action: [-0.9960159   0.10406822]\n",
      "Episode: 80/3000, Reward: -3612.646057221487, done: False\n",
      "state: [-0.55842736 80.45161729], action: [ 0.18364634 -0.14682324]\n",
      "Episode: 81/3000, Reward: -2819.3891542430674, done: False\n",
      "state: [-0.52629931 46.69303292], action: [-1.          0.16690566]\n",
      "Episode: 82/3000, Reward: -1959.6749581934268, done: False\n",
      "state: [-0.53333341 51.79956618], action: [-1.          0.16697945]\n",
      "Episode: 83/3000, Reward: -2226.5573546650335, done: False\n",
      "state: [-0.502632   52.40536639], action: [ 0.7598687  -0.02047452]\n",
      "Episode: 84/3000, Reward: -2715.3928388327563, done: False\n",
      "state: [-0.52680734 50.88260407], action: [ 0.8936437  -0.11677634]\n",
      "Episode: 85/3000, Reward: -2030.3031280729301, done: False\n",
      "state: [-0.5323498  50.13149875], action: [-0.36935398  0.06307621]\n",
      "Episode: 86/3000, Reward: -1913.044805330645, done: False\n",
      "state: [-0.51218348 42.26466457], action: [ 0.28786647 -0.06302302]\n",
      "Episode: 87/3000, Reward: -2790.778307697817, done: False\n",
      "state: [-0.54310243 52.46560835], action: [-0.88912296  0.14818458]\n",
      "Episode: 88/3000, Reward: -644.8418737157756, done: False\n",
      "state: [ 6.01905297e-02 -7.05997363e+01], action: [-1.    -0.167]\n",
      "Episode: 89/3000, Reward: -699.9315519247928, done: False\n",
      "state: [ 4.86821255e-02 -7.15203519e+01], action: [-0.05011201  0.02140833]\n",
      "Episode: 90/3000, Reward: -812.0149024223746, done: False\n",
      "state: [ 3.67721442e-02 -7.31204661e+01], action: [-1.    -0.167]\n",
      "Episode: 91/3000, Reward: -720.7484969650639, done: False\n",
      "state: [  0.09606298 -71.81666724], action: [-0.30873978  0.14008784]\n",
      "Episode: 92/3000, Reward: -792.0654085789841, done: False\n",
      "state: [  0.09192831 -72.75905245], action: [ 0.12727085 -0.12093219]\n",
      "Episode: 93/3000, Reward: -785.5544530824114, done: False\n",
      "state: [  0.09252336 -72.59128416], action: [-1.    -0.167]\n",
      "Episode: 94/3000, Reward: -806.577878082304, done: False\n",
      "state: [ 3.87558361e-02 -7.29517358e+01], action: [-0.8447178   0.12107972]\n",
      "Episode: 95/3000, Reward: -784.9993631156998, done: False\n",
      "state: [ 5.02166988e-02 -7.26626552e+01], action: [-1.    -0.167]\n",
      "Episode: 96/3000, Reward: -824.1911132501579, done: False\n",
      "state: [ 2.08254540e-02 -7.32899274e+01], action: [-0.33004457  0.11169591]\n",
      "Episode: 97/3000, Reward: -767.6082836852903, done: False\n",
      "state: [ 1.72378560e-02 -7.26843917e+01], action: [0.7166001  0.08323262]\n",
      "Episode: 98/3000, Reward: -776.4464005344892, done: False\n",
      "state: [ 2.92979270e-02 -7.27131217e+01], action: [ 0.43413857 -0.04365927]\n",
      "Episode: 99/3000, Reward: -719.156132599291, done: False\n",
      "state: [-8.29218885e-04 -7.22715613e+01], action: [-0.37555596  0.05503739]\n",
      "Episode: 100/3000, Reward: -839.922544695519, done: False\n",
      "state: [ 3.02473809e-02 -7.34915143e+01], action: [-0.75376654 -0.07717437]\n",
      "Episode: 101/3000, Reward: -830.0047165832616, done: False\n",
      "state: [ 3.92382962e-02 -7.33472775e+01], action: [-0.32645112  0.11204974]\n",
      "Episode: 102/3000, Reward: -755.0404498545379, done: False\n",
      "state: [ 7.03253473e-02 -7.26532115e+01], action: [-1.    -0.167]\n",
      "Episode: 103/3000, Reward: -745.7064833753351, done: False\n",
      "state: [ 8.89446052e-03 -7.26313607e+01], action: [-0.66347146  0.08827256]\n",
      "Episode: 104/3000, Reward: -825.4508740484654, done: False\n",
      "state: [ 3.58399917e-02 -7.34540344e+01], action: [-1.    -0.167]\n",
      "Episode: 105/3000, Reward: -789.6985363883931, done: False\n",
      "state: [ 8.40215697e-03 -7.29394957e+01], action: [-0.4313538   0.10653593]\n",
      "Episode: 106/3000, Reward: -770.9726893884178, done: False\n",
      "state: [ 1.41493330e-02 -7.28428187e+01], action: [-0.09113326  0.08964097]\n",
      "Episode: 107/3000, Reward: -760.3945582133452, done: False\n",
      "state: [ 2.18696712e-02 -7.26054201e+01], action: [-1.    -0.167]\n",
      "Episode: 108/3000, Reward: -745.4466177645112, done: False\n",
      "state: [ 2.59916078e-02 -7.22856182e+01], action: [0.9033237  0.00924251]\n",
      "Episode: 109/3000, Reward: -801.5676020123918, done: False\n",
      "state: [-1.44724873e-02 -7.32221664e+01], action: [-1.    -0.167]\n",
      "Episode: 110/3000, Reward: -778.6819623598432, done: False\n",
      "state: [-1.86006709e-02 -7.28306846e+01], action: [-1.    -0.167]\n",
      "Episode: 111/3000, Reward: -745.7565634741436, done: False\n",
      "state: [-2.17985409e-02 -7.26201464e+01], action: [-1.    -0.167]\n",
      "Episode: 112/3000, Reward: -771.9439707131759, done: False\n",
      "state: [-3.83049776e-03 -7.27990534e+01], action: [ 0.43453944 -0.05448537]\n",
      "Episode: 113/3000, Reward: -786.4234507819657, done: False\n",
      "state: [-2.84051518e-02 -7.32262061e+01], action: [-0.26286525  0.09588272]\n",
      "Episode: 114/3000, Reward: -818.533979247907, done: False\n",
      "state: [-2.35429172e-02 -7.35028341e+01], action: [-1.    -0.167]\n",
      "Episode: 115/3000, Reward: -766.6438408354868, done: False\n",
      "state: [-5.21222246e-02 -7.33037746e+01], action: [-0.28438112  0.00700605]\n",
      "Episode: 116/3000, Reward: -815.898164294504, done: False\n",
      "state: [-1.62691878e-02 -7.35531213e+01], action: [-1.    -0.167]\n",
      "Episode: 117/3000, Reward: -797.5348172715723, done: False\n",
      "state: [ 1.52173012e-02 -7.33373202e+01], action: [0.43104613 0.08390091]\n",
      "Episode: 118/3000, Reward: -847.9152912506904, done: False\n",
      "state: [ -0.07931937 -73.94499358], action: [0.62369215 0.04778579]\n",
      "Episode: 119/3000, Reward: -782.6601203490495, done: False\n",
      "state: [-3.33971027e-02 -7.31997258e+01], action: [-1.    -0.167]\n",
      "Episode: 120/3000, Reward: -732.3176159561814, done: False\n",
      "state: [-6.67097614e-02 -7.26839467e+01], action: [-0.5489132   0.16232277]\n",
      "Episode: 121/3000, Reward: -800.1905023951863, done: False\n",
      "state: [-1.82915911e-02 -7.34096012e+01], action: [-0.49866807  0.10624523]\n",
      "Episode: 122/3000, Reward: -797.901691616112, done: False\n",
      "state: [ 1.01103703e-02 -7.33813708e+01], action: [-1.    -0.167]\n",
      "Episode: 123/3000, Reward: -786.5862939257984, done: False\n",
      "state: [-6.80808887e-02 -7.31191457e+01], action: [-0.7819978   0.02790721]\n",
      "Episode: 124/3000, Reward: -777.2653174330925, done: False\n",
      "state: [-6.78884558e-02 -7.32218609e+01], action: [-0.3909743  -0.04938223]\n",
      "Episode: 125/3000, Reward: -818.4158388485829, done: False\n",
      "state: [-4.09805611e-02 -7.35328007e+01], action: [-1.    -0.167]\n",
      "Episode: 126/3000, Reward: -811.9307030142397, done: False\n",
      "state: [-2.26836923e-02 -7.35899865e+01], action: [ 0.82792574 -0.02049506]\n",
      "Episode: 127/3000, Reward: -834.5277223552946, done: False\n",
      "state: [ -0.07715391 -74.01782736], action: [ 0.04737622 -0.05381474]\n",
      "Episode: 128/3000, Reward: -825.8638306408114, done: False\n",
      "state: [ -0.0767002  -73.84470372], action: [-1.    -0.167]\n",
      "Episode: 129/3000, Reward: -836.2157720275923, done: False\n",
      "state: [-5.57497488e-03 -7.37657803e+01], action: [0.38571197 0.09846565]\n",
      "Episode: 130/3000, Reward: -754.2978907979801, done: False\n",
      "state: [ -0.09687637 -72.95988525], action: [-1.    -0.167]\n",
      "Episode: 131/3000, Reward: -831.033474233502, done: False\n",
      "state: [-3.00336361e-02 -7.38048383e+01], action: [-1.    -0.167]\n",
      "Episode: 132/3000, Reward: -813.0954047204302, done: False\n",
      "state: [-2.22157223e-03 -7.34358531e+01], action: [-1.    -0.167]\n",
      "Episode: 133/3000, Reward: -808.8274911885314, done: False\n",
      "state: [-4.62214042e-02 -7.35293525e+01], action: [-0.6151836  -0.14734784]\n",
      "Episode: 134/3000, Reward: -840.8576781630086, done: False\n",
      "state: [-6.97491851e-02 -7.39805455e+01], action: [ 0.7580554 -0.0105959]\n",
      "Episode: 135/3000, Reward: -820.3774102408806, done: False\n",
      "state: [ -0.09739684 -73.88845666], action: [-1.    -0.167]\n",
      "Episode: 136/3000, Reward: -838.1770537649743, done: False\n",
      "state: [-6.29346946e-02 -7.40952977e+01], action: [-1.    -0.167]\n",
      "Episode: 137/3000, Reward: -801.1587582122237, done: False\n",
      "state: [-7.19811610e-02 -7.36404034e+01], action: [ 0.0517505 -0.0183845]\n",
      "Episode: 138/3000, Reward: -783.9548376459876, done: False\n",
      "state: [ -0.09233158 -73.38138677], action: [ 0.46970445 -0.07483897]\n",
      "Episode: 139/3000, Reward: -814.5567853446169, done: False\n",
      "state: [ -0.10439089 -73.80879672], action: [-0.74138486  0.00924477]\n",
      "Episode: 140/3000, Reward: -808.4463646737307, done: False\n",
      "state: [-4.21740574e-02 -7.36519128e+01], action: [-1.    -0.167]\n",
      "Episode: 141/3000, Reward: -837.6474214423928, done: False\n",
      "state: [ -0.13511074 -74.12568867], action: [-1.    -0.167]\n",
      "Episode: 142/3000, Reward: -793.3737136420453, done: False\n",
      "state: [-6.37995007e-02 -7.35140836e+01], action: [0.50615865 0.1597269 ]\n",
      "Episode: 143/3000, Reward: -844.47580079619, done: False\n",
      "state: [ 3.18920744e-02 -7.39322954e+01], action: [-0.1615855  -0.00249407]\n",
      "Episode: 144/3000, Reward: -800.8409217644789, done: False\n",
      "state: [ -0.07987812 -73.53001432], action: [-1.    -0.167]\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(memory_size)\n",
    "epsilon = 1.0\n",
    "# Set up the environment\n",
    "env = cstr_env()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize networks and optimizer\n",
    "naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network.load_state_dict(naf_network.state_dict())\n",
    "target_naf_network.eval()\n",
    "\n",
    "optimizer = optim.Adam(naf_network.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = get_action(state, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        train_model()\n",
    "        if step == 500:\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"Episode: {e+1}/{num_episodes}, Reward: {total_reward}, done: {done}\")\n",
    "    print(f\"state: {state}, action: {action}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a08d4-b186-44b8-bf8b-f93700e4377a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
