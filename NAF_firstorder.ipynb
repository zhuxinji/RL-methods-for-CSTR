{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac5833c-5ded-406e-ac52-41350c82cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from CSTREnv import cstr_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7629042-5a6e-479c-b7c0-88305ec82066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for NAF\n",
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "        self.fc_mu = nn.Linear(256, action_size)\n",
    "        self.fc_l = nn.Linear(256, action_size * action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.fc_value(x)\n",
    "        mu = torch.tanh(self.fc_mu(x))*torch.tensor([1, 0.167])\n",
    "        l = self.fc_l(x)\n",
    "\n",
    "        l_matrix = l.view(-1, action_size, action_size)\n",
    "        l_matrix = torch.tril(l_matrix, -1) + torch.diag_embed(torch.exp(torch.diagonal(l_matrix, dim1=-2, dim2=-1)))\n",
    "        p_matrix = torch.bmm(l_matrix, l_matrix.transpose(2, 1))\n",
    "\n",
    "        return value, mu, p_matrix\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        value, mu, p_matrix = self.forward(state)\n",
    "        action_diff = action - mu\n",
    "        advantage = -0.5 * torch.bmm(action_diff.unsqueeze(1), torch.bmm(p_matrix, action_diff.unsqueeze(2))).squeeze(2)\n",
    "        q_value = value + advantage\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f277494d-0c7f-441e-8e6f-daa020fa580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf41011-f469-4b54-a33e-888854179614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch):\n",
    "    if epoch < 50:\n",
    "        return \n",
    "\n",
    "    minibatch = memory.sample(batch_size)\n",
    "    states = torch.FloatTensor([e[0] for e in minibatch])\n",
    "    actions = torch.FloatTensor([e[1] for e in minibatch])\n",
    "    rewards = torch.FloatTensor([e[2] for e in minibatch])\n",
    "    next_states = torch.FloatTensor([e[3] for e in minibatch])\n",
    "    dones = torch.FloatTensor([e[4] for e in minibatch])\n",
    "\n",
    "    q_values = naf_network.q_value(states, actions)\n",
    "    next_actions = target_naf_network(next_states)[1]\n",
    "    next_q_values = target_naf_network.q_value(next_states, next_actions)\n",
    "    target_q_values = rewards.unsqueeze(1) + (1 - dones).unsqueeze(1) * discount_factor * next_q_values\n",
    "\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_naf_network.parameters(), naf_network.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbc1d7d-3a4b-4c0a-8b48-583b9db8785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    _, mu, _ = naf_network(state)\n",
    "    return mu.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9640bf2-ffde-400c-bd69-955b7dfb1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "discount_factor = 0.99\n",
    "batch_size = 256\n",
    "tau = 0.001\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "memory_size = 1000000\n",
    "num_episodes = 3000\n",
    "episode_length = num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b119a2-3554-4cd6-acc1-0d4515cf9e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/3000, Reward: -12.33353226999207, done: False\n",
      "state: [  0.12322117 -11.22304723], action: [-0.735866   -0.08041313]\n",
      "Episode: 2/3000, Reward: -6.630148423835987, done: False\n",
      "state: [ 0.10490079 -6.00923733], action: [0.7513824  0.06663113]\n",
      "Episode: 3/3000, Reward: -13.466535160302872, done: False\n",
      "state: [  0.1093568  -12.41413409], action: [-0.539509   -0.01864239]\n",
      "Episode: 4/3000, Reward: -12.733531425899884, done: False\n",
      "state: [  0.12022271 -11.89198817], action: [0.05764126 0.10464754]\n",
      "Episode: 5/3000, Reward: -3.4171713515843583, done: False\n",
      "state: [ 0.07763018 -0.7778555 ], action: [-0.8969545  -0.15452193]\n",
      "Episode: 6/3000, Reward: -12.9601971457035, done: False\n",
      "state: [  0.1200199  -11.86461866], action: [-0.60460764  0.0614411 ]\n",
      "Episode: 7/3000, Reward: -10.295431334740979, done: False\n",
      "state: [ 0.11998705 -9.68283004], action: [ 0.44213656 -0.04603828]\n",
      "Episode: 8/3000, Reward: -12.416522313798108, done: False\n",
      "state: [  0.12731271 -11.19483099], action: [ 0.15747249 -0.01943566]\n",
      "Episode: 9/3000, Reward: -6.681882870461873, done: False\n",
      "state: [ 0.08161776 -6.65017949], action: [-0.84223163  0.05903482]\n",
      "Episode: 10/3000, Reward: -11.449069931718007, done: False\n",
      "state: [  0.11206155 -10.55237112], action: [-0.6845355   0.08509678]\n",
      "Episode: 11/3000, Reward: -11.600995519468459, done: False\n",
      "state: [  0.11796544 -11.05378547], action: [0.42358208 0.05254789]\n",
      "Episode: 12/3000, Reward: -13.469354352918916, done: False\n",
      "state: [  0.10983584 -11.95087074], action: [-0.73414916  0.09433707]\n",
      "Episode: 13/3000, Reward: -8.697651578758988, done: False\n",
      "state: [ 0.12363102 -8.78078686], action: [-0.15544005  0.03402465]\n",
      "Episode: 14/3000, Reward: -7.495630507986801, done: False\n",
      "state: [ 0.09265952 -8.28148712], action: [-0.5452792   0.11552459]\n",
      "Episode: 15/3000, Reward: -14.175578201766363, done: False\n",
      "state: [  0.11820197 -12.59619137], action: [-0.29237425 -0.05570405]\n",
      "Episode: 16/3000, Reward: -19.721071403404164, done: False\n",
      "state: [  0.0955933 -15.5555782], action: [-0.8332309  0.1143371]\n",
      "Episode: 17/3000, Reward: -20.535917321989285, done: False\n",
      "state: [  0.0944596  -15.99818553], action: [-0.16430539  0.14443074]\n",
      "Episode: 18/3000, Reward: -5.522601826169567, done: False\n",
      "state: [ 0.07079355 -5.85913543], action: [0.40419078 0.01859316]\n",
      "Episode: 19/3000, Reward: -17.129128671943498, done: False\n",
      "state: [  0.11519101 -14.64877966], action: [-0.6358176   0.07877538]\n",
      "Episode: 20/3000, Reward: -9.28624718141994, done: False\n",
      "state: [ 0.11580882 -8.93423205], action: [0.3748247  0.00787518]\n",
      "Episode: 21/3000, Reward: -14.872356674642077, done: False\n",
      "state: [  0.06859195 -13.58236391], action: [0.94108796 0.05981917]\n",
      "Episode: 22/3000, Reward: -7.9037677707686065, done: False\n",
      "state: [ 0.10245997 -7.71576847], action: [-0.5603131   0.06385104]\n",
      "Episode: 23/3000, Reward: -12.186335818365606, done: False\n",
      "state: [  0.08917159 -11.30392172], action: [ 0.65565044 -0.14007722]\n",
      "Episode: 24/3000, Reward: -10.118012951895636, done: False\n",
      "state: [  0.12051593 -10.13213734], action: [ 0.97638756 -0.08873139]\n",
      "Episode: 25/3000, Reward: -6.4894606135783235, done: False\n",
      "state: [ 0.11094127 -6.5333659 ], action: [ 0.63312453 -0.16245352]\n",
      "Episode: 26/3000, Reward: -21.979605200057744, done: False\n",
      "state: [  0.12430093 -16.5357306 ], action: [0.4674684  0.07482672]\n",
      "Episode: 27/3000, Reward: -8.508156820267857, done: False\n",
      "state: [ 0.07904843 -9.13690188], action: [0.5955579  0.02851496]\n",
      "Episode: 28/3000, Reward: -23.172719069591913, done: False\n",
      "state: [  0.09711061 -17.27365726], action: [-0.17390293 -0.16214173]\n",
      "Episode: 29/3000, Reward: -22.600427875445977, done: False\n",
      "state: [  0.10887203 -16.94014579], action: [0.3191174  0.13202265]\n",
      "Episode: 30/3000, Reward: -3.3287834882711675, done: False\n",
      "state: [0.04091067 6.15676874], action: [ 0.17849493 -0.11327942]\n",
      "Episode: 31/3000, Reward: -24.435407152549626, done: False\n",
      "state: [  0.09398341 -17.56578061], action: [-0.8729668   0.12331898]\n",
      "Episode: 32/3000, Reward: -37.26441539808425, done: False\n",
      "state: [  0.10950451 -21.66413764], action: [-0.5032378  0.0360017]\n",
      "Episode: 33/3000, Reward: -10.649774394814912, done: False\n",
      "state: [  0.10785831 -10.50678998], action: [0.8046202  0.10650612]\n",
      "Episode: 34/3000, Reward: -16.85861603389449, done: False\n",
      "state: [  0.080084   -14.42212664], action: [-0.80681586  0.10874429]\n",
      "Episode: 35/3000, Reward: -13.829725490300042, done: False\n",
      "state: [  0.10702629 -12.35709546], action: [-0.747352    0.09686261]\n",
      "Episode: 36/3000, Reward: -13.461822852643929, done: False\n",
      "state: [  0.09183795 -12.85280531], action: [-0.7622106  -0.13765627]\n",
      "Episode: 37/3000, Reward: -21.436156012272995, done: False\n",
      "state: [  0.08039703 -16.59010386], action: [-0.5957619   0.07437413]\n",
      "Episode: 38/3000, Reward: -24.557780231379716, done: False\n",
      "state: [  0.10830794 -17.5204155 ], action: [ 0.03852613 -0.16633947]\n",
      "Episode: 39/3000, Reward: -14.41050464186188, done: False\n",
      "state: [  0.11006837 -12.68870819], action: [-0.47226    -0.15828197]\n",
      "Episode: 40/3000, Reward: -24.95944689657405, done: False\n",
      "state: [  0.06803426 -18.28261812], action: [0.04048644 0.03721452]\n",
      "Episode: 41/3000, Reward: -10.51131183173917, done: False\n",
      "state: [  0.09400706 -10.55166216], action: [ 0.8225125  -0.12429644]\n",
      "Episode: 42/3000, Reward: -32.935610919621524, done: False\n",
      "state: [  0.09130879 -20.50245586], action: [-0.91551846  0.13410455]\n",
      "Episode: 43/3000, Reward: -22.792714874735417, done: False\n",
      "state: [  0.07584807 -17.16369445], action: [ 0.1698983 -0.0696112]\n",
      "Episode: 44/3000, Reward: -28.979322868947833, done: False\n",
      "state: [  0.093665   -19.17482772], action: [-0.38319767  0.06141929]\n",
      "Episode: 45/3000, Reward: -35.183420630649685, done: False\n",
      "state: [  0.07337629 -21.44805592], action: [-0.23154402 -0.13017881]\n",
      "Episode: 46/3000, Reward: -28.4915166786866, done: False\n",
      "state: [  0.09109888 -19.12106891], action: [ 0.9299486  -0.11313852]\n",
      "Episode: 47/3000, Reward: -16.886349798336024, done: False\n",
      "state: [  0.06715429 -14.80599335], action: [-0.8166573  0.1108119]\n",
      "Episode: 48/3000, Reward: -33.43731423997282, done: False\n",
      "state: [  0.07313805 -20.99147714], action: [-0.8905653 -0.1491465]\n",
      "Episode: 49/3000, Reward: -34.16428512096643, done: False\n",
      "state: [  0.09220359 -21.10529177], action: [-0.4586378  -0.12774931]\n",
      "Episode: 50/3000, Reward: -8.685960002548294, done: False\n",
      "state: [ 0.09795792 -8.95665279], action: [0.22169164 0.01345082]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuxinji\\AppData\\Local\\Temp\\ipykernel_9676\\3107169584.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor([e[0] for e in minibatch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51/3000, Reward: -6.554611512628095, done: False\n",
      "state: [ 0.10614925 -6.43374768], action: [ 0.9866632  -0.07201547]\n",
      "Episode: 52/3000, Reward: -5.0946024256550775, done: False\n",
      "state: [ 0.08005541 -4.55694003], action: [-0.26343143  0.08051584]\n",
      "Episode: 53/3000, Reward: -5.113306455647635, done: False\n",
      "state: [ 0.09822194 -3.61725901], action: [-0.9388875   0.01176811]\n",
      "Episode: 54/3000, Reward: -5.971054302680004, done: False\n",
      "state: [ 0.111601  -5.2452245], action: [-0.9600574   0.11023979]\n",
      "Episode: 55/3000, Reward: -16.510499666474768, done: False\n",
      "state: [  0.12540351 -14.23357564], action: [ 0.27113354 -0.01803673]\n",
      "Episode: 56/3000, Reward: -11.088267074095997, done: False\n",
      "state: [  0.09420266 -10.93522325], action: [ 0.06608845 -0.01484043]\n",
      "Episode: 57/3000, Reward: -16.05736620795416, done: False\n",
      "state: [  0.12377315 -13.83868786], action: [0.8266247  0.06982926]\n",
      "Episode: 58/3000, Reward: -15.555090771356035, done: False\n",
      "state: [  0.1671146  -12.85796134], action: [ 0.21097313 -0.01556375]\n",
      "Episode: 59/3000, Reward: -13.014235694552546, done: False\n",
      "state: [  0.13555098 -11.95666824], action: [ 0.29327568 -0.01327392]\n",
      "Episode: 60/3000, Reward: -15.757884190808578, done: False\n",
      "state: [  0.15070993 -13.34428452], action: [ 0.71324885 -0.11661252]\n",
      "Episode: 61/3000, Reward: -4.487812843208036, done: False\n",
      "state: [ 0.09538175 -2.52210721], action: [-0.01988501  0.04262187]\n",
      "Episode: 62/3000, Reward: -11.85096857200237, done: False\n",
      "state: [  0.1629569  -10.63003833], action: [-0.86111695 -0.16594048]\n",
      "Episode: 63/3000, Reward: -5.617461479132222, done: False\n",
      "state: [ 0.11545979 -5.26441845], action: [-0.50195396 -0.06776021]\n",
      "Episode: 64/3000, Reward: -8.471719077302511, done: False\n",
      "state: [ 0.10679071 -8.46430149], action: [ 0.5060201  -0.16294426]\n",
      "Episode: 65/3000, Reward: -8.625780397289434, done: False\n",
      "state: [ 0.13978324 -7.98571721], action: [0.03667971 0.02075819]\n",
      "Episode: 66/3000, Reward: -4.138744107832402, done: False\n",
      "state: [ 0.08247537 -2.46180184], action: [-0.8007692  -0.03557006]\n",
      "Episode: 67/3000, Reward: -9.369641788502912, done: False\n",
      "state: [ 0.13496443 -8.54697105], action: [ 0.3304816  -0.16167362]\n",
      "Episode: 68/3000, Reward: -9.42249578932009, done: False\n",
      "state: [ 0.16109683 -8.89952862], action: [-0.56418043  0.05149745]\n",
      "Episode: 69/3000, Reward: -9.60916061316088, done: False\n",
      "state: [ 0.11357735 -9.1568863 ], action: [0.7259485  0.02163363]\n",
      "Episode: 70/3000, Reward: -4.689409659395587, done: False\n",
      "state: [ 0.10363744 -3.32686949], action: [-0.02191791  0.0138318 ]\n",
      "Episode: 71/3000, Reward: -9.4830625294967, done: False\n",
      "state: [ 0.12308842 -9.20234924], action: [-0.84987706  0.08960281]\n",
      "Episode: 72/3000, Reward: -10.984622722046943, done: False\n",
      "state: [  0.12292886 -10.59452047], action: [0.01866792 0.08156192]\n",
      "Episode: 73/3000, Reward: -11.943211548277274, done: False\n",
      "state: [  0.1131126  -11.27727047], action: [0.8023713 0.1613449]\n",
      "Episode: 74/3000, Reward: -12.327714352557573, done: False\n",
      "state: [  0.11399705 -11.28267213], action: [-0.5479171   0.09613048]\n",
      "Episode: 75/3000, Reward: -11.59638806465128, done: False\n",
      "state: [  0.13584676 -10.28962646], action: [ 0.02017239 -0.02883241]\n",
      "Episode: 76/3000, Reward: -5.808367064312032, done: False\n",
      "state: [ 0.09725246 -5.29169015], action: [-0.08925061  0.06408768]\n",
      "Episode: 77/3000, Reward: -5.690871475622643, done: False\n",
      "state: [ 0.10596506 -4.76866146], action: [-0.8308235  -0.12533982]\n",
      "Episode: 78/3000, Reward: -12.218628828562897, done: False\n",
      "state: [  0.15807084 -10.60684933], action: [-0.2865339  -0.07950562]\n",
      "Episode: 79/3000, Reward: -8.7480610818065, done: False\n",
      "state: [ 0.12652665 -8.22298293], action: [0.10838834 0.13638923]\n",
      "Episode: 80/3000, Reward: -3.165757141061353, done: False\n",
      "state: [0.11440832 4.53227737], action: [0.53241354 0.04779014]\n",
      "Episode: 81/3000, Reward: -3.2958545250999554, done: False\n",
      "state: [0.10362708 1.36998977], action: [-0.41430882  0.02948693]\n",
      "Episode: 82/3000, Reward: -20.15532760468994, done: False\n",
      "state: [  0.17405514 -14.87413325], action: [ 0.475778   -0.06946277]\n",
      "Episode: 83/3000, Reward: -10.19578233734049, done: False\n",
      "state: [ 0.1687594  -9.26344464], action: [0.71248543 0.04280822]\n",
      "Episode: 84/3000, Reward: -244.78630005847074, done: False\n",
      "state: [ -0.57090174 143.49755439], action: [-0.23233683 -0.08555491]\n",
      "Episode: 85/3000, Reward: -5.133637424660962, done: False\n",
      "state: [ 0.0910509  11.58900685], action: [0.75126725 0.15733677]\n",
      "Episode: 86/3000, Reward: -10.582948784934494, done: False\n",
      "state: [ 0.06479595 21.57290144], action: [ 0.12829259 -0.16193184]\n",
      "Episode: 87/3000, Reward: -6.444674670572581, done: False\n",
      "state: [ 0.11067253 14.06626431], action: [0.99992925 0.16699994]\n",
      "Episode: 88/3000, Reward: -687.9644310061628, done: False\n",
      "state: [ -0.37076515 124.05434846], action: [1.    0.167]\n",
      "Episode: 89/3000, Reward: -1595.4291504676874, done: False\n",
      "state: [ -0.25670471 100.35170362], action: [-0.6482547 -0.1355356]\n",
      "Episode: 90/3000, Reward: -724.544213734997, done: False\n",
      "state: [ -0.65732311 188.61318114], action: [1.    0.167]\n",
      "Episode: 91/3000, Reward: -524.7704178949148, done: False\n",
      "state: [-0.08403085 70.39656956], action: [-0.45196494 -0.03345609]\n",
      "Episode: 92/3000, Reward: -1080.1533476402642, done: False\n",
      "state: [ -0.60957151 173.8120522 ], action: [1.    0.167]\n",
      "Episode: 93/3000, Reward: -6.084290842876638, done: False\n",
      "state: [ 0.06598654 -6.61580767], action: [-0.8054901  -0.14377056]\n",
      "Episode: 94/3000, Reward: -20.638993209360628, done: False\n",
      "state: [  0.10957019 -16.57866615], action: [0.62672323 0.00740532]\n",
      "Episode: 95/3000, Reward: -38.27473990433243, done: False\n",
      "state: [  0.15765961 -22.12436223], action: [-0.02805248  0.02330592]\n",
      "Episode: 96/3000, Reward: -60.41683485528723, done: False\n",
      "state: [  0.16657551 -27.13053853], action: [ 0.39082703 -0.09896545]\n",
      "Episode: 97/3000, Reward: -54.87346210859672, done: False\n",
      "state: [  0.09633919 -26.52246467], action: [0.8786302  0.07187592]\n",
      "Episode: 98/3000, Reward: -61.26150092610847, done: False\n",
      "state: [  0.11232769 -27.63622278], action: [-0.16181639  0.08831302]\n",
      "Episode: 99/3000, Reward: -54.22515285027225, done: False\n",
      "state: [  0.12118452 -26.31019173], action: [0.9327929  0.15505257]\n",
      "Episode: 100/3000, Reward: -65.6137404192268, done: False\n",
      "state: [  0.20642935 -27.94702234], action: [0.9957342  0.15845576]\n",
      "Episode: 101/3000, Reward: -65.87823723441417, done: False\n",
      "state: [  0.17127228 -28.1254829 ], action: [0.9985825 0.1162761]\n",
      "Episode: 102/3000, Reward: -50.09142926595792, done: False\n",
      "state: [  0.19070401 -24.99852651], action: [0.9960218 0.0849186]\n",
      "Episode: 103/3000, Reward: -66.79263564768347, done: False\n",
      "state: [  0.21865666 -27.92877697], action: [0.99994093 0.04832269]\n",
      "Episode: 104/3000, Reward: -49.30494652675083, done: False\n",
      "state: [  0.23341686 -23.90196953], action: [ 0.05479533 -0.1178012 ]\n",
      "Episode: 105/3000, Reward: -57.92164008560399, done: False\n",
      "state: [  0.27033502 -25.93040122], action: [ 0.9999641  -0.10838462]\n",
      "Episode: 106/3000, Reward: -57.018008517856394, done: False\n",
      "state: [  0.24182098 -25.88232727], action: [-0.65791273  0.10538004]\n",
      "Episode: 107/3000, Reward: -51.22438847392983, done: False\n",
      "state: [  0.26624918 -24.54729625], action: [0.99995846 0.02200943]\n",
      "Episode: 108/3000, Reward: -54.12674025290871, done: False\n",
      "state: [  0.27656824 -25.20450847], action: [-0.9153572   0.00241475]\n",
      "Episode: 109/3000, Reward: -56.668988192226166, done: False\n",
      "state: [  0.23755989 -26.01430173], action: [0.44897944 0.1299437 ]\n",
      "Episode: 110/3000, Reward: -71.82791972057046, done: False\n",
      "state: [ -0.0303552  -29.81189487], action: [-0.99603623  0.09486133]\n",
      "Episode: 111/3000, Reward: -81.66955715804814, done: False\n",
      "state: [ -0.0412582  -31.42936564], action: [-0.9984719   0.11112506]\n",
      "Episode: 112/3000, Reward: -80.4275805462679, done: False\n",
      "state: [-2.75410559e-02 -3.11767650e+01], action: [0.77720106 0.06615666]\n",
      "Episode: 113/3000, Reward: -78.22484191990235, done: False\n",
      "state: [-2.47108899e-02 -3.08767495e+01], action: [ 0.7565854  -0.04495367]\n",
      "Episode: 114/3000, Reward: -81.82023210251192, done: False\n",
      "state: [-1.00368309e-02 -3.13175553e+01], action: [-0.9975275  -0.01787891]\n",
      "Episode: 115/3000, Reward: -77.92801122537479, done: False\n",
      "state: [ -0.04783253 -30.87831963], action: [-0.99626946 -0.10430141]\n",
      "Episode: 116/3000, Reward: -75.44129847290534, done: False\n",
      "state: [-1.03927527e-02 -3.03463982e+01], action: [-0.21776311 -0.01360689]\n",
      "Episode: 117/3000, Reward: -73.83086774899567, done: False\n",
      "state: [ 8.00823737e-03 -2.98967820e+01], action: [-0.3783796   0.10865767]\n",
      "Episode: 118/3000, Reward: -57.861157984327164, done: False\n",
      "state: [  0.17113245 -26.66532427], action: [-0.6449167   0.12539989]\n",
      "Episode: 119/3000, Reward: -68.49329274350521, done: False\n",
      "state: [  0.10941535 -28.69548971], action: [-0.1251611   0.12004079]\n",
      "Episode: 120/3000, Reward: -70.36525582013358, done: False\n",
      "state: [  0.1033177  -29.18535553], action: [ 0.5687868  -0.00684491]\n",
      "Episode: 121/3000, Reward: -73.4490702927969, done: False\n",
      "state: [  0.10200035 -29.72546165], action: [0.30511886 0.13968928]\n",
      "Episode: 122/3000, Reward: -74.15421049233898, done: False\n",
      "state: [  0.09298006 -29.72934638], action: [0.42039934 0.03486905]\n",
      "Episode: 123/3000, Reward: -77.17483449000325, done: False\n",
      "state: [  0.08039192 -30.09053927], action: [-0.72777444 -0.07731672]\n",
      "Episode: 124/3000, Reward: -51.95715861276775, done: False\n",
      "state: [  0.17527088 -25.51921446], action: [ 0.84565544 -0.09517142]\n",
      "Episode: 125/3000, Reward: -63.88681260055654, done: False\n",
      "state: [  0.21823756 -27.65792139], action: [ 0.99749005 -0.12953806]\n",
      "Episode: 126/3000, Reward: -58.24372986002149, done: False\n",
      "state: [  0.32230363 -25.65241076], action: [0.9957525  0.02994972]\n",
      "Episode: 127/3000, Reward: -68.94829710345122, done: False\n",
      "state: [  0.31089435 -27.68600039], action: [0.9877825  0.06706044]\n",
      "Episode: 128/3000, Reward: -71.92764246465774, done: False\n",
      "state: [  0.28104047 -28.52687644], action: [ 0.5556394  -0.15127173]\n",
      "Episode: 129/3000, Reward: -56.03128600098593, done: False\n",
      "state: [  0.26095631 -25.47601357], action: [ 0.10276826 -0.06110942]\n",
      "Episode: 130/3000, Reward: -61.48693017599, done: False\n",
      "state: [  0.24266994 -26.64441037], action: [ 0.9958994  -0.11714467]\n",
      "Episode: 131/3000, Reward: -49.28004367051802, done: False\n",
      "state: [  0.26237129 -24.2130285 ], action: [ 0.9943046  -0.06571557]\n",
      "Episode: 132/3000, Reward: -62.882343322420255, done: False\n",
      "state: [  0.3083607  -26.60087743], action: [0.9993363  0.03801988]\n",
      "Episode: 133/3000, Reward: -62.96953681779551, done: False\n",
      "state: [  0.30703351 -26.50478087], action: [-0.08603568  0.10763726]\n",
      "Episode: 134/3000, Reward: -62.005096249861296, done: False\n",
      "state: [  0.33843182 -26.04792441], action: [ 0.9995483  -0.08457737]\n",
      "Episode: 135/3000, Reward: -58.85643233343841, done: False\n",
      "state: [  0.29294207 -25.56691285], action: [-0.30914924 -0.04962194]\n",
      "Episode: 136/3000, Reward: -51.93496210092641, done: False\n",
      "state: [  0.35869307 -23.68597485], action: [0.9995806  0.16622292]\n",
      "Episode: 137/3000, Reward: -64.49880112821953, done: False\n",
      "state: [  0.33080829 -26.62951916], action: [0.99907005 0.16660689]\n",
      "Episode: 138/3000, Reward: -62.010631430474334, done: False\n",
      "state: [  0.35284774 -25.8449732 ], action: [-0.70634615  0.13293248]\n",
      "Episode: 139/3000, Reward: -49.8397473926197, done: False\n",
      "state: [  0.32256816 -23.69646287], action: [-0.2697667   0.08204123]\n",
      "Episode: 140/3000, Reward: -55.53060802994998, done: False\n",
      "state: [  0.35627926 -24.61082237], action: [ 0.9979234  -0.10127898]\n",
      "Episode: 141/3000, Reward: -57.609395461153966, done: False\n",
      "state: [  0.37958187 -24.94725682], action: [0.99846727 0.09301598]\n",
      "Episode: 142/3000, Reward: -61.56865356362792, done: False\n",
      "state: [  0.35310058 -25.91908098], action: [0.66032    0.02776137]\n",
      "Episode: 143/3000, Reward: -55.983491633371074, done: False\n",
      "state: [  0.32676455 -24.87188625], action: [0.99687505 0.05651706]\n",
      "Episode: 144/3000, Reward: -49.33533168109773, done: False\n",
      "state: [  0.29573781 -23.80795614], action: [0.4969573  0.16044252]\n",
      "Episode: 145/3000, Reward: -65.87799233835848, done: False\n",
      "state: [  0.37123506 -26.52271519], action: [ 0.12307872 -0.12124243]\n",
      "Episode: 146/3000, Reward: -59.164132900054604, done: False\n",
      "state: [  0.32629831 -25.55269166], action: [ 0.9999933 -0.167    ]\n",
      "Episode: 147/3000, Reward: -42.90604762331872, done: False\n",
      "state: [  0.37079552 -21.3579336 ], action: [0.35839286 0.14801997]\n",
      "Episode: 148/3000, Reward: -55.81142173736916, done: False\n",
      "state: [  0.38974005 -24.2589678 ], action: [ 0.99999684 -0.16699998]\n",
      "Episode: 149/3000, Reward: -55.6070721168378, done: False\n",
      "state: [  0.36278111 -24.21773141], action: [ 0.9999897 -0.167    ]\n",
      "Episode: 150/3000, Reward: -53.53967923494465, done: False\n",
      "state: [  0.40684586 -23.58395511], action: [ 0.9997973  -0.16699998]\n",
      "Episode: 151/3000, Reward: -49.69574202187532, done: False\n",
      "state: [  0.37245657 -22.68008406], action: [-0.00781069 -0.06037596]\n",
      "Episode: 152/3000, Reward: -47.804708978459175, done: False\n",
      "state: [  0.36045768 -22.36908018], action: [ 0.9994883  -0.16699998]\n",
      "Episode: 153/3000, Reward: -48.496663049994325, done: False\n",
      "state: [  0.34617596 -22.8370288 ], action: [ 0.9989858 -0.167    ]\n",
      "Episode: 154/3000, Reward: -43.63717678705225, done: False\n",
      "state: [  0.34969258 -21.4856964 ], action: [ 0.99676126 -0.16699998]\n",
      "Episode: 155/3000, Reward: -48.053193927961075, done: False\n",
      "state: [  0.40750035 -21.96284932], action: [-0.69765604  0.10161881]\n",
      "Episode: 156/3000, Reward: -59.48529961015092, done: False\n",
      "state: [  0.38772492 -24.85723331], action: [ 0.99186623 -0.167     ]\n",
      "Episode: 157/3000, Reward: -48.84301788002868, done: False\n",
      "state: [  0.38634603 -22.12840546], action: [ 0.99231577 -0.167     ]\n",
      "Episode: 158/3000, Reward: -50.01652376506134, done: False\n",
      "state: [  0.38353835 -22.40262382], action: [0.15984038 0.0601396 ]\n",
      "Episode: 159/3000, Reward: -46.40533235678288, done: False\n",
      "state: [  0.39385203 -21.61519266], action: [-0.73022175 -0.15380114]\n",
      "Episode: 160/3000, Reward: -50.74409488795168, done: False\n",
      "state: [  0.36531209 -23.19688012], action: [ 0.9992564 -0.167    ]\n",
      "Episode: 161/3000, Reward: -40.663674706437284, done: False\n",
      "state: [  0.37666224 -20.26382645], action: [ 0.9962549 -0.167    ]\n",
      "Episode: 162/3000, Reward: -43.10421901344049, done: False\n",
      "state: [  0.37686129 -20.90814105], action: [ 0.87827075 -0.07995487]\n",
      "Episode: 163/3000, Reward: -41.17553830982816, done: False\n",
      "state: [  0.38175016 -19.9748894 ], action: [ 0.9933806  -0.16699992]\n",
      "Episode: 164/3000, Reward: -38.88356275277944, done: False\n",
      "state: [  0.37929121 -19.55559018], action: [ 0.6272604  -0.13465372]\n",
      "Episode: 165/3000, Reward: -43.012312062185806, done: False\n",
      "state: [  0.37949577 -20.78128442], action: [-0.7971874  0.1385262]\n",
      "Episode: 166/3000, Reward: -42.24645814829466, done: False\n",
      "state: [  0.39350573 -20.24114907], action: [ 0.9977211  -0.16699106]\n",
      "Episode: 167/3000, Reward: -35.53449650009679, done: False\n",
      "state: [  0.33511251 -18.77303492], action: [-0.8108429 -0.071066 ]\n",
      "Episode: 168/3000, Reward: -42.76786120983909, done: False\n",
      "state: [  0.38477675 -20.55072933], action: [0.99849206 0.167     ]\n",
      "Episode: 169/3000, Reward: -41.33968387317818, done: False\n",
      "state: [  0.42247164 -19.73776263], action: [-0.05729545 -0.11041187]\n",
      "Episode: 170/3000, Reward: -32.25983761360425, done: False\n",
      "state: [  0.38622921 -16.93528577], action: [-0.19465843  0.02727183]\n",
      "Episode: 171/3000, Reward: -45.31564859648746, done: False\n",
      "state: [  0.41478728 -21.07677496], action: [0.9995354 0.167    ]\n",
      "Episode: 172/3000, Reward: -40.64187851862657, done: False\n",
      "state: [  0.38493608 -20.1602693 ], action: [0.99981654 0.167     ]\n",
      "Episode: 173/3000, Reward: -32.985319901985044, done: False\n",
      "state: [  0.41121274 -17.24636528], action: [0.9999275 0.167    ]\n",
      "Episode: 174/3000, Reward: -34.10320730441855, done: False\n",
      "state: [  0.39679135 -17.11827209], action: [-0.9088919  -0.11700673]\n",
      "Episode: 175/3000, Reward: -29.240113940589122, done: False\n",
      "state: [  0.40559416 -15.51782725], action: [0.99994403 0.167     ]\n",
      "Episode: 176/3000, Reward: -35.230603924818546, done: False\n",
      "state: [  0.40561076 -18.00482467], action: [0.9999341 0.167    ]\n",
      "Episode: 177/3000, Reward: -32.10802468288442, done: False\n",
      "state: [  0.40689557 -16.62515898], action: [ 0.62395686 -0.16097902]\n",
      "Episode: 178/3000, Reward: -43.06256711019416, done: False\n",
      "state: [  0.38516526 -20.62671114], action: [0.99996144 0.167     ]\n",
      "Episode: 179/3000, Reward: -25.915163593846106, done: False\n",
      "state: [  0.39544638 -14.08036943], action: [0.99995714 0.167     ]\n",
      "Episode: 180/3000, Reward: -33.854573931303044, done: False\n",
      "state: [  0.36885713 -17.34057238], action: [0.9998977 0.167    ]\n",
      "Episode: 181/3000, Reward: -38.67054304142171, done: False\n",
      "state: [  0.4065213 -18.8227058], action: [-0.8877732   0.05286466]\n",
      "Episode: 182/3000, Reward: -35.64165936321503, done: False\n",
      "state: [  0.38227623 -18.52064947], action: [0.15472466 0.16192837]\n",
      "Episode: 183/3000, Reward: -44.47669699299875, done: False\n",
      "state: [  0.37873574 -21.02188812], action: [0.9998733 0.167    ]\n",
      "Episode: 184/3000, Reward: -33.7490757676039, done: False\n",
      "state: [  0.37947046 -17.8126597 ], action: [-0.08180956  0.15734775]\n",
      "Episode: 185/3000, Reward: -34.16193082757472, done: False\n",
      "state: [  0.42629281 -17.13639849], action: [0.99988765 0.167     ]\n",
      "Episode: 186/3000, Reward: -33.43252350205404, done: False\n",
      "state: [  0.4238552  -16.89446189], action: [ 0.19105934 -0.0948654 ]\n",
      "Episode: 187/3000, Reward: -28.925247703800594, done: False\n",
      "state: [  0.42380959 -14.87749146], action: [0.99995166 0.167     ]\n",
      "Episode: 188/3000, Reward: -29.233803722329167, done: False\n",
      "state: [  0.43789935 -15.0538771 ], action: [0.99994236 0.167     ]\n",
      "Episode: 189/3000, Reward: -39.120177683718175, done: False\n",
      "state: [  0.41125862 -19.0152385 ], action: [0.9998828 0.167    ]\n",
      "Episode: 190/3000, Reward: -28.18133869469818, done: False\n",
      "state: [  0.44160491 -14.55708328], action: [0.9999388 0.167    ]\n",
      "Episode: 191/3000, Reward: -31.865574187008033, done: False\n",
      "state: [  0.39040167 -16.52635783], action: [-0.7297849  -0.01725513]\n",
      "Episode: 192/3000, Reward: -28.598145067047636, done: False\n",
      "state: [  0.43756717 -14.51322935], action: [0.33021167 0.1650458 ]\n",
      "Episode: 193/3000, Reward: -40.27429349065902, done: False\n",
      "state: [  0.39877056 -19.69035084], action: [-0.07168033 -0.03303662]\n",
      "Episode: 194/3000, Reward: -33.52973392203442, done: False\n",
      "state: [  0.4011448  -17.29497125], action: [-0.10729638  0.14920539]\n",
      "Episode: 195/3000, Reward: -32.67540169575506, done: False\n",
      "state: [  0.39584791 -16.80896774], action: [0.63012487 0.14736308]\n",
      "Episode: 196/3000, Reward: -30.51272005570644, done: False\n",
      "state: [  0.42327053 -15.63249993], action: [-0.96370226 -0.12189908]\n",
      "Episode: 197/3000, Reward: -30.165095387416294, done: False\n",
      "state: [  0.42251698 -15.29617356], action: [-0.2450234  -0.10660005]\n",
      "Episode: 198/3000, Reward: -31.304597212496546, done: False\n",
      "state: [  0.44823925 -15.86502565], action: [0.9999691 0.167    ]\n",
      "Episode: 199/3000, Reward: -32.20642722559606, done: False\n",
      "state: [  0.44224995 -16.07857616], action: [0.9999593 0.167    ]\n",
      "Episode: 200/3000, Reward: -34.82539384271053, done: False\n",
      "state: [  0.39171732 -17.65737448], action: [-0.89031214  0.00514133]\n",
      "Episode: 201/3000, Reward: -20.694459737806802, done: False\n",
      "state: [  0.4135991  -10.47131727], action: [0.9999974 0.167    ]\n",
      "Episode: 202/3000, Reward: -27.510857798755772, done: False\n",
      "state: [  0.4286356  -14.29430017], action: [0.9999958 0.167    ]\n",
      "Episode: 203/3000, Reward: -16.60811516614029, done: False\n",
      "state: [ 0.40214525 -7.31685014], action: [0.9999883 0.167    ]\n",
      "Episode: 204/3000, Reward: -29.627132833498354, done: False\n",
      "state: [  0.4168495  -15.32298011], action: [0.99999815 0.167     ]\n",
      "Episode: 205/3000, Reward: -13.61662070191094, done: False\n",
      "state: [ 0.29238323 -3.98316645], action: [-0.19369209 -0.10845847]\n",
      "Episode: 206/3000, Reward: -18.78660392283295, done: False\n",
      "state: [ 0.40254105 -8.91490199], action: [ 0.08851605 -0.06887694]\n",
      "Episode: 207/3000, Reward: -23.281808709723222, done: False\n",
      "state: [  0.43422836 -11.90517571], action: [0.99999934 0.167     ]\n",
      "Episode: 208/3000, Reward: -12.989131911057918, done: False\n",
      "state: [ 0.26800246 -3.3350793 ], action: [-1.     0.167]\n",
      "Episode: 209/3000, Reward: -15.51331856231164, done: False\n",
      "state: [ 0.39808463 -6.70912597], action: [0.99998087 0.167     ]\n",
      "Episode: 210/3000, Reward: -22.44445212983383, done: False\n",
      "state: [  0.44683585 -11.15550824], action: [0.99999964 0.167     ]\n",
      "Episode: 211/3000, Reward: -20.47575483132241, done: False\n",
      "state: [ 0.43172772 -9.83146325], action: [0.99999905 0.167     ]\n",
      "Episode: 212/3000, Reward: -26.177685695104106, done: False\n",
      "state: [  0.41028623 -13.63296329], action: [0.99999976 0.167     ]\n",
      "Episode: 213/3000, Reward: -18.27749382567005, done: False\n",
      "state: [ 0.39448558 -9.19320837], action: [-0.38014537  0.14445661]\n",
      "Episode: 214/3000, Reward: -12.687318326352512, done: False\n",
      "state: [ 0.26395085 -3.2108968 ], action: [-1.     0.167]\n",
      "Episode: 215/3000, Reward: -23.213428590235807, done: False\n",
      "state: [  0.43101741 -11.79309233], action: [ 0.84742683 -0.10914801]\n",
      "Episode: 216/3000, Reward: -16.088913869078482, done: False\n",
      "state: [ 0.41032449 -6.92461506], action: [0.99999756 0.167     ]\n",
      "Episode: 217/3000, Reward: -15.044835016358595, done: False\n",
      "state: [ 0.39244126 -5.85750754], action: [0.90398365 0.167     ]\n",
      "Episode: 218/3000, Reward: -19.069983448503162, done: False\n",
      "state: [ 0.45612161 -8.30165794], action: [0.9999986 0.167    ]\n",
      "Episode: 219/3000, Reward: -22.979526587858533, done: False\n",
      "state: [  0.43276719 -12.09676723], action: [0.9999997 0.167    ]\n",
      "Episode: 220/3000, Reward: -20.36984446154427, done: False\n",
      "state: [ 0.43081783 -9.97420563], action: [0.9999993 0.167    ]\n",
      "Episode: 221/3000, Reward: -20.195807084062213, done: False\n",
      "state: [  0.41893309 -10.09075013], action: [0.99999934 0.167     ]\n",
      "Episode: 222/3000, Reward: -20.20133022399682, done: False\n",
      "state: [ 0.40608934 -9.88210882], action: [0.9999993 0.167    ]\n",
      "Episode: 223/3000, Reward: -17.867874680238742, done: False\n",
      "state: [ 0.39710023 -8.83068273], action: [-0.8300724   0.11395139]\n",
      "Episode: 224/3000, Reward: -12.27395626198549, done: False\n",
      "state: [ 0.23682185 -3.27864513], action: [ 0.46514964 -0.11833045]\n",
      "Episode: 225/3000, Reward: -11.395149933929734, done: False\n",
      "state: [ 0.19351773 -3.27583531], action: [ 0.85920566 -0.11686467]\n",
      "Episode: 226/3000, Reward: -19.665233208428248, done: False\n",
      "state: [ 0.39427304 -9.85542807], action: [-0.9187927   0.14371102]\n",
      "Episode: 227/3000, Reward: -15.611218593186146, done: False\n",
      "state: [ 0.41106079 -6.19845846], action: [0.99842227 0.167     ]\n",
      "Episode: 228/3000, Reward: -11.179469401144464, done: False\n",
      "state: [ 0.17145577 -3.66521162], action: [-1.     0.167]\n",
      "Episode: 229/3000, Reward: -22.98133163462973, done: False\n",
      "state: [  0.41063221 -12.08757072], action: [0.9999996 0.167    ]\n",
      "Episode: 230/3000, Reward: -15.129057447029476, done: False\n",
      "state: [ 0.39581869 -5.32625446], action: [-0.7352268  0.167    ]\n",
      "Episode: 231/3000, Reward: -23.613648246225093, done: False\n",
      "state: [  0.42829484 -12.67915223], action: [ 0.27058    -0.07390165]\n",
      "Episode: 232/3000, Reward: -19.034664651359332, done: False\n",
      "state: [ 0.38682786 -9.85003058], action: [0.99999857 0.167     ]\n",
      "Episode: 233/3000, Reward: -24.798150492249242, done: False\n",
      "state: [  0.42259592 -12.40796812], action: [-0.20066524  0.01673683]\n",
      "Episode: 234/3000, Reward: -16.356271612427406, done: False\n",
      "state: [ 0.39868485 -7.2613183 ], action: [-0.9677208   0.11695705]\n",
      "Episode: 235/3000, Reward: -12.56309522376195, done: False\n",
      "state: [ 0.26400124 -4.27606377], action: [-0.99999946  0.167     ]\n",
      "Episode: 236/3000, Reward: -13.216781738059556, done: False\n",
      "state: [ 0.29137576 -3.60148446], action: [ 0.338167   -0.03364897]\n",
      "Episode: 237/3000, Reward: -17.590116300381137, done: False\n",
      "state: [ 0.43407399 -7.56719519], action: [0.9999976 0.167    ]\n",
      "Episode: 238/3000, Reward: -13.701629701465526, done: False\n",
      "state: [ 0.34355349 -4.1267582 ], action: [-0.99999976  0.167     ]\n",
      "Episode: 239/3000, Reward: -9.41031427521921, done: False\n",
      "state: [ 0.09846486 -2.79243417], action: [-1.     0.167]\n",
      "Episode: 240/3000, Reward: -9.713944123248087, done: False\n",
      "state: [ 0.0875577  -6.39771003], action: [0.9998518 0.167    ]\n",
      "Episode: 241/3000, Reward: -19.50386687996149, done: False\n",
      "state: [ 0.43315148 -9.4691861 ], action: [0.99999845 0.167     ]\n",
      "Episode: 242/3000, Reward: -12.55326747352773, done: False\n",
      "state: [ 0.26844151 -3.24373825], action: [ 0.54354405 -0.06074318]\n",
      "Episode: 243/3000, Reward: -17.220989591957945, done: False\n",
      "state: [ 0.44235071 -6.69658555], action: [0.9999942 0.167    ]\n",
      "Episode: 244/3000, Reward: -8.842015560072522, done: False\n",
      "state: [ 0.04850262 -3.83190024], action: [ 0.954834   -0.00144083]\n",
      "Episode: 245/3000, Reward: -9.505097845166501, done: False\n",
      "state: [ 0.11068284 -2.64129154], action: [-1.     0.167]\n",
      "Episode: 246/3000, Reward: -13.574392564267892, done: False\n",
      "state: [ 0.35251351 -4.66185207], action: [-0.999815  0.167   ]\n",
      "Episode: 247/3000, Reward: -10.821285833186499, done: False\n",
      "state: [ 0.16771842 -1.99654881], action: [-0.33974698  0.12654379]\n",
      "Episode: 248/3000, Reward: -10.680222137677031, done: False\n",
      "state: [ 0.14830845 -2.53103855], action: [ 0.356645   -0.09707579]\n",
      "Episode: 249/3000, Reward: -11.9255383544988, done: False\n",
      "state: [ 0.18658251 -2.66943936], action: [-1.     0.167]\n",
      "Episode: 250/3000, Reward: -10.274751576995753, done: False\n",
      "state: [ 0.12871765 -3.36095859], action: [-1.     0.167]\n",
      "Episode: 251/3000, Reward: -10.939720382940619, done: False\n",
      "state: [ 0.16460214 -1.56373694], action: [-1.     0.167]\n",
      "Episode: 252/3000, Reward: -16.079649974519434, done: False\n",
      "state: [ 0.41955579 -6.37566539], action: [0.9998367 0.167    ]\n",
      "Episode: 253/3000, Reward: -12.445722062473967, done: False\n",
      "state: [ 0.22031139 -2.8720175 ], action: [-1.     0.167]\n",
      "Episode: 254/3000, Reward: -13.164415668027221, done: False\n",
      "state: [ 0.29001923 -3.76306786], action: [-1.     0.167]\n",
      "Episode: 255/3000, Reward: -11.137406061841848, done: False\n",
      "state: [ 0.15184673 -2.5647219 ], action: [0.459491   0.04264793]\n",
      "Episode: 256/3000, Reward: -10.045182979338424, done: False\n",
      "state: [ 0.10090067 -2.52690219], action: [-1.     0.167]\n",
      "Episode: 257/3000, Reward: -21.993745256983722, done: False\n",
      "state: [  0.40665087 -10.87593153], action: [ 0.24719946 -0.12724955]\n",
      "Episode: 258/3000, Reward: -10.553783729658177, done: False\n",
      "state: [ 0.16101827 -1.87264472], action: [-1.     0.167]\n",
      "Episode: 259/3000, Reward: -13.939757741261841, done: False\n",
      "state: [ 0.31225285 -3.73288438], action: [-1.     0.167]\n",
      "Episode: 260/3000, Reward: -11.027752096123557, done: False\n",
      "state: [ 0.16402    -2.33468545], action: [-1.     0.167]\n",
      "Episode: 261/3000, Reward: -10.468508263869648, done: False\n",
      "state: [ 0.15334976 -2.90439671], action: [0.930064   0.11287285]\n",
      "Episode: 262/3000, Reward: -8.56809934814711, done: False\n",
      "state: [ 0.01060732 -4.5918866 ], action: [-0.99997526  0.167     ]\n",
      "Episode: 263/3000, Reward: -9.152402887825499, done: False\n",
      "state: [ 0.073527   -3.26085308], action: [-1.     0.167]\n",
      "Episode: 264/3000, Reward: -10.247951663300222, done: False\n",
      "state: [ 0.12988449 -2.23550973], action: [-1.     0.167]\n",
      "Episode: 265/3000, Reward: -10.104571633688463, done: False\n",
      "state: [ 0.13041196 -2.1641273 ], action: [-1.     0.167]\n",
      "Episode: 266/3000, Reward: -9.27811921473147, done: False\n",
      "state: [ 0.05557967 -4.10899328], action: [-0.9999999  0.167    ]\n",
      "Episode: 267/3000, Reward: -8.594110837066276, done: False\n",
      "state: [ 0.03732532 -3.69172061], action: [-1.     0.167]\n",
      "Episode: 268/3000, Reward: -8.980209562791096, done: False\n",
      "state: [ 0.03398389 -4.6923727 ], action: [-0.99988925  0.167     ]\n",
      "Episode: 269/3000, Reward: -9.003259207443694, done: False\n",
      "state: [ 0.06353714 -6.24880293], action: [0.28369942 0.05230796]\n",
      "Episode: 270/3000, Reward: -9.37910723554586, done: False\n",
      "state: [ 0.09665053 -2.94612807], action: [0.12553236 0.01276   ]\n",
      "Episode: 271/3000, Reward: -12.217976516394424, done: False\n",
      "state: [ 0.24518933 -3.37201533], action: [-1.     0.167]\n",
      "Episode: 272/3000, Reward: -8.629904447180516, done: False\n",
      "state: [ 0.02880476 -4.23275179], action: [-0.9999996  0.167    ]\n",
      "Episode: 273/3000, Reward: -11.096430219084485, done: False\n",
      "state: [ 0.149468   -3.16632746], action: [0.93972164 0.01352502]\n",
      "Episode: 274/3000, Reward: -8.78398146221561, done: False\n",
      "state: [ 0.0324649  -4.21885543], action: [-0.9999997  0.167    ]\n",
      "Episode: 275/3000, Reward: -9.11448124808364, done: False\n",
      "state: [ 0.04392237 -3.90255605], action: [-1.     0.167]\n",
      "Episode: 276/3000, Reward: -9.896581029099695, done: False\n",
      "state: [ 0.09876587 -2.89193342], action: [-1.     0.167]\n",
      "Episode: 277/3000, Reward: -9.59327527046446, done: False\n",
      "state: [ 0.0756018  -3.24986787], action: [-1.     0.167]\n",
      "Episode: 278/3000, Reward: -8.87744432122412, done: False\n",
      "state: [ 0.03686127 -6.27629658], action: [0.9999473 0.167    ]\n",
      "Episode: 279/3000, Reward: -9.701720378288424, done: False\n",
      "state: [ 0.08912676 -3.64341978], action: [-1.     0.167]\n",
      "Episode: 280/3000, Reward: -8.924670993713239, done: False\n",
      "state: [ 0.03487593 -4.09052992], action: [-0.99999994  0.167     ]\n",
      "Episode: 281/3000, Reward: -9.530367430604665, done: False\n",
      "state: [ 0.06220506 -4.16479989], action: [-0.9999998  0.167    ]\n",
      "Episode: 282/3000, Reward: -9.360523618951557, done: False\n",
      "state: [ 0.05880128 -6.04734825], action: [0.99919516 0.167     ]\n",
      "Episode: 283/3000, Reward: -9.089539994883449, done: False\n",
      "state: [ 0.0240417  -5.37374908], action: [-0.48674828  0.167     ]\n",
      "Episode: 284/3000, Reward: -9.454629686112835, done: False\n",
      "state: [ 0.07130235 -3.39326675], action: [-1.     0.167]\n",
      "Episode: 285/3000, Reward: -11.354872362104423, done: False\n",
      "state: [ 0.17315395 -2.96608296], action: [-1.     0.167]\n",
      "Episode: 286/3000, Reward: -9.06822275255989, done: False\n",
      "state: [ 0.03404613 -5.90911699], action: [0.99607533 0.167     ]\n",
      "Episode: 287/3000, Reward: -9.90317878551074, done: False\n",
      "state: [ 0.15456436 -7.07909236], action: [0.99999756 0.167     ]\n",
      "Episode: 288/3000, Reward: -10.494100402555821, done: False\n",
      "state: [ 0.15747606 -2.31822237], action: [-0.23255002  0.05074491]\n",
      "Episode: 289/3000, Reward: -9.89729634632734, done: False\n",
      "state: [ 0.10091729 -2.04263627], action: [-1.     0.167]\n",
      "Episode: 290/3000, Reward: -10.796973910103475, done: False\n",
      "state: [ 0.15375703 -2.7148917 ], action: [-1.     0.167]\n",
      "Episode: 291/3000, Reward: -9.213780328818585, done: False\n",
      "state: [ 0.02720072 -5.47862669], action: [0.47223556 0.167     ]\n",
      "Episode: 292/3000, Reward: -9.638240393829163, done: False\n",
      "state: [ 0.09504933 -6.39493085], action: [ 0.76349646 -0.02310338]\n",
      "Episode: 293/3000, Reward: -10.693634560183053, done: False\n",
      "state: [ 0.13234766 -2.61648893], action: [0.7647329  0.09854706]\n",
      "Episode: 294/3000, Reward: -9.335111404353924, done: False\n",
      "state: [ 0.08015553 -3.6673941 ], action: [-1.     0.167]\n",
      "Episode: 295/3000, Reward: -10.20711626787878, done: False\n",
      "state: [ 0.09566414 -3.20897877], action: [-1.     0.167]\n",
      "Episode: 296/3000, Reward: -9.521268941025054, done: False\n",
      "state: [ 0.14293775 -7.2854351 ], action: [-0.04188534 -0.10102038]\n",
      "Episode: 297/3000, Reward: -9.3124995619631, done: False\n",
      "state: [ 0.10057024 -6.50005454], action: [0.9999956 0.167    ]\n",
      "Episode: 298/3000, Reward: -9.631729102655532, done: False\n",
      "state: [ 0.1070621  -6.62976921], action: [0.10566688 0.13228561]\n",
      "Episode: 299/3000, Reward: -9.392257018642827, done: False\n",
      "state: [ 0.02506083 -4.84031538], action: [-0.9991595  0.167    ]\n",
      "Episode: 300/3000, Reward: -9.640245969979315, done: False\n",
      "state: [ 0.1128318  -6.73934583], action: [0.99999547 0.167     ]\n",
      "Episode: 301/3000, Reward: -9.653723956217375, done: False\n",
      "state: [ 0.12304887 -6.26449937], action: [0.9998663 0.167    ]\n",
      "Episode: 302/3000, Reward: -9.742247730398747, done: False\n",
      "state: [ 0.11811599 -6.90636416], action: [0.99999547 0.167     ]\n",
      "Episode: 303/3000, Reward: -10.000139987248792, done: False\n",
      "state: [ 0.23614974 -6.93379871], action: [0.999993 0.167   ]\n",
      "Episode: 304/3000, Reward: -9.42381779821046, done: False\n",
      "state: [ 0.11387064 -6.73436828], action: [-0.8397497   0.15012315]\n",
      "Episode: 305/3000, Reward: -9.231050988718266, done: False\n",
      "state: [ 0.0380333  -3.85606882], action: [-1.     0.167]\n",
      "Episode: 306/3000, Reward: -9.42467437738527, done: False\n",
      "state: [ 0.05458882 -5.94835601], action: [0.998423 0.167   ]\n",
      "Episode: 307/3000, Reward: -9.939987373432066, done: False\n",
      "state: [ 0.178357   -6.97589346], action: [0.99999595 0.167     ]\n",
      "Episode: 308/3000, Reward: -9.518114276931337, done: False\n",
      "state: [ 0.05395861 -4.09177289], action: [-0.99999994  0.167     ]\n",
      "Episode: 309/3000, Reward: -9.97842363571352, done: False\n",
      "state: [ 0.06363329 -4.41725992], action: [-0.99999905  0.167     ]\n",
      "Episode: 310/3000, Reward: -11.276385532002685, done: False\n",
      "state: [ 0.16749272 -2.66152406], action: [-1.     0.167]\n",
      "Episode: 311/3000, Reward: -9.82501479091861, done: False\n",
      "state: [ 0.03911807 -4.7693342 ], action: [-0.9996515  0.167    ]\n",
      "Episode: 312/3000, Reward: -9.839642089651882, done: False\n",
      "state: [ 0.11359888 -6.25026397], action: [0.9998313 0.167    ]\n",
      "Episode: 313/3000, Reward: -9.709879211708996, done: False\n",
      "state: [ 0.0832842  -6.09165952], action: [0.9992852 0.167    ]\n",
      "Episode: 314/3000, Reward: -9.897385770518493, done: False\n",
      "state: [ 0.1205741 -6.1353684], action: [0.9992417 0.167    ]\n",
      "Episode: 315/3000, Reward: -9.25472244995223, done: False\n",
      "state: [ 0.01049454 -5.2479097 ], action: [-0.7939332  0.167    ]\n",
      "Episode: 316/3000, Reward: -9.811727263118566, done: False\n",
      "state: [ 0.09435825 -6.12832812], action: [0.99949217 0.167     ]\n",
      "Episode: 317/3000, Reward: -10.292090484466058, done: False\n",
      "state: [ 0.0997143  -3.48111726], action: [-1.     0.167]\n",
      "Episode: 318/3000, Reward: -9.11235786051395, done: False\n",
      "state: [ 0.02306495 -5.46966392], action: [ 0.60832995 -0.13941094]\n",
      "Episode: 319/3000, Reward: -9.649008483436656, done: False\n",
      "state: [ 0.06565684 -5.95462527], action: [0.9972337 0.167    ]\n",
      "Episode: 320/3000, Reward: -9.98202484543112, done: False\n",
      "state: [ 0.19686431 -6.92087604], action: [-0.84849614 -0.07350586]\n",
      "Episode: 321/3000, Reward: -10.130044921266705, done: False\n",
      "state: [ 0.18410918 -6.68239232], action: [0.23344289 0.05702027]\n",
      "Episode: 322/3000, Reward: -10.247414869889457, done: False\n",
      "state: [ 0.22704072 -6.6474402 ], action: [0.9999594 0.167    ]\n",
      "Episode: 323/3000, Reward: -9.662296447477775, done: False\n",
      "state: [ 0.0344783 -5.296683 ], action: [-0.552586  0.167   ]\n",
      "Episode: 324/3000, Reward: -9.805099434633716, done: False\n",
      "state: [ 0.15031281 -6.46596618], action: [0.99998134 0.167     ]\n",
      "Episode: 325/3000, Reward: -10.217982259057056, done: False\n",
      "state: [ 0.07118011 -3.64450852], action: [-0.41910005  0.01434698]\n",
      "Episode: 326/3000, Reward: -10.068779366884657, done: False\n",
      "state: [ 0.22109654 -6.63086776], action: [0.99997795 0.167     ]\n",
      "Episode: 327/3000, Reward: -10.177063180303454, done: False\n",
      "state: [ 0.24759212 -6.81716941], action: [-0.6964799   0.05552359]\n",
      "Episode: 328/3000, Reward: -11.26896264244454, done: False\n",
      "state: [ 0.10012394 -4.28604125], action: [-0.6028164  -0.07474791]\n",
      "Episode: 329/3000, Reward: -10.547797308023211, done: False\n",
      "state: [ 0.17006668 -6.59195753], action: [0.99965334 0.167     ]\n",
      "Episode: 330/3000, Reward: -10.968917344633804, done: False\n",
      "state: [ 0.08932278 -4.56492268], action: [ 0.5751756  -0.02920677]\n",
      "Episode: 331/3000, Reward: -13.81508779355216, done: False\n",
      "state: [ 0.25860246 -4.05980906], action: [-1.     0.167]\n",
      "Episode: 332/3000, Reward: -10.780461336367384, done: False\n",
      "state: [ 0.10289482 -6.24250086], action: [0.99919003 0.167     ]\n",
      "Episode: 333/3000, Reward: -12.252327274637635, done: False\n",
      "state: [ 0.18067387 -3.30011539], action: [ 0.33117196 -0.07002871]\n",
      "Episode: 334/3000, Reward: -10.76735472442466, done: False\n",
      "state: [ 0.05731981 -4.94672007], action: [-0.9991459  0.167    ]\n",
      "Episode: 335/3000, Reward: -10.475611331672994, done: False\n",
      "state: [ 0.25831776 -6.10472628], action: [0.8778484 0.167    ]\n",
      "Episode: 336/3000, Reward: -11.711377748057075, done: False\n",
      "state: [ 0.11304916 -4.18237656], action: [-1.     0.167]\n",
      "Episode: 337/3000, Reward: -10.394905888100068, done: False\n",
      "state: [ 0.05334059 -4.98750596], action: [0.9327707  0.10564213]\n",
      "Episode: 338/3000, Reward: -12.479351582756584, done: False\n",
      "state: [ 0.1904192  -3.23576278], action: [ 0.28872368 -0.09777851]\n",
      "Episode: 339/3000, Reward: -10.051889700379075, done: False\n",
      "state: [ 0.05164034 -4.42991685], action: [-0.9999988  0.167    ]\n",
      "Episode: 340/3000, Reward: -10.984869203123276, done: False\n",
      "state: [ 0.11124943 -3.08872239], action: [-0.16002835 -0.03051678]\n",
      "Episode: 341/3000, Reward: -10.476675821200418, done: False\n",
      "state: [ 0.04868433 -5.027462  ], action: [-0.9973136  0.167    ]\n",
      "Episode: 342/3000, Reward: -10.737103407994297, done: False\n",
      "state: [ 0.06746963 -4.53845817], action: [-0.9999952  0.167    ]\n",
      "Episode: 343/3000, Reward: -11.350752553537234, done: False\n",
      "state: [ 0.10274147 -4.29108365], action: [-0.9999999  0.167    ]\n",
      "Episode: 344/3000, Reward: -10.851274191499856, done: False\n",
      "state: [ 0.17647295 -6.5531939 ], action: [0.99959123 0.167     ]\n",
      "Episode: 345/3000, Reward: -10.77210700470166, done: False\n",
      "state: [ 0.10760637 -6.09153203], action: [ 0.73278    -0.08603588]\n",
      "Episode: 346/3000, Reward: -11.427609736727613, done: False\n",
      "state: [ 0.11238064 -4.30502896], action: [-0.99999994  0.167     ]\n",
      "Episode: 347/3000, Reward: -10.391371820658003, done: False\n",
      "state: [ 0.06280135 -4.85796353], action: [-0.99987835  0.167     ]\n",
      "Episode: 348/3000, Reward: -12.149588785104731, done: False\n",
      "state: [ 0.13323208 -3.88458669], action: [0.84036714 0.054079  ]\n",
      "Episode: 349/3000, Reward: -11.065656573129878, done: False\n",
      "state: [ 0.07967883 -5.18450252], action: [-0.9899968  0.167    ]\n",
      "Episode: 350/3000, Reward: -10.732958051660844, done: False\n",
      "state: [ 0.05173569 -4.96472283], action: [-0.999324  0.167   ]\n",
      "Episode: 351/3000, Reward: -10.70498367130347, done: False\n",
      "state: [ 0.06679814 -4.6444089 ], action: [-0.9999929  0.167    ]\n",
      "Episode: 352/3000, Reward: -11.179736060143844, done: False\n",
      "state: [ 0.12357732 -6.16265902], action: [0.9970394 0.167    ]\n",
      "Episode: 353/3000, Reward: -11.486922700709169, done: False\n",
      "state: [ 0.11167119 -3.94273244], action: [0.66686904 0.00400163]\n",
      "Episode: 354/3000, Reward: -10.798957649444462, done: False\n",
      "state: [ 0.05022442 -5.36778401], action: [0.9327844  0.00146214]\n",
      "Episode: 355/3000, Reward: -10.66916709645788, done: False\n",
      "state: [ 0.06630803 -5.37320819], action: [-0.84627557  0.167     ]\n",
      "Episode: 356/3000, Reward: -11.394759349848961, done: False\n",
      "state: [ 0.10334601 -4.55375905], action: [-0.9999985  0.167    ]\n",
      "Episode: 357/3000, Reward: -10.87552370353541, done: False\n",
      "state: [ 0.13768364 -6.31323862], action: [0.9991908 0.167    ]\n",
      "Episode: 358/3000, Reward: -10.980078252950506, done: False\n",
      "state: [ 0.06936597 -4.58249505], action: [-0.9999963  0.167    ]\n",
      "Episode: 359/3000, Reward: -10.716968395121077, done: False\n",
      "state: [ 0.05483908 -5.54568162], action: [0.21802841 0.167     ]\n",
      "Episode: 360/3000, Reward: -10.996110867882448, done: False\n",
      "state: [ 0.1911192  -6.72455865], action: [0.9993082 0.167    ]\n",
      "Episode: 361/3000, Reward: -10.968924281350624, done: False\n",
      "state: [ 0.09154339 -5.86062484], action: [0.9560068 0.167    ]\n",
      "Episode: 362/3000, Reward: -14.378086017814102, done: False\n",
      "state: [ 0.26635988 -4.51553166], action: [-0.99999994  0.167     ]\n",
      "Episode: 363/3000, Reward: -11.138255826559984, done: False\n",
      "state: [ 0.06788624 -5.32980528], action: [-0.88994455  0.167     ]\n",
      "Episode: 364/3000, Reward: -11.048653931065697, done: False\n",
      "state: [ 0.2260819  -6.34579806], action: [0.99426174 0.11161546]\n",
      "Episode: 365/3000, Reward: -10.777184548354436, done: False\n",
      "state: [ 0.12193572 -6.31142653], action: [0.99919474 0.167     ]\n",
      "Episode: 366/3000, Reward: -11.030303740416626, done: False\n",
      "state: [ 0.18272821 -6.4138826 ], action: [0.9989231 0.167    ]\n",
      "Episode: 367/3000, Reward: -11.325852111234704, done: False\n",
      "state: [ 0.1733482  -6.40167571], action: [0.9991464 0.167    ]\n",
      "Episode: 368/3000, Reward: -11.168359080243619, done: False\n",
      "state: [ 0.06243811 -5.34762259], action: [-0.80149245  0.167     ]\n",
      "Episode: 369/3000, Reward: -10.941758819613213, done: False\n",
      "state: [ 0.25613878 -6.22072868], action: [0.90332294 0.167     ]\n",
      "Episode: 370/3000, Reward: -12.427396606827283, done: False\n",
      "state: [ 0.15527536 -4.01931507], action: [-1.     0.167]\n",
      "Episode: 371/3000, Reward: -11.141136809871256, done: False\n",
      "state: [ 0.20440376 -6.28641813], action: [0.98594433 0.167     ]\n",
      "Episode: 372/3000, Reward: -10.88163731494884, done: False\n",
      "state: [ 0.21164643 -6.40867746], action: [0.29244632 0.15947583]\n",
      "Episode: 373/3000, Reward: -10.758815598570473, done: False\n",
      "state: [ 0.23029191 -6.28384908], action: [0.9786189 0.167    ]\n",
      "Episode: 374/3000, Reward: -10.823825435699563, done: False\n",
      "state: [ 0.24495709 -6.08961229], action: [ 0.5682431  -0.08489335]\n",
      "Episode: 375/3000, Reward: -10.877022674638765, done: False\n",
      "state: [ 0.23354722 -6.22018881], action: [0.95805466 0.167     ]\n",
      "Episode: 376/3000, Reward: -10.915910842853894, done: False\n",
      "state: [ 0.13462605 -6.0327506 ], action: [0.99323666 0.167     ]\n",
      "Episode: 377/3000, Reward: -10.526000297855848, done: False\n",
      "state: [ 0.22456528 -5.79281178], action: [-0.1719146  0.167    ]\n",
      "Episode: 378/3000, Reward: -10.513705747158221, done: False\n",
      "state: [ 0.24309807 -5.98728717], action: [0.54992485 0.167     ]\n",
      "Episode: 379/3000, Reward: -10.893211031904853, done: False\n",
      "state: [ 0.172759   -6.26767481], action: [0.99898225 0.167     ]\n",
      "Episode: 380/3000, Reward: -10.56220784511817, done: False\n",
      "state: [ 0.24423767 -5.92421424], action: [0.3967408  0.06180745]\n",
      "Episode: 381/3000, Reward: -10.342240397879392, done: False\n",
      "state: [ 0.2109703  -5.70807699], action: [-0.27905837  0.167     ]\n",
      "Episode: 382/3000, Reward: -10.231516893668093, done: False\n",
      "state: [ 0.21070288 -5.67720798], action: [0.06937467 0.167     ]\n",
      "Episode: 383/3000, Reward: -10.863490097556403, done: False\n",
      "state: [ 0.23017624 -5.8779977 ], action: [0.27237266 0.167     ]\n",
      "Episode: 384/3000, Reward: -10.278305916070291, done: False\n",
      "state: [ 0.20449688 -5.69710142], action: [0.8774441  0.04105252]\n",
      "Episode: 385/3000, Reward: -10.14519383248689, done: False\n",
      "state: [ 0.20322121 -5.73236304], action: [0.08554151 0.167     ]\n",
      "Episode: 386/3000, Reward: -10.416137560725373, done: False\n",
      "state: [ 0.21174605 -5.72172156], action: [0.0506278 0.167    ]\n",
      "Episode: 387/3000, Reward: -9.985374814066425, done: False\n",
      "state: [ 0.20095426 -5.71503742], action: [0.05841015 0.167     ]\n",
      "Episode: 388/3000, Reward: -10.445260895649449, done: False\n",
      "state: [ 0.22870767 -5.79750074], action: [-0.01304872  0.167     ]\n",
      "Episode: 389/3000, Reward: -9.971130340977805, done: False\n",
      "state: [ 0.20015306 -5.71071578], action: [0.11459727 0.167     ]\n",
      "Episode: 390/3000, Reward: -10.03430775149454, done: False\n",
      "state: [ 0.20047671 -5.68652527], action: [0.01984124 0.167     ]\n",
      "Episode: 391/3000, Reward: -9.859296297189005, done: False\n",
      "state: [ 0.20250066 -5.71949049], action: [0.27066246 0.167     ]\n",
      "Episode: 392/3000, Reward: -10.10304791001029, done: False\n",
      "state: [ 0.21593596 -5.66520025], action: [-0.07887774  0.167     ]\n",
      "Episode: 393/3000, Reward: -9.73157999057777, done: False\n",
      "state: [ 0.19984294 -5.69992489], action: [0.21450885 0.167     ]\n",
      "Episode: 394/3000, Reward: -10.012477413347352, done: False\n",
      "state: [ 0.20115804 -5.6669682 ], action: [-0.02174402  0.167     ]\n",
      "Episode: 395/3000, Reward: -9.798417319080537, done: False\n",
      "state: [ 0.19588906 -5.69734101], action: [-0.3227168   0.01409176]\n",
      "Episode: 396/3000, Reward: -9.713579843066034, done: False\n",
      "state: [ 0.2058194 -5.7198396], action: [0.19043249 0.167     ]\n",
      "Episode: 397/3000, Reward: -9.68760606482126, done: False\n",
      "state: [ 0.19884511 -5.70814348], action: [0.3556666  0.10703604]\n",
      "Episode: 398/3000, Reward: -9.661134349728028, done: False\n",
      "state: [ 0.19735176 -5.67067404], action: [-0.8325043  -0.01478663]\n",
      "Episode: 399/3000, Reward: -9.878436763735856, done: False\n",
      "state: [ 0.21212995 -5.65028295], action: [0.37147504 0.167     ]\n",
      "Episode: 400/3000, Reward: -9.78128468445774, done: False\n",
      "state: [ 0.19951089 -5.65478374], action: [0.10823757 0.167     ]\n",
      "Episode: 401/3000, Reward: -9.690907979144978, done: False\n",
      "state: [ 0.19871644 -5.65378189], action: [-0.1666257  -0.09249473]\n",
      "Episode: 402/3000, Reward: -9.544473705059856, done: False\n",
      "state: [ 0.19940899 -5.6951691 ], action: [0.34074968 0.167     ]\n",
      "Episode: 403/3000, Reward: -9.59912201845437, done: False\n",
      "state: [ 0.20237964 -5.67825484], action: [0.18527833 0.167     ]\n",
      "Episode: 404/3000, Reward: -9.649006849100557, done: False\n",
      "state: [ 0.19960045 -5.65412169], action: [0.10914052 0.167     ]\n",
      "Episode: 405/3000, Reward: -9.162709906470273, done: False\n",
      "state: [ 0.19821512 -5.63239415], action: [0.18347125 0.167     ]\n",
      "Episode: 406/3000, Reward: -9.422623977011755, done: False\n",
      "state: [ 0.19697502 -5.63360135], action: [0.26453003 0.167     ]\n",
      "Episode: 407/3000, Reward: -9.417538857139405, done: False\n",
      "state: [ 0.20043748 -5.64174442], action: [0.21666384 0.167     ]\n",
      "Episode: 408/3000, Reward: -9.434667589610182, done: False\n",
      "state: [ 0.19885741 -5.5584846 ], action: [-0.12435256  0.167     ]\n",
      "Episode: 409/3000, Reward: -9.268406941736599, done: False\n",
      "state: [ 0.18968451 -5.58536123], action: [0.20172752 0.167     ]\n",
      "Episode: 410/3000, Reward: -9.28113194816753, done: False\n",
      "state: [ 0.19996896 -5.73318047], action: [0.48215136 0.167     ]\n",
      "Episode: 411/3000, Reward: -9.392679457857886, done: False\n",
      "state: [ 0.19750177 -5.60671926], action: [0.18112533 0.167     ]\n",
      "Episode: 412/3000, Reward: -9.350365201162234, done: False\n",
      "state: [ 0.1980335  -5.60144329], action: [0.18027729 0.167     ]\n",
      "Episode: 413/3000, Reward: -9.267819131832432, done: False\n",
      "state: [ 0.1934993  -5.57682535], action: [0.18676966 0.167     ]\n",
      "Episode: 414/3000, Reward: -9.275578792354889, done: False\n",
      "state: [ 0.19911783 -5.59746434], action: [0.38526925 0.167     ]\n",
      "Episode: 415/3000, Reward: -9.265810813248141, done: False\n",
      "state: [ 0.19475866 -5.58127049], action: [0.15365273 0.167     ]\n",
      "Episode: 416/3000, Reward: -9.293342688777095, done: False\n",
      "state: [ 0.19670253 -5.5840263 ], action: [0.15939786 0.167     ]\n",
      "Episode: 417/3000, Reward: -9.177174990602072, done: False\n",
      "state: [ 0.19638881 -5.55791678], action: [0.15174563 0.167     ]\n",
      "Episode: 418/3000, Reward: -9.205706147013592, done: False\n",
      "state: [ 0.2016876  -5.56152584], action: [0.24777606 0.167     ]\n",
      "Episode: 419/3000, Reward: -9.114283527983009, done: False\n",
      "state: [ 0.19312173 -5.53630934], action: [0.17607193 0.167     ]\n",
      "Episode: 420/3000, Reward: -9.109038182054391, done: False\n",
      "state: [ 0.19674101 -5.55132079], action: [0.19513516 0.167     ]\n",
      "Episode: 421/3000, Reward: -9.121820135946827, done: False\n",
      "state: [ 0.19471621 -5.5437197 ], action: [0.15531327 0.167     ]\n",
      "Episode: 422/3000, Reward: -9.063641505530741, done: False\n",
      "state: [ 0.19171325 -5.54824174], action: [0.27230468 0.167     ]\n",
      "Episode: 423/3000, Reward: -9.038847351250768, done: False\n",
      "state: [ 0.18987388 -5.5312818 ], action: [-0.61968446 -0.13750215]\n",
      "Episode: 424/3000, Reward: -9.117015370470241, done: False\n",
      "state: [ 0.19568664 -5.56609136], action: [0.29297197 0.167     ]\n",
      "Episode: 425/3000, Reward: -9.055607578451477, done: False\n",
      "state: [ 0.19716275 -5.53858285], action: [-0.06013466  0.03572011]\n",
      "Episode: 426/3000, Reward: -8.978940856922371, done: False\n",
      "state: [ 0.19923906 -5.50667623], action: [0.27731493 0.167     ]\n",
      "Episode: 427/3000, Reward: -8.88989144370863, done: False\n",
      "state: [ 0.19396962 -5.49854113], action: [0.1646361 0.167    ]\n",
      "Episode: 428/3000, Reward: -8.93808300594751, done: False\n",
      "state: [ 0.18906167 -5.45498747], action: [0.09859567 0.167     ]\n",
      "Episode: 429/3000, Reward: -8.857159943545108, done: False\n",
      "state: [ 0.19456946 -5.47157193], action: [0.13574046 0.167     ]\n",
      "Episode: 430/3000, Reward: -8.85798605796194, done: False\n",
      "state: [ 0.19180862 -5.48990883], action: [0.20552558 0.167     ]\n",
      "Episode: 431/3000, Reward: -8.846891282468786, done: False\n",
      "state: [ 0.18644222 -5.48204247], action: [0.36980826 0.167     ]\n",
      "Episode: 432/3000, Reward: -8.892494150053732, done: False\n",
      "state: [ 0.19227627 -5.47790689], action: [0.15664406 0.167     ]\n",
      "Episode: 433/3000, Reward: -8.769936210910515, done: False\n",
      "state: [ 0.18756923 -5.44357433], action: [0.13502339 0.167     ]\n",
      "Episode: 434/3000, Reward: -8.825679113785203, done: False\n",
      "state: [ 0.19104669 -5.47157637], action: [-0.64382464  0.09562027]\n",
      "Episode: 435/3000, Reward: -8.804994109520468, done: False\n",
      "state: [ 0.19031456 -5.46867935], action: [0.26706776 0.167     ]\n",
      "Episode: 436/3000, Reward: -8.71431342919525, done: False\n",
      "state: [ 0.1909834  -5.44182395], action: [0.11196631 0.167     ]\n",
      "Episode: 437/3000, Reward: -8.725487298869226, done: False\n",
      "state: [ 0.19134124 -5.44464477], action: [0.20335923 0.167     ]\n",
      "Episode: 438/3000, Reward: -8.726678961352953, done: False\n",
      "state: [ 0.19013883 -5.42665119], action: [0.09123496 0.167     ]\n",
      "Episode: 439/3000, Reward: -8.662721215559541, done: False\n",
      "state: [ 0.19059492 -5.44609785], action: [0.24375041 0.167     ]\n",
      "Episode: 440/3000, Reward: -8.600101524472652, done: False\n",
      "state: [ 0.18784713 -5.40756709], action: [0.27347293 0.167     ]\n",
      "Episode: 441/3000, Reward: -8.634065052027754, done: False\n",
      "state: [ 0.1902674  -5.40815066], action: [ 0.7617385  -0.07089071]\n",
      "Episode: 442/3000, Reward: -8.607923983144532, done: False\n",
      "state: [ 0.18898188 -5.40169189], action: [0.17659742 0.167     ]\n",
      "Episode: 443/3000, Reward: -8.571677848491644, done: False\n",
      "state: [ 0.18845715 -5.38166311], action: [0.1501738 0.167    ]\n",
      "Episode: 444/3000, Reward: -8.488393032427423, done: False\n",
      "state: [ 0.18533361 -5.3563002 ], action: [0.22625974 0.167     ]\n",
      "Episode: 445/3000, Reward: -8.406044016796502, done: False\n",
      "state: [ 0.1863561  -5.34697872], action: [0.20453109 0.167     ]\n",
      "Episode: 446/3000, Reward: -8.466370722821669, done: False\n",
      "state: [ 0.18611854 -5.34233214], action: [0.14755791 0.167     ]\n",
      "Episode: 447/3000, Reward: -8.40810842505035, done: False\n",
      "state: [ 0.18368584 -5.32511634], action: [0.23657086 0.167     ]\n",
      "Episode: 448/3000, Reward: -8.34887297563728, done: False\n",
      "state: [ 0.18540504 -5.31273348], action: [0.24257143 0.167     ]\n",
      "Episode: 449/3000, Reward: -8.318192979609709, done: False\n",
      "state: [ 0.18437539 -5.27610236], action: [0.21271363 0.167     ]\n",
      "Episode: 450/3000, Reward: -8.26126559393393, done: False\n",
      "state: [ 0.18530712 -5.2714691 ], action: [0.15329616 0.167     ]\n",
      "Episode: 451/3000, Reward: -8.186640095035928, done: False\n",
      "state: [ 0.18329207 -5.25986601], action: [0.17469618 0.167     ]\n",
      "Episode: 452/3000, Reward: -8.104985855435265, done: False\n",
      "state: [ 0.1894426  -5.22922138], action: [0.27274716 0.167     ]\n",
      "Episode: 453/3000, Reward: -8.056761533091846, done: False\n",
      "state: [ 0.1802871  -5.20721586], action: [0.20303807 0.167     ]\n",
      "Episode: 454/3000, Reward: -8.030269004275777, done: False\n",
      "state: [ 0.17973429 -5.19666501], action: [0.21730533 0.167     ]\n",
      "Episode: 455/3000, Reward: -7.984969033955504, done: False\n",
      "state: [ 0.18119211 -5.1794344 ], action: [0.1521845 0.167    ]\n",
      "Episode: 456/3000, Reward: -7.932956556745634, done: False\n",
      "state: [ 0.17889809 -5.14596637], action: [0.1650585 0.167    ]\n",
      "Episode: 457/3000, Reward: -7.866980367589981, done: False\n",
      "state: [ 0.1770067  -5.12460912], action: [ 0.36715883 -0.15386964]\n",
      "Episode: 458/3000, Reward: -7.868913426059362, done: False\n",
      "state: [ 0.17591872 -5.13271293], action: [0.21100381 0.167     ]\n",
      "Episode: 459/3000, Reward: -7.8460672559787055, done: False\n",
      "state: [ 0.1798301 -5.1204445], action: [0.1184314 0.167    ]\n",
      "Episode: 460/3000, Reward: -7.831598683468142, done: False\n",
      "state: [ 0.17548411 -5.11790882], action: [0.2204169 0.167    ]\n",
      "Episode: 461/3000, Reward: -7.816332094088983, done: False\n",
      "state: [ 0.17521166 -5.12302007], action: [0.30693382 0.167     ]\n",
      "Episode: 462/3000, Reward: -7.770211717705473, done: False\n",
      "state: [ 0.17782213 -5.08804302], action: [0.11777866 0.167     ]\n",
      "Episode: 463/3000, Reward: -7.6827770709929775, done: False\n",
      "state: [ 0.17627436 -5.07382029], action: [0.2325292 0.167    ]\n",
      "Episode: 464/3000, Reward: -7.604533705438335, done: False\n",
      "state: [ 0.17303086 -5.01886048], action: [-0.584301   -0.08089057]\n",
      "Episode: 465/3000, Reward: -7.563525024330836, done: False\n",
      "state: [ 0.17652586 -5.03256337], action: [0.14017045 0.167     ]\n",
      "Episode: 466/3000, Reward: -7.6009231599300335, done: False\n",
      "state: [ 0.17462821 -5.040912  ], action: [0.03859068 0.167     ]\n",
      "Episode: 467/3000, Reward: -7.520597241457905, done: False\n",
      "state: [ 0.17529691 -5.01973509], action: [0.1274433 0.167    ]\n",
      "Episode: 468/3000, Reward: -7.490276602167197, done: False\n",
      "state: [ 0.17444571 -5.02021554], action: [0.18043533 0.167     ]\n",
      "Episode: 469/3000, Reward: -7.512244873832532, done: False\n",
      "state: [ 0.17330624 -5.00348871], action: [0.14726001 0.167     ]\n",
      "Episode: 470/3000, Reward: -7.503709837170099, done: False\n",
      "state: [ 0.17453138 -4.9947532 ], action: [0.09752834 0.167     ]\n",
      "Episode: 471/3000, Reward: -7.481509362620715, done: False\n",
      "state: [ 0.17314908 -4.98753041], action: [0.14236768 0.167     ]\n",
      "Episode: 472/3000, Reward: -7.446476767853036, done: False\n",
      "state: [ 0.17112308 -4.98716806], action: [ 0.22257741 -0.09422895]\n",
      "Episode: 473/3000, Reward: -7.394759366310999, done: False\n",
      "state: [ 0.17132347 -4.95122355], action: [0.12932765 0.167     ]\n",
      "Episode: 474/3000, Reward: -7.34865263288365, done: False\n",
      "state: [ 0.17011178 -4.91842342], action: [0.04735774 0.167     ]\n",
      "Episode: 475/3000, Reward: -7.301652613524276, done: False\n",
      "state: [ 0.16918796 -4.92279108], action: [0.14045641 0.167     ]\n",
      "Episode: 476/3000, Reward: -7.288872287144066, done: False\n",
      "state: [ 0.17233478 -4.92531962], action: [0.11440947 0.167     ]\n",
      "Episode: 477/3000, Reward: -7.266124117743466, done: False\n",
      "state: [ 0.17096963 -4.93063207], action: [0.30356514 0.167     ]\n",
      "Episode: 478/3000, Reward: -7.186596165069723, done: False\n",
      "state: [ 0.16748055 -4.88510341], action: [0.17782246 0.167     ]\n",
      "Episode: 479/3000, Reward: -7.170217391224944, done: False\n",
      "state: [ 0.1697137  -4.89000483], action: [0.14819321 0.167     ]\n",
      "Episode: 480/3000, Reward: -7.184591321384742, done: False\n",
      "state: [ 0.16802044 -4.88888378], action: [0.22927484 0.167     ]\n",
      "Episode: 481/3000, Reward: -7.138138877872664, done: False\n",
      "state: [ 0.16688049 -4.88555281], action: [0.16668336 0.167     ]\n",
      "Episode: 482/3000, Reward: -7.133557707841468, done: False\n",
      "state: [ 0.16763455 -4.87495229], action: [0.2558562 0.167    ]\n",
      "Episode: 483/3000, Reward: -7.087298995697905, done: False\n",
      "state: [ 0.16492406 -4.84738106], action: [0.18112788 0.167     ]\n",
      "Episode: 484/3000, Reward: -7.087380717799073, done: False\n",
      "state: [ 0.17045698 -4.86821352], action: [0.18317707 0.167     ]\n",
      "Episode: 485/3000, Reward: -7.061457576316183, done: False\n",
      "state: [ 0.16727934 -4.84212531], action: [-0.12333073  0.04717334]\n",
      "Episode: 486/3000, Reward: -7.05373744317966, done: False\n",
      "state: [ 0.16778094 -4.83650103], action: [0.10406111 0.167     ]\n",
      "Episode: 487/3000, Reward: -7.055860887905663, done: False\n",
      "state: [ 0.16882158 -4.84057975], action: [0.06200862 0.167     ]\n",
      "Episode: 488/3000, Reward: -7.055308024989095, done: False\n",
      "state: [ 0.17080609 -4.8455219 ], action: [0.15464288 0.167     ]\n",
      "Episode: 489/3000, Reward: -7.047275236082095, done: False\n",
      "state: [ 0.16779261 -4.84776482], action: [0.18293951 0.167     ]\n",
      "Episode: 490/3000, Reward: -7.02167080171159, done: False\n",
      "state: [ 0.16604799 -4.82404981], action: [0.13990982 0.167     ]\n",
      "Episode: 491/3000, Reward: -6.99684026143037, done: False\n",
      "state: [ 0.16432995 -4.81650621], action: [0.0357029 0.167    ]\n",
      "Episode: 492/3000, Reward: -7.000169701474838, done: False\n",
      "state: [ 0.16835354 -4.81590551], action: [0.07730421 0.167     ]\n",
      "Episode: 493/3000, Reward: -6.981210824490289, done: False\n",
      "state: [ 0.1644    -4.8164202], action: [0.17903021 0.167     ]\n",
      "Episode: 494/3000, Reward: -6.98037142524715, done: False\n",
      "state: [ 0.16645303 -4.79922488], action: [0.10067815 0.167     ]\n",
      "Episode: 495/3000, Reward: -6.961645351650326, done: False\n",
      "state: [ 0.16539511 -4.82198334], action: [0.23774758 0.167     ]\n",
      "Episode: 496/3000, Reward: -6.968809683675569, done: False\n",
      "state: [ 0.16315268 -4.82066491], action: [0.30527675 0.167     ]\n",
      "Episode: 497/3000, Reward: -6.960880707930512, done: False\n",
      "state: [ 0.16532369 -4.80254583], action: [0.10950822 0.167     ]\n",
      "Episode: 498/3000, Reward: -6.948916689732106, done: False\n",
      "state: [ 0.16488372 -4.80350195], action: [0.19118749 0.167     ]\n",
      "Episode: 499/3000, Reward: -6.93926557550926, done: False\n",
      "state: [ 0.16406582 -4.79995068], action: [0.16354527 0.167     ]\n",
      "Episode: 500/3000, Reward: -6.93331969644692, done: False\n",
      "state: [ 0.16666379 -4.81033431], action: [0.24362724 0.167     ]\n",
      "Episode: 501/3000, Reward: -6.908610728230054, done: False\n",
      "state: [ 0.16416397 -4.78421674], action: [0.17744625 0.167     ]\n",
      "Episode: 502/3000, Reward: -6.906003439366883, done: False\n",
      "state: [ 0.16906888 -4.79612404], action: [ 0.834597  -0.1269571]\n",
      "Episode: 503/3000, Reward: -6.920200886872153, done: False\n",
      "state: [ 0.16537015 -4.80938474], action: [0.18915588 0.167     ]\n",
      "Episode: 504/3000, Reward: -6.945956452599669, done: False\n",
      "state: [ 0.1631906  -4.80368045], action: [-0.00531566  0.167     ]\n",
      "Episode: 505/3000, Reward: -6.962938159825657, done: False\n",
      "state: [ 0.16461074 -4.80455068], action: [0.14472443 0.167     ]\n",
      "Episode: 506/3000, Reward: -6.969885947367842, done: False\n",
      "state: [ 0.16600929 -4.81682149], action: [0.12559439 0.167     ]\n",
      "Episode: 507/3000, Reward: -7.054480398507517, done: False\n",
      "state: [ 0.1675429  -4.85238554], action: [0.12219331 0.167     ]\n",
      "Episode: 508/3000, Reward: -7.081544914952299, done: False\n",
      "state: [ 0.16371647 -4.83983449], action: [0.17619036 0.167     ]\n",
      "Episode: 509/3000, Reward: -7.0620080384622055, done: False\n",
      "state: [ 0.16734262 -4.84404236], action: [0.12715693 0.167     ]\n",
      "Episode: 510/3000, Reward: -7.059911189555612, done: False\n",
      "state: [ 0.16637746 -4.83847054], action: [0.11128503 0.167     ]\n",
      "Episode: 511/3000, Reward: -7.213936773973318, done: False\n",
      "state: [ 0.16862976 -4.91822157], action: [0.1755389 0.167    ]\n",
      "Episode: 512/3000, Reward: -7.274786876303846, done: False\n",
      "state: [ 0.16734093 -4.90208109], action: [0.16051342 0.167     ]\n",
      "Episode: 513/3000, Reward: -7.246112784260566, done: False\n",
      "state: [ 0.1699885  -4.90640699], action: [0.10856134 0.167     ]\n",
      "Episode: 514/3000, Reward: -7.230255044736215, done: False\n",
      "state: [ 0.16992408 -4.92016324], action: [0.24434012 0.167     ]\n",
      "Episode: 515/3000, Reward: -7.236546494152641, done: False\n",
      "state: [ 0.16560942 -4.90279907], action: [0.09032355 0.167     ]\n",
      "Episode: 516/3000, Reward: -7.223149225285591, done: False\n",
      "state: [ 0.16659831 -4.87610787], action: [0.14822641 0.167     ]\n",
      "Episode: 517/3000, Reward: -7.189645305184524, done: False\n",
      "state: [ 0.16722213 -4.88553164], action: [0.16880511 0.167     ]\n",
      "Episode: 518/3000, Reward: -7.179910054943149, done: False\n",
      "state: [ 0.17131097 -4.90041203], action: [0.2511771 0.167    ]\n",
      "Episode: 519/3000, Reward: -7.170402464331887, done: False\n",
      "state: [ 0.16636251 -4.90014359], action: [0.28286538 0.167     ]\n",
      "Episode: 520/3000, Reward: -7.169139843065412, done: False\n",
      "state: [ 0.16709026 -4.88565381], action: [0.20331164 0.167     ]\n",
      "Episode: 521/3000, Reward: -7.172661775735851, done: False\n",
      "state: [ 0.16580721 -4.87964876], action: [0.22809601 0.167     ]\n",
      "Episode: 522/3000, Reward: -7.163270889980202, done: False\n",
      "state: [ 0.16576683 -4.87085308], action: [0.131589 0.167   ]\n",
      "Episode: 523/3000, Reward: -7.156950366766931, done: False\n",
      "state: [ 0.16954106 -4.87199684], action: [0.06979571 0.167     ]\n",
      "Episode: 524/3000, Reward: -7.140161533958267, done: False\n",
      "state: [ 0.162484   -4.86895936], action: [0.29046226 0.167     ]\n",
      "Episode: 525/3000, Reward: -6.767681755680703, done: False\n",
      "state: [ 0.14602549 -4.3693383 ], action: [0.31530997 0.167     ]\n",
      "Episode: 526/3000, Reward: -5.766475839450765, done: False\n",
      "state: [ 0.1473716  -4.33715216], action: [0.1248586 0.167    ]\n",
      "Episode: 527/3000, Reward: -5.766262143958318, done: False\n",
      "state: [ 0.14897001 -4.34577374], action: [0.00374258 0.167     ]\n",
      "Episode: 528/3000, Reward: -5.781329218341867, done: False\n",
      "state: [ 0.14525515 -4.34933533], action: [0.19640414 0.167     ]\n",
      "Episode: 529/3000, Reward: -5.774902174753258, done: False\n",
      "state: [ 0.14542405 -4.33887286], action: [0.1710037 0.167    ]\n",
      "Episode: 530/3000, Reward: -5.751905395859232, done: False\n",
      "state: [ 0.14485004 -4.32721249], action: [-0.8835309   0.05207201]\n",
      "Episode: 531/3000, Reward: -5.7561379324354345, done: False\n",
      "state: [ 0.15098003 -4.32993756], action: [0.15904257 0.167     ]\n",
      "Episode: 532/3000, Reward: -5.749817568573905, done: False\n",
      "state: [ 0.14409305 -4.33888374], action: [0.09994438 0.167     ]\n",
      "Episode: 533/3000, Reward: -5.778686578576901, done: False\n",
      "state: [ 0.14915225 -4.34889819], action: [0.07351159 0.167     ]\n",
      "Episode: 534/3000, Reward: -5.762742269055787, done: False\n",
      "state: [ 0.14823837 -4.35060816], action: [0.15884952 0.167     ]\n",
      "Episode: 535/3000, Reward: -5.726800355408665, done: False\n",
      "state: [ 0.14994553 -4.33458461], action: [0.30199862 0.167     ]\n",
      "Episode: 536/3000, Reward: -5.720205106393706, done: False\n",
      "state: [ 0.14695631 -4.32602383], action: [0.11330375 0.167     ]\n",
      "Episode: 537/3000, Reward: -5.724822198399951, done: False\n",
      "state: [ 0.14669396 -4.33824803], action: [0.2007338 0.167    ]\n",
      "Episode: 538/3000, Reward: -5.708195530714896, done: False\n",
      "state: [ 0.1518486 -4.246881 ], action: [-0.1702466  0.167    ]\n",
      "Episode: 539/3000, Reward: -5.564827103559215, done: False\n",
      "state: [ 0.14570988 -4.30711215], action: [0.6257185  0.03271364]\n",
      "Episode: 540/3000, Reward: -5.6711128608786545, done: False\n",
      "state: [ 0.14596124 -4.30703138], action: [0.1176086 0.167    ]\n",
      "Episode: 541/3000, Reward: -5.665140204049462, done: False\n",
      "state: [ 0.14495358 -4.30881109], action: [0.1504219 0.167    ]\n",
      "Episode: 542/3000, Reward: -5.669831809604094, done: False\n",
      "state: [ 0.14397569 -4.29765096], action: [0.17363517 0.167     ]\n",
      "Episode: 543/3000, Reward: -5.6678635257440355, done: False\n",
      "state: [ 0.1399361  -4.32094492], action: [0.13299142 0.167     ]\n",
      "Episode: 544/3000, Reward: -5.74917403863204, done: False\n",
      "state: [ 0.14403301 -4.29491465], action: [0.13640352 0.167     ]\n",
      "Episode: 545/3000, Reward: -5.475588882866991, done: False\n",
      "state: [ 0.14085252 -4.22533238], action: [0.1639017 0.167    ]\n",
      "Episode: 546/3000, Reward: -5.5557053000759025, done: False\n",
      "state: [ 0.14268742 -4.28505978], action: [0.2790283 0.167    ]\n",
      "Episode: 547/3000, Reward: -5.561097777535323, done: False\n",
      "state: [ 0.14335098 -4.25422927], action: [0.16948973 0.167     ]\n",
      "Episode: 548/3000, Reward: -5.528124101503025, done: False\n",
      "state: [ 0.14490584 -4.24640063], action: [0.08210021 0.167     ]\n",
      "Episode: 549/3000, Reward: -5.5423936690036175, done: False\n",
      "state: [ 0.14357772 -4.26462779], action: [0.1471606 0.167    ]\n",
      "Episode: 550/3000, Reward: -5.47395490321426, done: False\n",
      "state: [ 0.14546971 -4.1585451 ], action: [-0.01348071  0.167     ]\n",
      "Episode: 551/3000, Reward: -5.391471195322531, done: False\n",
      "state: [ 0.14235747 -4.2012255 ], action: [0.11024249 0.167     ]\n",
      "Episode: 552/3000, Reward: -5.4135234284776885, done: False\n",
      "state: [ 0.14206335 -4.18622219], action: [0.06678202 0.167     ]\n",
      "Episode: 553/3000, Reward: -5.411429652262129, done: False\n",
      "state: [ 0.14445184 -4.20545861], action: [0.14252116 0.167     ]\n",
      "Episode: 554/3000, Reward: -5.4201493202598865, done: False\n",
      "state: [ 0.14104906 -4.18406238], action: [0.03669853 0.167     ]\n",
      "Episode: 555/3000, Reward: -5.387658368642222, done: False\n",
      "state: [ 0.13974591 -4.15421988], action: [0.00397144 0.167     ]\n",
      "Episode: 556/3000, Reward: -5.407226172208565, done: False\n",
      "state: [ 0.14208333 -4.195706  ], action: [0.14605775 0.167     ]\n",
      "Episode: 557/3000, Reward: -5.39420056642731, done: False\n",
      "state: [ 0.14023631 -4.19218519], action: [0.14956474 0.167     ]\n",
      "Episode: 558/3000, Reward: -5.332929100930114, done: False\n",
      "state: [ 0.14082446 -4.18578407], action: [0.14993551 0.167     ]\n",
      "Episode: 559/3000, Reward: -5.345217268857848, done: False\n",
      "state: [ 0.14069668 -4.18924124], action: [0.25722462 0.167     ]\n",
      "Episode: 560/3000, Reward: -5.371952419133506, done: False\n",
      "state: [ 0.14089183 -4.17428518], action: [0.10104554 0.167     ]\n",
      "Episode: 561/3000, Reward: -5.359672018289709, done: False\n",
      "state: [ 0.14227831 -4.17336507], action: [0.05065577 0.167     ]\n",
      "Episode: 562/3000, Reward: -5.364425366454825, done: False\n",
      "state: [ 0.14106983 -4.18736332], action: [0.17738838 0.167     ]\n",
      "Episode: 563/3000, Reward: -5.377566625063111, done: False\n",
      "state: [ 0.13781395 -4.16132815], action: [0.14707972 0.167     ]\n",
      "Episode: 564/3000, Reward: -5.2984342926087376, done: False\n",
      "state: [ 0.11198202 -4.1467322 ], action: [0.02452174 0.167     ]\n",
      "Episode: 565/3000, Reward: -5.793384962122803, done: False\n",
      "state: [ 0.14623803 -4.34422026], action: [0.18585105 0.167     ]\n",
      "Episode: 566/3000, Reward: -5.662733567942958, done: False\n",
      "state: [ 0.14473315 -4.30920859], action: [ 0.15906307 -0.01595537]\n",
      "Episode: 567/3000, Reward: -5.596068930811369, done: False\n",
      "state: [ 0.14265174 -4.26063621], action: [0.19330148 0.167     ]\n",
      "Episode: 568/3000, Reward: -5.602079376317365, done: False\n",
      "state: [ 0.14357109 -4.28316875], action: [0.11777804 0.167     ]\n",
      "Episode: 569/3000, Reward: -5.664945438527836, done: False\n",
      "state: [ 0.14357348 -4.27654197], action: [0.1385478 0.167    ]\n",
      "Episode: 570/3000, Reward: -5.620906938628765, done: False\n",
      "state: [ 0.14477876 -4.26790723], action: [0.11370047 0.167     ]\n",
      "Episode: 571/3000, Reward: -5.546228244507131, done: False\n",
      "state: [ 0.14368128 -4.24621787], action: [0.16095892 0.167     ]\n",
      "Episode: 572/3000, Reward: -5.41633621106668, done: False\n",
      "state: [ 0.14052444 -4.17804506], action: [0.11311828 0.167     ]\n",
      "Episode: 573/3000, Reward: -5.341961016459326, done: False\n",
      "state: [ 0.14138646 -4.17547417], action: [0.16637827 0.167     ]\n",
      "Episode: 574/3000, Reward: -5.337930248736961, done: False\n",
      "state: [ 0.14192255 -4.17359753], action: [0.12427837 0.167     ]\n",
      "Episode: 575/3000, Reward: -5.3114114211170635, done: False\n",
      "state: [ 0.1377951  -4.11304891], action: [0.03689927 0.167     ]\n",
      "Episode: 576/3000, Reward: -5.194557350475719, done: False\n",
      "state: [ 0.1375128  -4.10799039], action: [0.1225465 0.167    ]\n",
      "Episode: 577/3000, Reward: -5.1025601665470415, done: False\n",
      "state: [ 0.13743455 -4.04157132], action: [0.12076858 0.167     ]\n",
      "Episode: 578/3000, Reward: -5.027657484671149, done: False\n",
      "state: [ 0.13572439 -4.02836697], action: [0.09588175 0.167     ]\n",
      "Episode: 579/3000, Reward: -4.909546894631224, done: False\n",
      "state: [ 0.13282879 -3.96706618], action: [0.10956897 0.167     ]\n",
      "Episode: 580/3000, Reward: -4.70119629911254, done: False\n",
      "state: [ 0.13084174 -3.89357804], action: [0.05625047 0.167     ]\n",
      "Episode: 581/3000, Reward: -4.749156448401949, done: False\n",
      "state: [ 0.12877219 -3.92645041], action: [0.28568372 0.167     ]\n",
      "Episode: 582/3000, Reward: -4.716751596759242, done: False\n",
      "state: [ 0.13808812 -3.85719779], action: [0.33360225 0.167     ]\n",
      "Episode: 583/3000, Reward: -4.344426035615941, done: False\n",
      "state: [ 0.12241927 -3.76144291], action: [-0.6901672  -0.13424915]\n",
      "Episode: 584/3000, Reward: -4.429764250359597, done: False\n",
      "state: [ 0.12703117 -3.80315889], action: [0.09916366 0.167     ]\n",
      "Episode: 585/3000, Reward: -4.545839079715907, done: False\n",
      "state: [ 0.12767015 -3.83881153], action: [0.1286299 0.167    ]\n",
      "Episode: 586/3000, Reward: -4.531986265372449, done: False\n",
      "state: [ 0.12682121 -3.82845746], action: [0.12955366 0.167     ]\n",
      "Episode: 587/3000, Reward: -4.54159916653876, done: False\n",
      "state: [ 0.12297634 -3.77100132], action: [-0.49896562  0.167     ]\n",
      "Episode: 588/3000, Reward: -4.92331993310391, done: False\n",
      "state: [ 0.13673372 -4.04022653], action: [0.06470348 0.167     ]\n",
      "Episode: 589/3000, Reward: -5.084394852637271, done: False\n",
      "state: [ 0.13716559 -4.06543289], action: [0.06724986 0.167     ]\n",
      "Episode: 590/3000, Reward: -5.091463771956254, done: False\n",
      "state: [ 0.13584096 -4.06140936], action: [0.08233313 0.167     ]\n",
      "Episode: 591/3000, Reward: -4.318481803404403, done: False\n",
      "state: [ 0.12476    -3.76051806], action: [0.09868816 0.167     ]\n",
      "Episode: 592/3000, Reward: -4.46025929628665, done: False\n",
      "state: [ 0.12812066 -3.84760645], action: [0.1134325 0.167    ]\n",
      "Episode: 593/3000, Reward: -4.647492322863556, done: False\n",
      "state: [ 0.12881091 -3.89865449], action: [0.16677849 0.167     ]\n",
      "Episode: 594/3000, Reward: -4.637812315446101, done: False\n",
      "state: [ 0.12925861 -3.86784365], action: [0.1346245 0.167    ]\n",
      "Episode: 595/3000, Reward: -4.620840581035384, done: False\n",
      "state: [ 0.13020931 -3.86672828], action: [0.09179472 0.167     ]\n",
      "Episode: 596/3000, Reward: -4.51250463241035, done: False\n",
      "state: [ 0.12637688 -3.82453935], action: [0.21342999 0.167     ]\n",
      "Episode: 597/3000, Reward: -4.574203974473769, done: False\n",
      "state: [ 0.12843717 -3.86041864], action: [0.13389874 0.167     ]\n",
      "Episode: 598/3000, Reward: -4.606702741594665, done: False\n",
      "state: [ 0.12963777 -3.84254666], action: [-0.01855591  0.167     ]\n",
      "Episode: 599/3000, Reward: -4.5082976461736886, done: False\n",
      "state: [ 0.122916   -3.79545703], action: [0.08975994 0.167     ]\n",
      "Episode: 600/3000, Reward: -4.516691063446108, done: False\n",
      "state: [ 0.12778847 -3.85218687], action: [0.10030182 0.167     ]\n",
      "Episode: 601/3000, Reward: -4.539661666141855, done: False\n",
      "state: [ 0.1291333 -3.856674 ], action: [0.17819904 0.167     ]\n",
      "Episode: 602/3000, Reward: -4.629154467092609, done: False\n",
      "state: [ 0.12901022 -3.86631886], action: [0.17941517 0.167     ]\n",
      "Episode: 603/3000, Reward: -4.471378484473068, done: False\n",
      "state: [ 0.12971141 -3.83071798], action: [0.12556241 0.167     ]\n",
      "Episode: 604/3000, Reward: -4.549249996700385, done: False\n",
      "state: [ 0.12849746 -3.84801145], action: [0.14649117 0.167     ]\n",
      "Episode: 605/3000, Reward: -4.582761368896694, done: False\n",
      "state: [ 0.12944243 -3.85386035], action: [0.13840653 0.167     ]\n",
      "Episode: 606/3000, Reward: -4.5242076395003075, done: False\n",
      "state: [ 0.1292686  -3.83783453], action: [0.1449991 0.167    ]\n",
      "Episode: 607/3000, Reward: -4.487825770813618, done: False\n",
      "state: [ 0.1258054  -3.80739501], action: [0.09227823 0.167     ]\n",
      "Episode: 608/3000, Reward: -4.500433097627233, done: False\n",
      "state: [ 0.12774001 -3.82043375], action: [0.171122 0.167   ]\n",
      "Episode: 609/3000, Reward: -4.483129503946824, done: False\n",
      "state: [ 0.12939462 -3.81282215], action: [0.09089352 0.167     ]\n",
      "Episode: 610/3000, Reward: -4.466853360829375, done: False\n",
      "state: [ 0.1287954  -3.78575674], action: [0.05029702 0.167     ]\n",
      "Episode: 611/3000, Reward: -4.419592113448186, done: False\n",
      "state: [ 0.12548022 -3.77237037], action: [-0.04395387  0.167     ]\n",
      "Episode: 612/3000, Reward: -4.439306891078766, done: False\n",
      "state: [ 0.12806807 -3.8028055 ], action: [0.05928967 0.167     ]\n",
      "Episode: 613/3000, Reward: -4.456835043092793, done: False\n",
      "state: [ 0.12534215 -3.79657747], action: [0.11496057 0.167     ]\n",
      "Episode: 614/3000, Reward: -4.433329862854588, done: False\n",
      "state: [ 0.12581184 -3.79634304], action: [0.14322002 0.167     ]\n",
      "Episode: 615/3000, Reward: -4.478970655768309, done: False\n",
      "state: [ 0.12741219 -3.80255926], action: [0.08287329 0.167     ]\n",
      "Episode: 616/3000, Reward: -4.500854823052859, done: False\n",
      "state: [ 0.1279707  -3.80823565], action: [0.10846429 0.167     ]\n",
      "Episode: 617/3000, Reward: -4.425892419065592, done: False\n",
      "state: [ 0.12693553 -3.78676548], action: [0.12269358 0.167     ]\n",
      "Episode: 618/3000, Reward: -4.4476169559586936, done: False\n",
      "state: [ 0.12779903 -3.81577076], action: [0.11262294 0.167     ]\n",
      "Episode: 619/3000, Reward: -4.482289367698145, done: False\n",
      "state: [ 0.12802595 -3.80201623], action: [0.1107733 0.167    ]\n",
      "Episode: 620/3000, Reward: -4.319915894342663, done: False\n",
      "state: [ 0.12786094 -3.78940098], action: [0.11025542 0.167     ]\n",
      "Episode: 621/3000, Reward: -4.422406550756148, done: False\n",
      "state: [ 0.1261505  -3.77458062], action: [0.1187452 0.167    ]\n",
      "Episode: 622/3000, Reward: -4.419258236325743, done: False\n",
      "state: [ 0.12530178 -3.77770738], action: [0.17270164 0.167     ]\n",
      "Episode: 623/3000, Reward: -4.385724717171475, done: False\n",
      "state: [ 0.12213697 -3.74726904], action: [0.1288325 0.167    ]\n",
      "Episode: 624/3000, Reward: -4.366488208598543, done: False\n",
      "state: [ 0.12440265 -3.76347741], action: [0.13866267 0.167     ]\n",
      "Episode: 625/3000, Reward: -4.442716020788499, done: False\n",
      "state: [ 0.12979679 -3.756501  ], action: [0.07627485 0.167     ]\n",
      "Episode: 626/3000, Reward: -4.365105767209606, done: False\n",
      "state: [ 0.1267095  -3.73458072], action: [-0.06594384  0.167     ]\n",
      "Episode: 627/3000, Reward: -4.339548630464062, done: False\n",
      "state: [ 0.12352465 -3.73711695], action: [0.17722285 0.167     ]\n",
      "Episode: 628/3000, Reward: -4.3804013116427445, done: False\n",
      "state: [ 0.12552857 -3.7487518 ], action: [0.10003792 0.167     ]\n",
      "Episode: 629/3000, Reward: -4.393990679920241, done: False\n",
      "state: [ 0.12315525 -3.7438374 ], action: [0.09915083 0.167     ]\n",
      "Episode: 630/3000, Reward: -4.330302049307929, done: False\n",
      "state: [ 0.12553151 -3.74740027], action: [0.11408713 0.167     ]\n",
      "Episode: 631/3000, Reward: -4.296656988663252, done: False\n",
      "state: [ 0.12325924 -3.73330755], action: [0.07884591 0.167     ]\n",
      "Episode: 632/3000, Reward: -4.329597148644164, done: False\n",
      "state: [ 0.12414348 -3.73668406], action: [0.09876271 0.167     ]\n",
      "Episode: 633/3000, Reward: -4.287986371301219, done: False\n",
      "state: [ 0.12501469 -3.74105969], action: [0.11273786 0.167     ]\n",
      "Episode: 634/3000, Reward: -4.303877990878294, done: False\n",
      "state: [ 0.12471649 -3.74518778], action: [0.14377221 0.167     ]\n",
      "Episode: 635/3000, Reward: -4.371884132196758, done: False\n",
      "state: [ 0.12418805 -3.74429463], action: [0.10074248 0.167     ]\n",
      "Episode: 636/3000, Reward: -4.377768427067919, done: False\n",
      "state: [ 0.12284516 -3.75206376], action: [0.13668141 0.167     ]\n",
      "Episode: 637/3000, Reward: -4.362000658009875, done: False\n",
      "state: [ 0.12427044 -3.74411564], action: [0.11350171 0.167     ]\n",
      "Episode: 638/3000, Reward: -4.395393578889584, done: False\n",
      "state: [ 0.12428854 -3.72617624], action: [-0.05589044  0.167     ]\n",
      "Episode: 639/3000, Reward: -4.3584684323444645, done: False\n",
      "state: [ 0.12479038 -3.74277913], action: [0.07523027 0.167     ]\n",
      "Episode: 640/3000, Reward: -4.336496412176489, done: False\n",
      "state: [ 0.12335532 -3.74310785], action: [0.18254893 0.167     ]\n",
      "Episode: 641/3000, Reward: -4.347008113126334, done: False\n",
      "state: [ 0.12448638 -3.73046525], action: [0.06338763 0.167     ]\n",
      "Episode: 642/3000, Reward: -4.332465728837875, done: False\n",
      "state: [ 0.1253823  -3.74201632], action: [0.10511547 0.167     ]\n",
      "Episode: 643/3000, Reward: -4.341328222544904, done: False\n",
      "state: [ 0.12706544 -3.73973007], action: [0.16334374 0.167     ]\n",
      "Episode: 644/3000, Reward: -4.29083205169904, done: False\n",
      "state: [ 0.12240139 -3.70481923], action: [0.13435797 0.167     ]\n",
      "Episode: 645/3000, Reward: -4.28725135378753, done: False\n",
      "state: [ 0.12348887 -3.73416197], action: [0.10906283 0.167     ]\n",
      "Episode: 646/3000, Reward: -4.34768696812121, done: False\n",
      "state: [ 0.12474753 -3.73944385], action: [0.10146992 0.167     ]\n",
      "Episode: 647/3000, Reward: -4.307573927364386, done: False\n",
      "state: [ 0.12516037 -3.74774506], action: [0.22209981 0.167     ]\n",
      "Episode: 648/3000, Reward: -4.359056990976658, done: False\n",
      "state: [ 0.12507616 -3.73618063], action: [0.10331009 0.167     ]\n",
      "Episode: 649/3000, Reward: -4.360032265371443, done: False\n",
      "state: [ 0.12497771 -3.74203318], action: [0.10271706 0.167     ]\n",
      "Episode: 650/3000, Reward: -4.352918547217779, done: False\n",
      "state: [ 0.12468138 -3.73565486], action: [0.09086403 0.167     ]\n",
      "Episode: 651/3000, Reward: -4.299622921706689, done: False\n",
      "state: [ 0.12418215 -3.72802084], action: [0.14685868 0.167     ]\n",
      "Episode: 652/3000, Reward: -4.337048621662162, done: False\n",
      "state: [ 0.12442302 -3.7441312 ], action: [0.14428861 0.167     ]\n",
      "Episode: 653/3000, Reward: -4.259995303059992, done: False\n",
      "state: [ 0.12246807 -3.70743256], action: [0.11280093 0.167     ]\n",
      "Episode: 654/3000, Reward: -4.283394951320206, done: False\n",
      "state: [ 0.12618679 -3.74715316], action: [0.123854 0.167   ]\n",
      "Episode: 655/3000, Reward: -4.365523889482196, done: False\n",
      "state: [ 0.12427802 -3.74517127], action: [0.11600783 0.167     ]\n",
      "Episode: 656/3000, Reward: -4.338618639864775, done: False\n",
      "state: [ 0.12489098 -3.72501968], action: [0.05253259 0.167     ]\n",
      "Episode: 657/3000, Reward: -4.299060423834818, done: False\n",
      "state: [ 0.12368398 -3.70951168], action: [0.05294557 0.167     ]\n",
      "Episode: 658/3000, Reward: -4.3015518948515945, done: False\n",
      "state: [ 0.12411993 -3.69884419], action: [0.5100336  0.12542099]\n",
      "Episode: 659/3000, Reward: -4.240950618010793, done: False\n",
      "state: [ 0.12310765 -3.70006276], action: [0.13651523 0.167     ]\n",
      "Episode: 660/3000, Reward: -4.224544072938268, done: False\n",
      "state: [ 0.12283877 -3.71522589], action: [0.15239741 0.167     ]\n",
      "Episode: 661/3000, Reward: -4.250708067076261, done: False\n",
      "state: [ 0.1233064  -3.69408553], action: [0.06255855 0.167     ]\n",
      "Episode: 662/3000, Reward: -4.237254446732408, done: False\n",
      "state: [ 0.12005553 -3.64589103], action: [0.09792524 0.167     ]\n",
      "Episode: 663/3000, Reward: -4.220032935298879, done: False\n",
      "state: [ 0.1226458  -3.68403355], action: [0.10693037 0.167     ]\n",
      "Episode: 664/3000, Reward: -4.219160801011898, done: False\n",
      "state: [ 0.12169372 -3.68124521], action: [0.10035209 0.167     ]\n",
      "Episode: 665/3000, Reward: -4.20653855633191, done: False\n",
      "state: [ 0.12196281 -3.67291289], action: [0.09031313 0.167     ]\n",
      "Episode: 666/3000, Reward: -4.229119577550772, done: False\n",
      "state: [ 0.12298259 -3.67738194], action: [0.18055752 0.167     ]\n",
      "Episode: 667/3000, Reward: -4.209671111083145, done: False\n",
      "state: [ 0.1214967  -3.67337233], action: [0.08312968 0.167     ]\n",
      "Episode: 668/3000, Reward: -4.211572579590461, done: False\n",
      "state: [ 0.1219651  -3.68100756], action: [0.09696692 0.167     ]\n",
      "Episode: 669/3000, Reward: -4.233479431026136, done: False\n",
      "state: [ 0.12214218 -3.67424688], action: [0.11411446 0.167     ]\n",
      "Episode: 670/3000, Reward: -4.243807719182204, done: False\n",
      "state: [ 0.12421231 -3.69920678], action: [0.12420011 0.167     ]\n",
      "Episode: 671/3000, Reward: -4.251089484633682, done: False\n",
      "state: [ 0.12377663 -3.68823339], action: [0.13460268 0.167     ]\n",
      "Episode: 672/3000, Reward: -4.120478667093169, done: False\n",
      "state: [ 0.11557522 -3.62693178], action: [0.05863919 0.167     ]\n",
      "Episode: 673/3000, Reward: -4.193939319161236, done: False\n",
      "state: [ 0.12097733 -3.67036438], action: [0.1542768 0.167    ]\n",
      "Episode: 674/3000, Reward: -4.200801613018673, done: False\n",
      "state: [ 0.12230607 -3.65684869], action: [0.08745023 0.167     ]\n",
      "Episode: 675/3000, Reward: -4.17633134086629, done: False\n",
      "state: [ 0.12058226 -3.65008502], action: [0.0932107 0.167    ]\n",
      "Episode: 676/3000, Reward: -4.192644856057888, done: False\n",
      "state: [ 0.12133621 -3.65041331], action: [0.10966841 0.167     ]\n",
      "Episode: 677/3000, Reward: -4.154292824173786, done: False\n",
      "state: [ 0.12308952 -3.65146877], action: [0.0561039 0.167    ]\n",
      "Episode: 678/3000, Reward: -4.160520074219687, done: False\n",
      "state: [ 0.11794491 -3.60876123], action: [0.08779319 0.167     ]\n",
      "Episode: 679/3000, Reward: -4.203261582300815, done: False\n",
      "state: [ 0.12218035 -3.65858958], action: [0.10109778 0.167     ]\n",
      "Episode: 680/3000, Reward: -4.180499467211604, done: False\n",
      "state: [ 0.12236725 -3.66213975], action: [0.12125341 0.167     ]\n",
      "Episode: 681/3000, Reward: -4.175031874015991, done: False\n",
      "state: [ 0.12139343 -3.64555892], action: [0.03661456 0.167     ]\n",
      "Episode: 682/3000, Reward: -4.179428984431585, done: False\n",
      "state: [ 0.11875869 -3.6466682 ], action: [0.23568656 0.167     ]\n",
      "Episode: 683/3000, Reward: -4.143366873640995, done: False\n",
      "state: [ 0.12047688 -3.64082092], action: [0.09556895 0.167     ]\n",
      "Episode: 684/3000, Reward: -4.162216924017437, done: False\n",
      "state: [ 0.12264609 -3.64332608], action: [0.1186841 0.167    ]\n",
      "Episode: 685/3000, Reward: -4.211036017381808, done: False\n",
      "state: [ 0.12301818 -3.67529242], action: [0.14638746 0.167     ]\n",
      "Episode: 686/3000, Reward: -4.220373924144542, done: False\n",
      "state: [ 0.12258268 -3.6575145 ], action: [0.19564201 0.167     ]\n",
      "Episode: 687/3000, Reward: -4.19111919479992, done: False\n",
      "state: [ 0.12275115 -3.67295367], action: [0.1856582 0.167    ]\n",
      "Episode: 688/3000, Reward: -4.182895419542724, done: False\n",
      "state: [ 0.12099295 -3.64006309], action: [0.05814993 0.167     ]\n",
      "Episode: 689/3000, Reward: -4.230567210432454, done: False\n",
      "state: [ 0.123027   -3.66312456], action: [-0.05067275  0.167     ]\n",
      "Episode: 690/3000, Reward: -4.245125595187454, done: False\n",
      "state: [ 0.12146897 -3.6676618 ], action: [0.10195814 0.167     ]\n",
      "Episode: 691/3000, Reward: -4.22500609513632, done: False\n",
      "state: [ 0.12142678 -3.67395759], action: [0.12868445 0.167     ]\n",
      "Episode: 692/3000, Reward: -4.205245060761535, done: False\n",
      "state: [ 0.12195138 -3.6577394 ], action: [-0.07844701  0.167     ]\n",
      "Episode: 693/3000, Reward: -4.2581299924601455, done: False\n",
      "state: [ 0.1230212  -3.67185018], action: [0.1124007 0.167    ]\n",
      "Episode: 694/3000, Reward: -4.213074430942818, done: False\n",
      "state: [ 0.12046609 -3.64552258], action: [-0.01744411  0.167     ]\n",
      "Episode: 695/3000, Reward: -4.206612023290113, done: False\n",
      "state: [ 0.12071286 -3.64524349], action: [0.02970695 0.167     ]\n",
      "Episode: 696/3000, Reward: -4.198500298515498, done: False\n",
      "state: [ 0.12321749 -3.6480059 ], action: [0.14007251 0.167     ]\n",
      "Episode: 697/3000, Reward: -4.170925218227781, done: False\n",
      "state: [ 0.12119997 -3.65293934], action: [0.1327033 0.167    ]\n",
      "Episode: 698/3000, Reward: -4.175289280438593, done: False\n",
      "state: [ 0.12732301 -3.6883435 ], action: [0.24149367 0.167     ]\n",
      "Episode: 699/3000, Reward: -4.257832138182527, done: False\n",
      "state: [ 0.12141095 -3.65454271], action: [0.10777289 0.167     ]\n",
      "Episode: 700/3000, Reward: -4.207906244237882, done: False\n",
      "state: [ 0.12143405 -3.64703843], action: [0.08802643 0.167     ]\n",
      "Episode: 701/3000, Reward: -4.137511354732945, done: False\n",
      "state: [ 0.12178626 -3.64513619], action: [0.09176162 0.167     ]\n",
      "Episode: 702/3000, Reward: -4.145612071869507, done: False\n",
      "state: [ 0.1213369  -3.64458118], action: [0.12202161 0.167     ]\n",
      "Episode: 703/3000, Reward: -4.1859783155717825, done: False\n",
      "state: [ 0.12036334 -3.64344741], action: [0.14537592 0.167     ]\n",
      "Episode: 704/3000, Reward: -4.161621819389332, done: False\n",
      "state: [ 0.12195899 -3.65510779], action: [-0.0489853  0.167    ]\n",
      "Episode: 705/3000, Reward: -4.19267219196296, done: False\n",
      "state: [ 0.11991307 -3.63321897], action: [-0.00848835  0.167     ]\n",
      "Episode: 706/3000, Reward: -4.134292189114134, done: False\n",
      "state: [ 0.11998587 -3.64868617], action: [0.10182027 0.167     ]\n",
      "Episode: 707/3000, Reward: -4.2138215547469065, done: False\n",
      "state: [ 0.12307234 -3.66274756], action: [0.06996901 0.167     ]\n",
      "Episode: 708/3000, Reward: -4.205261603432563, done: False\n",
      "state: [ 0.12162773 -3.66333825], action: [0.13697413 0.167     ]\n",
      "Episode: 709/3000, Reward: -4.225471034813115, done: False\n",
      "state: [ 0.12136209 -3.65114138], action: [0.11930881 0.167     ]\n",
      "Episode: 710/3000, Reward: -4.197505612199142, done: False\n",
      "state: [ 0.11850543 -3.63308804], action: [0.05526371 0.167     ]\n",
      "Episode: 711/3000, Reward: -4.252836459735926, done: False\n",
      "state: [ 0.12171149 -3.67100855], action: [0.0943229 0.167    ]\n",
      "Episode: 712/3000, Reward: -4.223900346090043, done: False\n",
      "state: [ 0.12098133 -3.6681117 ], action: [0.11218875 0.167     ]\n",
      "Episode: 713/3000, Reward: -4.235784654274177, done: False\n",
      "state: [ 0.12200511 -3.64866738], action: [-0.07278398  0.167     ]\n",
      "Episode: 714/3000, Reward: -4.132132340752496, done: False\n",
      "state: [ 0.12101985 -3.66195404], action: [-0.23841117  0.14291191]\n",
      "Episode: 715/3000, Reward: -4.2332218113441575, done: False\n",
      "state: [ 0.12102161 -3.66242699], action: [0.11113892 0.167     ]\n",
      "Episode: 716/3000, Reward: -4.162120591262119, done: False\n",
      "state: [ 0.12042937 -3.64947407], action: [0.16075565 0.167     ]\n",
      "Episode: 717/3000, Reward: -4.169435593395948, done: False\n",
      "state: [ 0.12218379 -3.64936311], action: [0.11298247 0.167     ]\n",
      "Episode: 718/3000, Reward: -4.183267966742121, done: False\n",
      "state: [ 0.12160929 -3.64416424], action: [0.07887591 0.167     ]\n",
      "Episode: 719/3000, Reward: -4.208082330543154, done: False\n",
      "state: [ 0.12178631 -3.65028753], action: [0.1579088 0.167    ]\n",
      "Episode: 720/3000, Reward: -4.220291400861528, done: False\n",
      "state: [ 0.12234923 -3.65580494], action: [0.09976694 0.167     ]\n",
      "Episode: 721/3000, Reward: -4.180347019438166, done: False\n",
      "state: [ 0.11748012 -3.58761868], action: [0.14028044 0.167     ]\n",
      "Episode: 722/3000, Reward: -4.110120488339618, done: False\n",
      "state: [ 0.12074317 -3.63068298], action: [0.07664504 0.167     ]\n",
      "Episode: 723/3000, Reward: -4.18322051684365, done: False\n",
      "state: [ 0.12042355 -3.61945068], action: [0.05982251 0.167     ]\n",
      "Episode: 724/3000, Reward: -4.146366854080783, done: False\n",
      "state: [ 0.11974555 -3.62343164], action: [0.13775581 0.167     ]\n",
      "Episode: 725/3000, Reward: -4.140145023532217, done: False\n",
      "state: [ 0.12104591 -3.66224412], action: [0.09688423 0.167     ]\n",
      "Episode: 726/3000, Reward: -4.243890860053447, done: False\n",
      "state: [ 0.1223644  -3.66158539], action: [0.09421793 0.167     ]\n",
      "Episode: 727/3000, Reward: -4.22417941872559, done: False\n",
      "state: [ 0.12082336 -3.65942713], action: [0.10504863 0.167     ]\n",
      "Episode: 728/3000, Reward: -4.223275102953735, done: False\n",
      "state: [ 0.12366092 -3.66513322], action: [0.01122674 0.167     ]\n",
      "Episode: 729/3000, Reward: -4.274464030365564, done: False\n",
      "state: [ 0.12191706 -3.6737472 ], action: [0.119013 0.167   ]\n",
      "Episode: 730/3000, Reward: -4.251301936448203, done: False\n",
      "state: [ 0.12228567 -3.66984194], action: [0.09551957 0.167     ]\n",
      "Episode: 731/3000, Reward: -4.263358448452112, done: False\n",
      "state: [ 0.1221035  -3.66667811], action: [0.11383439 0.167     ]\n",
      "Episode: 732/3000, Reward: -4.252174218815621, done: False\n",
      "state: [ 0.12185256 -3.66884929], action: [0.05023651 0.167     ]\n",
      "Episode: 733/3000, Reward: -4.249381131974432, done: False\n",
      "state: [ 0.12186958 -3.68382235], action: [0.1835907 0.167    ]\n",
      "Episode: 734/3000, Reward: -4.294009199004217, done: False\n",
      "state: [ 0.12173567 -3.67814809], action: [0.20051254 0.167     ]\n",
      "Episode: 735/3000, Reward: -4.262637109038933, done: False\n",
      "state: [ 0.11781487 -3.60518488], action: [0.11452125 0.167     ]\n",
      "Episode: 736/3000, Reward: -4.161618543974641, done: False\n",
      "state: [ 0.12062845 -3.65097409], action: [0.07075016 0.167     ]\n",
      "Episode: 737/3000, Reward: -4.2317474392945424, done: False\n",
      "state: [ 0.12155646 -3.66396904], action: [0.13303223 0.167     ]\n",
      "Episode: 738/3000, Reward: -4.301869981609203, done: False\n",
      "state: [ 0.12446837 -3.73186393], action: [0.19504882 0.167     ]\n",
      "Episode: 739/3000, Reward: -4.3403070949541895, done: False\n",
      "state: [ 0.12086615 -3.68408216], action: [0.00569751 0.167     ]\n",
      "Episode: 740/3000, Reward: -4.319795931058347, done: False\n",
      "state: [ 0.12233864 -3.68730401], action: [0.10773173 0.167     ]\n",
      "Episode: 741/3000, Reward: -4.27263253932245, done: False\n",
      "state: [ 0.1224272  -3.67997708], action: [0.08351211 0.167     ]\n",
      "Episode: 742/3000, Reward: -4.276350026171241, done: False\n",
      "state: [ 0.12255219 -3.68118726], action: [0.11631189 0.167     ]\n",
      "Episode: 743/3000, Reward: -4.260361393713588, done: False\n",
      "state: [ 0.1243796  -3.69382487], action: [0.159392 0.167   ]\n",
      "Episode: 744/3000, Reward: -4.296351340677904, done: False\n",
      "state: [ 0.12318353 -3.711551  ], action: [0.10963567 0.167     ]\n",
      "Episode: 745/3000, Reward: -4.338576788284829, done: False\n",
      "state: [ 0.12335532 -3.70199983], action: [0.13419402 0.167     ]\n",
      "Episode: 746/3000, Reward: -4.3213196342335545, done: False\n",
      "state: [ 0.12254912 -3.68619102], action: [0.06781148 0.167     ]\n",
      "Episode: 747/3000, Reward: -4.293048610023428, done: False\n",
      "state: [ 0.12092668 -3.68129314], action: [0.04033894 0.167     ]\n",
      "Episode: 748/3000, Reward: -4.321204243527944, done: False\n",
      "state: [ 0.12072102 -3.6756652 ], action: [0.22203016 0.167     ]\n",
      "Episode: 749/3000, Reward: -4.213360438872841, done: False\n",
      "state: [ 0.12053285 -3.63326879], action: [0.08432185 0.167     ]\n",
      "Episode: 750/3000, Reward: -4.161572758769033, done: False\n",
      "state: [ 0.121078   -3.62697598], action: [0.09347944 0.167     ]\n",
      "Episode: 751/3000, Reward: -4.189764557024809, done: False\n",
      "state: [ 0.12090847 -3.63863448], action: [0.1114454 0.167    ]\n",
      "Episode: 752/3000, Reward: -4.192814772213974, done: False\n",
      "state: [ 0.11938252 -3.62693547], action: [0.07882456 0.167     ]\n",
      "Episode: 753/3000, Reward: -4.193952315819494, done: False\n",
      "state: [ 0.12012699 -3.62736888], action: [0.09638891 0.167     ]\n",
      "Episode: 754/3000, Reward: -4.170573429881388, done: False\n",
      "state: [ 0.12096442 -3.62598243], action: [0.15417929 0.167     ]\n",
      "Episode: 755/3000, Reward: -4.183032103790437, done: False\n",
      "state: [ 0.12202579 -3.63157587], action: [0.11341884 0.167     ]\n",
      "Episode: 756/3000, Reward: -4.186732845180392, done: False\n",
      "state: [ 0.12175946 -3.64451194], action: [0.04549473 0.167     ]\n",
      "Episode: 757/3000, Reward: -4.215671281824767, done: False\n",
      "state: [ 0.11889462 -3.62049356], action: [0.19455998 0.167     ]\n",
      "Episode: 758/3000, Reward: -4.159786224487824, done: False\n",
      "state: [ 0.12519827 -3.60201887], action: [0.09171785 0.167     ]\n",
      "Episode: 759/3000, Reward: -4.176671873586114, done: False\n",
      "state: [ 0.11975578 -3.61504815], action: [0.05395668 0.167     ]\n",
      "Episode: 760/3000, Reward: -4.119799389659708, done: False\n",
      "state: [ 0.11998673 -3.61142666], action: [0.10897145 0.167     ]\n",
      "Episode: 761/3000, Reward: -4.150341804200632, done: False\n",
      "state: [ 0.11895571 -3.60938443], action: [0.09925841 0.167     ]\n",
      "Episode: 762/3000, Reward: -4.16630609455286, done: False\n",
      "state: [ 0.12178051 -3.61890519], action: [-0.0146154  0.167    ]\n",
      "Episode: 763/3000, Reward: -4.478967255131476, done: False\n",
      "state: [ 0.13193683 -3.98420003], action: [0.05950818 0.167     ]\n",
      "Episode: 764/3000, Reward: -5.031986215457564, done: False\n",
      "state: [ 0.13527394 -4.01373405], action: [0.089777 0.167   ]\n",
      "Episode: 765/3000, Reward: -5.043486008758263, done: False\n",
      "state: [ 0.1342321  -4.01776213], action: [0.11677313 0.167     ]\n",
      "Episode: 766/3000, Reward: -4.894460038463692, done: False\n",
      "state: [ 0.13415299 -3.99806395], action: [0.10884202 0.167     ]\n",
      "Episode: 767/3000, Reward: -5.028888013055644, done: False\n",
      "state: [ 0.13372111 -4.01133427], action: [0.17535803 0.167     ]\n",
      "Episode: 768/3000, Reward: -5.051234324658326, done: False\n",
      "state: [ 0.13502309 -4.01881896], action: [0.09067918 0.167     ]\n",
      "Episode: 769/3000, Reward: -5.0509110467205085, done: False\n",
      "state: [ 0.13566315 -4.03075142], action: [0.21409018 0.167     ]\n",
      "Episode: 770/3000, Reward: -5.068291122740968, done: False\n",
      "state: [ 0.13530192 -4.02286272], action: [0.09182172 0.167     ]\n",
      "Episode: 771/3000, Reward: -5.036010372548817, done: False\n",
      "state: [ 0.13539779 -4.01624761], action: [0.1495045 0.167    ]\n",
      "Episode: 772/3000, Reward: -5.058023831810507, done: False\n",
      "state: [ 0.13608754 -4.04256585], action: [0.23013589 0.167     ]\n",
      "Episode: 773/3000, Reward: -5.070895869592651, done: False\n",
      "state: [ 0.13336815 -4.01202895], action: [0.13322821 0.167     ]\n",
      "Episode: 774/3000, Reward: -5.0865951745280436, done: False\n",
      "state: [ 0.13654204 -4.03848375], action: [0.11307967 0.167     ]\n",
      "Episode: 775/3000, Reward: -5.0972601773162065, done: False\n",
      "state: [ 0.13568108 -4.04222004], action: [0.07817594 0.167     ]\n",
      "Episode: 776/3000, Reward: -5.097328081656256, done: False\n",
      "state: [ 0.1364566  -4.03813222], action: [0.119697 0.167   ]\n",
      "Episode: 777/3000, Reward: -5.073917823127536, done: False\n",
      "state: [ 0.13442308 -4.03508355], action: [0.14170599 0.167     ]\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(memory_size)\n",
    "epsilon = 1.0\n",
    "# Set up the environment\n",
    "env = cstr_env(order=1)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Initialize networks and optimizer\n",
    "naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network = NAFNetwork(state_size, action_size)\n",
    "target_naf_network.load_state_dict(naf_network.state_dict())\n",
    "target_naf_network.eval()\n",
    "\n",
    "optimizer = optim.Adam(naf_network.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = get_action(state, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        train_model(e)\n",
    "        if step == 300:\n",
    "            break\n",
    "            \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"Episode: {e+1}/{num_episodes}, Reward: {total_reward}, done: {done}\")\n",
    "    print(f\"state: {state}, action: {action}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a08d4-b186-44b8-bf8b-f93700e4377a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
